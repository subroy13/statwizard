[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Open Source Softwares",
    "section": "",
    "text": "Below is a curated collection of software, packages, and tools I have developed or contributed to ‚Äî resulting from various academic and personal projects.\n\n\n\n\nhandanim A Python library for programmatically generating hand-drawn sketch-style animations.\nIdeal for instructional videos, visual explanations, and stylized presentations.\n\nüìò Docs: https://subroy13.github.io/handanim\nüíª GitHub: https://github.com/subroy13/handanim \n\n\n\n\n\n\n\n\nImportant\n\n\n\nI am actively looking for collaboration in the handanim project. If building hand-drawn sketch-style animations videos using reproducible Python code sounds interesting to you, please reach out to me by email! or create an issue in Github\n\n\n\n\ndecompy A Python package implementing several robust matrix decomposition algorithms, supporting outlier-resistant SVD/PCA pipelines.\n\nüì¶ PyPI: https://pypi.org/project/decompy\n\nüíª GitHub: https://github.com/subroy13/decompy\n \n\nroufcp Robust gradual changepoint detection in time series using Rough-Fuzzy set theory. Useful for weak or soft transitions where classic CPD fails.\n\nüì¶ PyPI: https://pypi.org/project/roufcp\n\nüíª GitHub: https://github.com/subroy13/roufcp\n \n\nrsvddpd Robust Singular Value Decomposition using the Minimum Density Power Divergence Estimator (MDPDE).\n\nüì¶ CRAN: https://cran.r-project.org/web/packages/rsvddpd/index.html\nüìò Vignette: https://cran.r-project.org/web/packages/rsvddpd/vignettes/rSVDdpd-intro.html\nüíª GitHub: https://github.com/subroy13/rsvddpd\n | [1] ‚Äú‚Äù\n\ncallgrind-reader A TypeScript library to parse callgrind-like profiling output, extract metrics, and integrate into analysis tools.\n\nüì¶ npm: https://www.npmjs.com/package/callgrind-reader\n\n \n\nmathaddons.sty A LaTeX package for writing research papers containing multiple useful mathematical commands. Click here to download."
  },
  {
    "objectID": "software.html#software-tools-packages",
    "href": "software.html#software-tools-packages",
    "title": "Open Source Softwares",
    "section": "",
    "text": "Below is a curated collection of software, packages, and tools I have developed or contributed to ‚Äî resulting from various academic and personal projects.\n\n\n\n\nhandanim A Python library for programmatically generating hand-drawn sketch-style animations.\nIdeal for instructional videos, visual explanations, and stylized presentations.\n\nüìò Docs: https://subroy13.github.io/handanim\nüíª GitHub: https://github.com/subroy13/handanim \n\n\n\n\n\n\n\n\nImportant\n\n\n\nI am actively looking for collaboration in the handanim project. If building hand-drawn sketch-style animations videos using reproducible Python code sounds interesting to you, please reach out to me by email! or create an issue in Github\n\n\n\n\ndecompy A Python package implementing several robust matrix decomposition algorithms, supporting outlier-resistant SVD/PCA pipelines.\n\nüì¶ PyPI: https://pypi.org/project/decompy\n\nüíª GitHub: https://github.com/subroy13/decompy\n \n\nroufcp Robust gradual changepoint detection in time series using Rough-Fuzzy set theory. Useful for weak or soft transitions where classic CPD fails.\n\nüì¶ PyPI: https://pypi.org/project/roufcp\n\nüíª GitHub: https://github.com/subroy13/roufcp\n \n\nrsvddpd Robust Singular Value Decomposition using the Minimum Density Power Divergence Estimator (MDPDE).\n\nüì¶ CRAN: https://cran.r-project.org/web/packages/rsvddpd/index.html\nüìò Vignette: https://cran.r-project.org/web/packages/rsvddpd/vignettes/rSVDdpd-intro.html\nüíª GitHub: https://github.com/subroy13/rsvddpd\n | [1] ‚Äú‚Äù\n\ncallgrind-reader A TypeScript library to parse callgrind-like profiling output, extract metrics, and integrate into analysis tools.\n\nüì¶ npm: https://www.npmjs.com/package/callgrind-reader\n\n \n\nmathaddons.sty A LaTeX package for writing research papers containing multiple useful mathematical commands. Click here to download."
  },
  {
    "objectID": "notes/concepts/spatial-stat.html",
    "href": "notes/concepts/spatial-stat.html",
    "title": "Spatial Statistics",
    "section": "",
    "text": "Often we find ourselves with a dataset which has a location / spatial information that should be part of the modelling. This can be useful to model the dependence structure of datasets, and this dependence structure can be positively / negatively correlated.\n\nAir Pollution Measurements: Air pollution levels of nearby locations are positively correlated.\nPlant growths: Plants in the nearby locations compete with each other for the same resources, growth of one kind of plant may be negatively correlated with the growth of another.\n\n\n\n\nWhen we observe say \\(Y_n = (Y(s_1), \\dots, Y(s_n))\\) where \\(s_1, \\dots, s_n\\) are the spatial locations, think of this as a single realization of a \\(n\\)-variable random vector, instead of the usual \\(n\\) replications of a univariate random vector. It is not clear at this point, whether any inference is possible based on a single sample observation[1].\nWhen time series methodologies are concerned, the sample indices have a natural ordering, hence \\(Y_t\\) depends only on the past values \\(\\{ Y_s : s &lt; t \\}\\). However, for the spatial case, say even when agricultural plots are arranged in a line to produce a natural ordering, the value \\(Y_{t}\\) may depend on both \\(Y_{t-1}\\) and \\(Y_{t+1}\\).\n\n\n\n\nIn the spatial data, there are two things in concern.\n\nThere is a spatial horizon \\(S\\). From this spatial horizon, some locations \\(s_1, \\dots, s_n\\) are where the samples are observed.\nFor each of these locations, some variables \\(Y(s)\\) is observed.\n\nTherefore, we assume the existence of a stochastic process \\(\\{  Y(s): s \\in S \\}\\).\nThere are 3 major types of spatial data based on the characteristics of this stochastic process.\n\nPoint Patterns: The location themselves are realizations of some stochastic process, e.g., events of earthquakes / volcano.\n\nThe main question here is to try to find patterns / clusters on which locations these events happen or are they happen randomly throughout the spatial horizon.\nUsually, if we just observe the locations alone, these are called unmarked point process. Example is location of volcanos.\nIf we also observe some variables associated with these event locations, these are called marked point process. Example is along with the location of earthquake, we also measure its intensity.\n\nGeostatistical Data: The underlying stochastic process for the \\(Y\\)-variable is defined on the continuous domain \\(S\\). However, due to fixed sampling design, we observe \\(Y(s)\\) only at few designated locations, e.g.¬†\\(Y(s_1), \\dots, Y(s_n)\\).\n\nThe aim is to use the observed values to predict the continuous surface \\(Y(s)\\), and use the neighbouring correlation to improve the prediction.\nExample is to predict the underground oil reserves based on the quantity of oil at few mining locations.\n\nLattice Data: The underlying sampling frame is a fixed designed areal units. In each of these areal units, some \\(Y\\)-observations are made. These areal units could be subplots, counties, census tracts, etc.\n\nExample is agricultural plots are sub-divided into subplots, where different types of crop-yields are observed.\nIf it is census-tract data, then often the aim is to do some kind of regression of one spatial-process on another to explain its variation.\nAnother aim is to predict the observation (called ‚Äúkriging‚Äù) at the unobserved lattice areal units based on the observed values."
  },
  {
    "objectID": "notes/concepts/spatial-stat.html#how-spatial-analysis-is-different",
    "href": "notes/concepts/spatial-stat.html#how-spatial-analysis-is-different",
    "title": "Spatial Statistics",
    "section": "",
    "text": "When we observe say \\(Y_n = (Y(s_1), \\dots, Y(s_n))\\) where \\(s_1, \\dots, s_n\\) are the spatial locations, think of this as a single realization of a \\(n\\)-variable random vector, instead of the usual \\(n\\) replications of a univariate random vector. It is not clear at this point, whether any inference is possible based on a single sample observation[1].\nWhen time series methodologies are concerned, the sample indices have a natural ordering, hence \\(Y_t\\) depends only on the past values \\(\\{ Y_s : s &lt; t \\}\\). However, for the spatial case, say even when agricultural plots are arranged in a line to produce a natural ordering, the value \\(Y_{t}\\) may depend on both \\(Y_{t-1}\\) and \\(Y_{t+1}\\)."
  },
  {
    "objectID": "notes/concepts/spatial-stat.html#types-of-spatial-data",
    "href": "notes/concepts/spatial-stat.html#types-of-spatial-data",
    "title": "Spatial Statistics",
    "section": "",
    "text": "In the spatial data, there are two things in concern.\n\nThere is a spatial horizon \\(S\\). From this spatial horizon, some locations \\(s_1, \\dots, s_n\\) are where the samples are observed.\nFor each of these locations, some variables \\(Y(s)\\) is observed.\n\nTherefore, we assume the existence of a stochastic process \\(\\{  Y(s): s \\in S \\}\\).\nThere are 3 major types of spatial data based on the characteristics of this stochastic process.\n\nPoint Patterns: The location themselves are realizations of some stochastic process, e.g., events of earthquakes / volcano.\n\nThe main question here is to try to find patterns / clusters on which locations these events happen or are they happen randomly throughout the spatial horizon.\nUsually, if we just observe the locations alone, these are called unmarked point process. Example is location of volcanos.\nIf we also observe some variables associated with these event locations, these are called marked point process. Example is along with the location of earthquake, we also measure its intensity.\n\nGeostatistical Data: The underlying stochastic process for the \\(Y\\)-variable is defined on the continuous domain \\(S\\). However, due to fixed sampling design, we observe \\(Y(s)\\) only at few designated locations, e.g.¬†\\(Y(s_1), \\dots, Y(s_n)\\).\n\nThe aim is to use the observed values to predict the continuous surface \\(Y(s)\\), and use the neighbouring correlation to improve the prediction.\nExample is to predict the underground oil reserves based on the quantity of oil at few mining locations.\n\nLattice Data: The underlying sampling frame is a fixed designed areal units. In each of these areal units, some \\(Y\\)-observations are made. These areal units could be subplots, counties, census tracts, etc.\n\nExample is agricultural plots are sub-divided into subplots, where different types of crop-yields are observed.\nIf it is census-tract data, then often the aim is to do some kind of regression of one spatial-process on another to explain its variation.\nAnother aim is to predict the observation (called ‚Äúkriging‚Äù) at the unobserved lattice areal units based on the observed values."
  },
  {
    "objectID": "notes/technicals/optimizer-consistency-proof.html",
    "href": "notes/technicals/optimizer-consistency-proof.html",
    "title": "Proof of Statistical Consistency of an Optimizer",
    "section": "",
    "text": "Suppose you have a function \\(Q(\\theta)\\) and its empirical counterpart is given by \\(Q_n(\\theta)\\). Let \\[\n\\theta^* = \\arg\\max_{\\theta \\in \\Theta} Q(\\theta), \\ \\quad\n\\hat{\\theta}_n = \\arg\\max_{\\theta \\in \\Theta} Q_n(\\theta)\n\\]\n(where one can replace the above by argmin as well). Then, one typical use-case is to show that \\(\\hat{\\theta}_n\\) is close to \\(\\theta^\\ast\\).\nOne trick to do that is as follows:\n\\[\n\\begin{align*}\n    Q(\\theta^*) - Q(\\hat{\\theta}_n)\n    & = Q(\\theta^*) - Q_n(\\theta^*) + Q_n(\\hat{\\theta}_n) - Q(\\hat{\\theta}_n) + (Q_n(\\theta^*) - Q_n(\\hat{\\theta}_n))\\\\\n    & \\leq Q(\\theta^*) - Q_n(\\theta^*) + Q_n(\\hat{\\theta}_n) - Q(\\hat{\\theta}_n)\\\\\n    & \\qquad \\text{ since the last term is negative by definition of } \\hat{\\theta}_n\\\\\n    & \\leq \\vert Q(\\theta^*) - Q_n(\\theta^*) \\vert + \\vert Q_n(\\hat{\\theta}_n) - Q(\\hat{\\theta}_n) \\vert\\\\\n    & 2 \\sup_{\\theta \\in \\Theta} \\vert Q(\\theta) - Q_n(\\theta)\\vert\n\\end{align*}\n\\]\nOften, one can show that this bound is very small (because \\(Q_n\\) is the empirical version of \\(Q\\)), all we need is to strengthen the usual bound to uniform convergence over \\(\\theta\\).\nAt this point, we have \\[\nQ(\\theta^*) - Q_n(\\hat{\\theta}_n) = o_p(1) \\ (\\approx \\text{small})\n\\] as \\(n \\to \\infty\\). This means, if \\(Q\\) is even locally convex near its unique global minima, then that would imply \\(\\hat{\\theta}_n\\) must be close to \\(\\theta^\\ast\\). Note that the significance here is than we do not have to produce any assumption of convexity (or something similar) to the random function \\(Q_n(\\cdot)\\), instead focus on only the deterministic function \\(Q(\\cdot)\\) instead.\nThe typical assumption required here is that there should be no sequence of local minimas of \\(Q\\) that can approximate the global minimum \\(Q(\\theta^*)\\) without approaching \\(\\theta^*\\). See (Van der Vaart 2000) for a more formal statement of this assumption.\n\n\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press."
  },
  {
    "objectID": "notes/technicals/optimizer-consistency-proof.html#references",
    "href": "notes/technicals/optimizer-consistency-proof.html#references",
    "title": "Proof of Statistical Consistency of an Optimizer",
    "section": "",
    "text": "Van der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Indian Statistical Institute, Kolkata | India PhD in Statistics | Aug 2021 - June 2025 During my PhD, I was fortunate to work with Ayanendranath Basu and Abhik Ghosh. My doctoral thesis is about developing fast and scalable algorithms for robust SVD and robust PCA using the minimum density power divergence.\n\nIf you are interested in applying these methods, see this R package or this Python package.\nIf you are more into maths, may be take a look at my PhD thesis here.\n\nIndian Statistical Institute, Kolkata | India M.Stat. | Aug 2019 - July 2021\nIndian Statistical Institute, Kolkata | India B.Stat. | Aug 2016 - July 2019"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "Indian Statistical Institute, Kolkata | India PhD in Statistics | Aug 2021 - June 2025 During my PhD, I was fortunate to work with Ayanendranath Basu and Abhik Ghosh. My doctoral thesis is about developing fast and scalable algorithms for robust SVD and robust PCA using the minimum density power divergence.\n\nIf you are interested in applying these methods, see this R package or this Python package.\nIf you are more into maths, may be take a look at my PhD thesis here.\n\nIndian Statistical Institute, Kolkata | India M.Stat. | Aug 2019 - July 2021\nIndian Statistical Institute, Kolkata | India B.Stat. | Aug 2016 - July 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nWashington Univeristy in St.¬†Louis | Postdoctoral Researcher | Aug 2025 - current\nSysCloud Inc. | Principal Information Researcher | July 2022 - June 2025\n\n\n\n\n\n\nNote\n\n\n\nThe job of an Information Researcher to research, design and build automated systems using algorithms, machine learning, and deep learning techniques. You can think of this as a convex combination of data scientist, solution architect and full-stack developer.\n\n\nSysCloud Inc. | Data Scientist | July 2021 - June 2022"
  },
  {
    "objectID": "about.html#places-i-have-been",
    "href": "about.html#places-i-have-been",
    "title": "About Me",
    "section": "Places I have been",
    "text": "Places I have been\nMost of my travel revolves around visiting academic conferences. So far, I have been to 4 out of 7 continents, and I hope to get to all 7 someday! üó∫Ô∏è‚ÅÄ‡™ú‚úàÔ∏é"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About Me",
    "section": "Hobbies",
    "text": "Hobbies\n\nPre-Stable Diffusion Era\nOne of my hobby was to build various 3D art forms using softwares like Bryce 3D, Daz Studio, Blender, etc. It was really fascinating to me to see how mathematical ideas (e.g.¬†coordinate geometries) are applied in the 3D modelling world. Here‚Äôs a beginner friendly tutorial on this topic.\nHere‚Äôs a few outputs that came out of this activities."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My current research interest concentrates on Robust Statistics, Dependent Data Analysis and development of Statistical Methods for Deep Learning.\n\n\n\nStatistical Methods for Deep Learning\n\nDeep learning models have found super-accurate inferences and profound applications in many fields. Unfortunately, we understand very little about them: There‚Äôs always concerns about safety, ethics and so many other things. There‚Äôs been a proliferation of statistical ideas that combines old techniques with these modern blackboxes to assure reliability and help us understand them more.\n\n\nBonnerjee, Soham, Sayar Karmakar, and Subhrajyoty Roy.\n‚ÄúWISER: Segmenting watermarked region ‚Äì an epidemic change-point perspective.‚Äù\narXiv preprint arXiv:2509.21160 (2025).\nOpen Access Link\n\n\n\nDependent Data Analysis\n\nObservations are often correlated with each other, sometimes these correlation occur temporally, or spatially, or both. Dependent Data Analysis aims to obtain valid inference guarantees for these kinds of datasets.\n\n\nDeb, Soudeep, Claudia Neves, and Subhrajyoty Roy. ‚ÄúNonparametric quantile regression for spatio-temporal processes.‚Äù arXiv preprint arXiv:2405.13783 (2024). Open Access Link\nBhaduri, Ritwik, Subhrajyoty Roy, and Sankar K. Pal. ‚ÄúRough-Fuzzy CPD: a gradual change point detection algorithm.‚Äù Journal of Data, Information and Management 4, no. 3 (2022): 243‚Äì266. Link | Open Access Link\n\n\n\nRobust Statistics\n\nRobust Statistics aims to deliver guaranteed inferences even when your data may contain outliers, or the assumptions underlying your inference strategy are slightly violated.\n\n\nRoy, Subhrajyoty, Abhik Ghosh, and Ayanendranath Basu. ‚ÄúRobust Rank Estimation for Noisy Matrices.‚Äù arXiv preprint arXiv:2510.19583 (2025). Open Access Link\nJana, Suryasis, Subhrajyoty Roy, Ayanendranath Basu, and Abhik Ghosh.\n‚ÄúAsymptotic breakdown point analysis of the minimum density power divergence estimator under independent non-homogeneous setups.‚Äù\narXiv preprint arXiv:2508.12426 (2025).\nOpen Access Link\nRoy, Subhrajyoty. ‚ÄúRobust Matrix Factorization using the Density Power Divergence and its Applications‚Äù. ISI PhD Thesis TH646 (2025). Open Access Link\nRoy, Subhrajyoty, Supratik Basu, Abhik Ghosh, and Ayanendranath Basu.\n‚ÄúGeneralized Alpha-Beta Divergence and Associated Entropy Measures.‚Äù\narXiv preprint arXiv:2507.04637 (2025).\nOpen Access Link\nRoy, Subhrajyoty, Abir Sarkar, Abhik Ghosh, and Ayanendranath Basu. ‚ÄúAsymptotic Breakdown Point Analysis for a General Class of Minimum Divergence Estimators.‚Äù Bernoulli (2025+) (Accepted). Open Access Link\nRoy, Subhrajyoty, Ayanendranath Basu, and Abhik Ghosh.\n‚ÄúRobust Principal Component Analysis using Density Power Divergence.‚Äù Journal of Machine Learning Research 25, no. 324 (2024): 1‚Äì40. Link | Open Access Link\nPyne, Arijit, Subhrajyoty Roy, Abhik Ghosh, and Ayanendranath Basu.\n‚ÄúRobust and efficient estimation in ordinal response models using the density power divergence.‚Äù\nStatistics 58, no. 3 (2024): 481‚Äì520.\nLink | Open Access Link\nRoy, Subhrajyoty, Abhik Ghosh, and Ayanendranath Basu.\n‚ÄúRobust singular value decomposition with application to video surveillance background modelling.‚Äù\nStatistics and Computing 34, no. 5 (2024): 178.\nLink | Open Access Link | Supplementary Material\n\n\n\nApplications and Exploratory Data Analysis\nIn addition of these broad topics of interests, there‚Äôs been a few works which is more application-oriented, and does not fit into one of these categories.\n\nRoy, Subhrajyoty.\n‚ÄúTrustworthy Dimensionality Reduction.‚Äù\narXiv preprint arXiv:2405.05868 (2024).\nOpen Access Link\nGhatak, Anirban, Shivshanker Singh Patel, Soham Bonnerjee, and Subhrajyoty Roy.\n‚ÄúA generalized epidemiological model with dynamic and asymptomatic population.‚Äù\nStatistical Methods in Medical Research 31, no. 11 (2022): 2137‚Äì2163.\nLink | Open Access Link\nRoy, Subhrajyoty, Debasis Sengupta, Kalyan Rudra, and Udit Surya Saha.\n‚ÄúAnalysis of Pollution Patterns in Regions of Kolkata.‚Äù\nCalcutta Statistical Association Bulletin 72, no. 2 (2020): 133‚Äì170.\nLink\nDalal, Abhinandan, Diganta Mukherjee, and Subhrajyoty Roy.\n‚ÄúThe Information Content of Taster‚Äôs Valuation in Tea Auctions of India.‚Äù\narXiv preprint arXiv:2005.02814 (2020).\nOpen Access Link\nMukherjee, Diganta, Abhinandan Dalal, and Subhrajyoty Roy.\n‚ÄúFeasibility of Transparent Price Discovery in Tea through Auction in India.‚Äù\nCommodity Insights Yearbook MCX (2019): 44‚Äì52.\nLink | Open Access Link\nBhaduri, Ritwik, Soham Bonnerjee, and Subhrajyoty Roy.\n‚ÄúOnset detection: A new approach to QBH system.‚Äù\narXiv preprint arXiv:1908.07409 (2019).\nOpen Access Link\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "research/talks.html",
    "href": "research/talks.html",
    "title": "Talks & Seminars",
    "section": "",
    "text": "Below is a curated list of invited lectures, conference presentations, public lectures, and special talks delivered over the years."
  },
  {
    "objectID": "research/talks.html#section",
    "href": "research/talks.html#section",
    "title": "Talks & Seminars",
    "section": "2025",
    "text": "2025\n\nLocalized Detection of Authenticity in Mixed Source Texts via an Epidemic Change-Point Perspective\nInvited Talk\nGraduate Student Seminar, Washington University in St.¬†Louis, USA.\nSlide deck\n\n\nGeneralized Alpha-Beta Divergence, its Properties and Associated Entropy\nContributory Talk\nInternational Conference on Robust Statistics (ICORS) 2025, Stresa, Piedmont, Italy.\nSlide deck"
  },
  {
    "objectID": "research/talks.html#section-1",
    "href": "research/talks.html#section-1",
    "title": "Talks & Seminars",
    "section": "2024",
    "text": "2024\n\nRobust Principal Component Analysis using Density Power Divergence\nBest Paper Nominee\nIISA Conference 2024, Cochin University of Science and Technology.\nSlide deck\n\n\nRobust Matrix Factorization using Density Power Divergence and its Applications\nPublic Lecture\nDoctoral Research Lecture, Indian Statistical Institute, Kolkata.\nSlide deck\n\n\nA Novel and Scalable Background Modelling Algorithm for Video Surveillance Data\nContributory Talk\nIMS Asia Pacific Rim Meeting (APRM 2024), University of Melbourne.\nSlide deck\n\n\nA Review of Robust Location and Scatter Estimators\nGuest Lecture\nRobust Statistics Course (M.Stat.‚Äô22 & M.Stat.‚Äô24), Indian Statistical Institute, Kolkata.\nSlide deck"
  },
  {
    "objectID": "research/talks.html#section-2",
    "href": "research/talks.html#section-2",
    "title": "Talks & Seminars",
    "section": "2021",
    "text": "2021\n\nAlgorithmic Fairness of Statistical Decision Systems\nMemorial Lecture\nPrasanta Chandra Mahalanobis Memorial Lecture, Indian Statistical Institute, Kolkata.\nSlide deck"
  },
  {
    "objectID": "research/talks.html#section-3",
    "href": "research/talks.html#section-3",
    "title": "Talks & Seminars",
    "section": "2019",
    "text": "2019\n\nVisualising Multivariate Data using t-SNE\nMemorial Lecture\nD. Basu Memorial Lecture, Indian Statistical Institute, Kolkata.\nSlide deck"
  },
  {
    "objectID": "research/rsvddpd.html",
    "href": "research/rsvddpd.html",
    "title": "Supplementary Material for rSVDdpd",
    "section": "",
    "text": "A basic algorithmic task in automated video surveillance is to separate background and foreground objects. Camera tampering, noisy videos, low frame rate, etc., pose difficulties in solving the problem. A general approach that classifies the tampered frames, and performs subsequent analysis on the remaining frames after discarding the tampered ones, results in loss of information. If the ultimate goal is to perform background modelling, foreground object detection or motion detection etc., then it would be useful to have an algorithm that robustly performs background foreground separation from video surveillance data. Thus, considerable effort has been expended to solve this problem.\n\nThere is a class of statistical background modelling that assumes that the intensity of the pixel values corresponding to the background content is distributed according to a known probability distribution (Gaussian or Mixture of Gaussian). These algorithms are fast, but their performances are not so great.\nBackground modelling based on deep neural network has also been performed. Usually a convolutional neural network with hundreds, thousands or millions of parameters with several layers of connection is built. These are fast for the inference, their performances are great, but they require an enormous amount of data and time for training.\nRecent approaches towards background modelling assumes a decomposition of the video data into a low rank matrix (corresponding to the background content) and a sparse or noisy matrix (corresponding to the foreground content). Matrix decomposition algorithms like robust PCA have been found to be useful here. However, these algorithms have high computational complexity, and performs a convex optimization problem which do not scale well for large scale real life video surveillance data. Also, an integral component of these foreground detection algorithms is singular value decomposition which is nonrobust.\n\nIn this paper, we aim to introduce a scalable, fast, efficient and robust singular value decomposition technique based on the popular minimum density power divergence estimator which naturally extends to a background modelling algorithm. We call this algorithm rSVDdpd. We also demonstrate the superiority of our proposed algorithm on a benchmark dataset (Background Models Challenge or BMC Dataset) and a new real-life video surveillance dataset (University of Houston Camera Tampering or UHCT Dataset) in presence of camera tampering."
  },
  {
    "objectID": "research/rsvddpd.html#extended-abstract",
    "href": "research/rsvddpd.html#extended-abstract",
    "title": "Supplementary Material for rSVDdpd",
    "section": "",
    "text": "A basic algorithmic task in automated video surveillance is to separate background and foreground objects. Camera tampering, noisy videos, low frame rate, etc., pose difficulties in solving the problem. A general approach that classifies the tampered frames, and performs subsequent analysis on the remaining frames after discarding the tampered ones, results in loss of information. If the ultimate goal is to perform background modelling, foreground object detection or motion detection etc., then it would be useful to have an algorithm that robustly performs background foreground separation from video surveillance data. Thus, considerable effort has been expended to solve this problem.\n\nThere is a class of statistical background modelling that assumes that the intensity of the pixel values corresponding to the background content is distributed according to a known probability distribution (Gaussian or Mixture of Gaussian). These algorithms are fast, but their performances are not so great.\nBackground modelling based on deep neural network has also been performed. Usually a convolutional neural network with hundreds, thousands or millions of parameters with several layers of connection is built. These are fast for the inference, their performances are great, but they require an enormous amount of data and time for training.\nRecent approaches towards background modelling assumes a decomposition of the video data into a low rank matrix (corresponding to the background content) and a sparse or noisy matrix (corresponding to the foreground content). Matrix decomposition algorithms like robust PCA have been found to be useful here. However, these algorithms have high computational complexity, and performs a convex optimization problem which do not scale well for large scale real life video surveillance data. Also, an integral component of these foreground detection algorithms is singular value decomposition which is nonrobust.\n\nIn this paper, we aim to introduce a scalable, fast, efficient and robust singular value decomposition technique based on the popular minimum density power divergence estimator which naturally extends to a background modelling algorithm. We call this algorithm rSVDdpd. We also demonstrate the superiority of our proposed algorithm on a benchmark dataset (Background Models Challenge or BMC Dataset) and a new real-life video surveillance dataset (University of Houston Camera Tampering or UHCT Dataset) in presence of camera tampering."
  },
  {
    "objectID": "research/rsvddpd.html#codes",
    "href": "research/rsvddpd.html#codes",
    "title": "Supplementary Material for rSVDdpd",
    "section": "Codes",
    "text": "Codes\n\nThe proposed rSVDdpd algorithm is made available in an R package rsvddpd.\nA simple introductory tutorial for the package is also available in form of a vignette Introduction to rSVDdpd.\nSome of the sample codes are available here."
  },
  {
    "objectID": "research/rsvddpd.html#figures",
    "href": "research/rsvddpd.html#figures",
    "title": "Supplementary Material for rSVDdpd",
    "section": "Figures",
    "text": "Figures\nThe enlarged version of the figures presented in the paper are available here.\n\nBMC Dataset\nThe Background Modelling Challenger (BMC) Dataset is taken from http://backgroundmodelschallenge.eu/ containing some simulated video surveillance data with proper ground truth of foreground and background content. Here we demonstrate the original video, the ground truth mask along with the estimated foregrounds from different algorithms as follows.\n\nSVD: Singular Value Decomposition of the Video Data Matrix\nrSVDdpd: The proposed Robust SVD using Density Power Divergence Algorithm. Reference https://arxiv.org/abs/2109.10680\nRobust PCA: Robust Principal Component Analysis. Reference https://arxiv.org/abs/0912.3599.\nInexact ALM: Inexact Augmented Lagrangian Method for Robust PCA. Reference https://doi.org/10.1080/10556788.2012.700713.\nSRPCP: Sparse Regularized Principal Component Pursuit. Reference http://code.ucsd.edu/~pcosman/Liu_2017-151.pdf.\nVB: Variational Bayesian Method for Robust PCA. Reference https://doi.org/10.1109/TSP.2012.2197748.\nOP: Outlier Pursuit Algorithm. Reference https://doi.org/10.1109/TIT.2011.2173156.\nGoDec: Go Decomposition Algorithm. Reference https://dl.acm.org/doi/10.5555/3104482.3104487.\n\nFollowing are the descriptions of the videos present in the dataset along with the experimental results.\n\n\n\n\n\n\nImportant\n\n\n\nThe videos and the GIF have been compressed at an increased frame per second to save storage. Please convert it to a reduced frame per second video clip for better inspection.\n\n\n\n\n\n\nVideo\nVideo Background\nNoise\nLink\n\n\n\n\nVideo 112\nA street with moving cars\nNoiseless\nDownload\n\n\nVideo 122\nA rotary with moving cars\nNoiseless\nDownload\n\n\nVideo 212\nA street with moving cars\nSlight imperceptible noise in pixel values\nDownload\n\n\nVideo 222\nA rotary with moving cars\nSlight imperceptible noise in pixel values\nDownload\n\n\nVideo 312\nA street with moving cars\nVarying illumination due to movement of the sun\nDownload\n\n\nVideo 322\nA rotary with moving cars\nVarying illumination due to movement of the sun\nDownload\n\n\nVideo 412\nA street with moving cars\nCloud and fog for a brief period\nDownload\n\n\nVideo 422\nA rotary with moving cars\nCloud and fog for a brief period\nDownload\n\n\nVideo 512\nA street with moving cars\nWindy movement of the trees\nDownload\n\n\nVideo 522\nA rotary with moving cars\nWindy movement of the trees\nDownload\n\n\n\n\n\n\nUHCT Dataset\nUniversity of Houston Camera Tampering is a dataset collected from University of Houston‚Äôs surveillance video footage data for two consecutive days across two cameras in the UH campus. Some frames of the videos have been synthentically tampered with noisy artifacts to emulate camera tampering. The dataset is taken from UHCTD: A Comprehensive Dataset for Camera Tampering Detection.\n\n\n\n\nVideo\nDescription\nOriginal Video Link\nResults Link\n\n\n\n\nStream 1\nA small part of the video surveillance footage in Day 2 from Camera B in the UHCT Dataset. Some of the frames have been tampered with random image partially.\nView\nDownload\n\n\nStream 2\nA small part of the nighttime video surveillance footage in Day 1 from Camera A in the UHCT Dataset. Some of the frames have been tampered with random image partially.\nView\nDownload\n\n\nStream 3\nA small part of a daytime video surveillance footage in Day 1 from Camera A in the UHCT Dataset. Some of the frames have been tampered with random image partially. There is also presence of change in intensity and illumination due to the position of the sun.\nView\nDownload\n\n\nStream 4\nA small part of a daytime video surveillance footage in Day 2 from Camera A in the UHCT Dataset. After some time, the camera has been tampered by moving it to point in a different direction. A reliable background modelling algorithm should be sensitive to this rapid change in background and indicate camera tampering.\nView\nDownload"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Subhrajyoty Roy",
    "section": "",
    "text": "I am a Postdoctoral Researcher in the Department of Statistics and Data Science at Washington University in St.¬†Louis.\nYou may be interested in my Research or the open-source software tools I have created (or maintaining), or learning about me in general. I also have some notes here based on my understanding of various topics, including statistics, personal finance, and even my take on the philosophy of life.\nFor more formal reasons, my Curriculum Vitae may be more useful.\n\n\n Back to top"
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html",
    "href": "notes/technicals/uniform-consistency-proof.html",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "Let us assume that \\(f\\) be a function and \\(f_n\\) be its empirical counterpart (possibly stochastic). Let \\(x \\in \\chi\\) be some input variable, \\(\\chi\\) is some abstract space.\nSuppose using empirical process (or law of large numbers, etc.) we can somehow show that for every fixed \\(x \\in \\chi\\), \\[\n\\vert f_n(x) - f(x) \\vert = o_p(1)\n\\] However, we need a stronger result that establish this uniformly over all choices of \\(x \\in \\chi\\), i.e., we require \\[\n\\sup_{x \\in \\chi} \\vert f_n(x) - f(x)\\vert = o_p(1)\n\\]\nThere are few typical ways to approach this problem.\n\n\nThis approach aims to leverage some compactness-kind characterization of the space \\(\\chi\\). Suppose that \\(\\chi\\) is compact. However, that is not enough. To see this, note that if \\(\\chi\\) is compact, we can consider a finite subcover for this, and if we can show that the uniform convergence holds for every subcover, because there are only finitely many, we can take further supremum over them. Therefore, we need to show something like: \\[\n\\sup_{x \\in B(y, \\cdot)} \\vert f_n(x) - f(x)\\vert = o_p(1)\n\\] where \\(B(y, \\cdot)\\) is a ball in \\(\\chi\\) centered around \\(y\\). However, because \\(\\chi\\) is compact, it is also complete, and hence one can actually work with the Cauchy-sequences kind of thing. Therefore, to show the above, one version would be to work with \\[\n\\sup_{x \\in B(y, \\cdot)} \\vert f_n(x) - f_n(y)\\vert = o_p(1)\n\\] This is a version of equicontinuity, but for possibly random functions, hence called stochastic equicontinuity. Also, it needs to be hold only probabilistically for most \\(y\\)s, not for all.\nHere are the final assumptions.\n\n(A1) \\(\\chi\\) is compact.\n(A2) For every \\(\\epsilon, \\eta &gt; 0\\), there exists a random \\(\\Delta_n(\\epsilon, \\eta)\\) and constant \\(n_0(\\epsilon, \\eta)\\) such that for all \\(n \\geq n_0(\\epsilon,\\eta)\\), \\(\\mathbb{P}(\\vert \\Delta_n(\\epsilon, \\eta) \\vert &gt; \\epsilon) \\leq \\eta\\) and for all \\(x \\in \\chi\\), there exists a neighbourhood (open set) \\(N(x, \\epsilon, \\eta)\\) such that \\[\n\\sup_{y, y' \\in N(x, \\epsilon, \\eta)} \\vert f_n(y) - f_n(y') \\vert \\leq \\Delta_n(\\epsilon, \\eta), \\ n \\geq n_0(\\epsilon, \\eta)\n\\]\n\nUnder this assumptions, we can lift the pointwise convergence of \\(f_n(x)\\) to \\(f(x)\\) for each fixed \\(x \\in \\chi\\) to the uniform convergence over all \\(x \\in \\chi\\). The detailed proof can be found in (Newey 1991).\n\n\n\nAnother interesting technique when the space \\(\\chi\\) is not compact, is to use an \\(\\epsilon\\)-net type bounding argument.\nFirst note that, \\[\n\\mathbb{P}(\\sup_{x \\in \\chi} | f_n(x) - f(x)| &gt; \\delta) = \\mathbb{P}(\\cup_{x \\in \\chi} \\{ |f_n(x) - f(x)| &gt; \\delta \\})\n\\] One can now try to bound this by an union bound, i.e., by \\(|\\chi| \\sup_{x \\in \\chi} \\mathbb{P}(|f_n(x) - f(x) | &gt; \\delta)\\), which is typically \\(|\\chi| \\times o_p(1)\\). However, \\(\\chi\\) is generally an uncountable set, hence the above bound is useless. The strategy is to bound this set by some number of open balls which (like an open cover) and then control this number of open balls with the decay rate of the probability.\nTypically, there are 3 steps of this kind of proof.\n\n\nHere, we use Hoeffding‚Äôs inequality (if the stochastic functions \\(f_n\\) are uniformly bounded) or use Berstein‚Äôs inequality (or even a version of Talagrand‚Äôs inequality), which provides some sort of exponential concentration: For every \\(x \\in \\chi\\), we have \\[\n\\mathbb{P}(|f_n(x) - f(x)| &gt; \\delta) \\leq C e^{-c h(n, \\delta)}\n\\] where \\(h(n,\\delta)\\) is some function of \\(n\\) and \\(\\delta &gt; 0\\). The constant \\(C\\) and \\(c\\) are typically chosen to be independent of \\(x \\in \\chi\\).\n\n\n\nAt this stage, we try to build an \\(\\epsilon\\)-net cover for the space \\(\\chi\\). It means, to find a set \\(S \\subset \\chi\\) of representative points such that for any \\(x \\in \\chi\\), there exists \\(s' \\in S\\) such that \\(|x - s'| &lt; \\epsilon\\).\nLet \\(N_\\epsilon(\\chi)\\) be the \\(\\epsilon\\)-net covering number for the space \\(\\chi\\), which is the size of the smallest such \\(\\epsilon\\)-net set \\(S\\) for \\(\\chi\\). In general, \\(N_\\epsilon(\\chi)\\) can just be a bound on the cardinality of that smallest set, it does not need to be exactly calculated.\n\n\n\nNow for any fixed \\(x \\in \\chi\\), let \\(s \\in S\\) be its representative point in the \\(\\epsilon\\)-net. Then, \\[\n\\begin{align*}\n    | f_n(x) - f(x) |\n    & = | f_n(x) - f_n(s) - (f(x) - f(s)) + f_n(s) - f(s) |\\\\\n    & \\leq |f_n(x) - f_n(s)| + |f(x) - f(s)| + |f_n(s) - f(s)|\\\\\n    & \\leq \\sup_{|x - s| &lt; \\epsilon} |f_n(x) - f_n(s)| + \\sup_{|x - s| &lt; \\epsilon} |f(x) - f(s)| + |f_n(s) - f(s)|\n\\end{align*}\n\\] Often, \\(f_n\\) and \\(f\\) are nice continuous type functions (sometimes even Lipschitz) so that the first two terms are of \\(O(g(\\epsilon))\\) for some function \\(g(\\cdot)\\), and the constant is actually free of the choice of \\(x \\in \\chi\\). This means, \\[\n\\begin{align*}\n\\mathbb{P}(\\sup_{x \\in \\chi} |f_n(x) - f(x)| &gt; \\delta)\n& = \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; \\delta - g(\\epsilon))\\\\\n& \\leq \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; g'(\\delta))\n\\end{align*}\n\\] for some function \\(g'(\\cdot)\\) and appropriately chosen small \\(\\epsilon\\) as a function of \\(\\delta\\). What this step shows is that there is only a little error in replacing the difference between \\(f_n\\) and \\(f\\) at this general point \\(x \\in \\chi\\) to the same difference but now evaluated in one of the points in the \\(\\epsilon\\)-net.\nThis now helps us to write: \\[\n\\begin{align*}\n    \\mathbb{P}(\\sup_{x \\in \\chi} |f_n(x) - f(x)| &gt; \\delta)\n    & = \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; g'(\\delta))\\\\\n    & \\leq N_\\epsilon(\\chi) \\mathbb{P}(|f_n(s) - f(s)| &gt; g'(\\delta))\\\\\n    & \\leq N_\\epsilon(\\chi) \\times C e^{-c h(n, g'(\\delta))}\n\\end{align*}\n\\] Now putting the expression of \\(\\epsilon\\) in terms of \\(\\delta\\) and taking appropriate limit of \\(n\\) typically makes this bound tend to \\(0\\). Note that, here union bound makes sense as we have a finite-set (i.e., the \\(\\epsilon\\)-net).\nMore details can be found in (Erdogdu 2024) and (Vershynin 2018).\n\n\n\n\nAnother way to approach this kind of problem is to hope for a variational-type bound, where the space \\(\\chi\\) is too broad for the covering number to be useful, or the exponential concentration does not hold.\n\n\n\n\n\nErdogdu, Murat A. 2024. ‚ÄúCSC 2532: Statistical Learning Theory (Lecture 05 Notes).‚Äù https://erdogdu.github.io/csc2532/lectures/lecture05.pdf.\n\n\nNewey, Whitney K. 1991. ‚ÄúUniform Convergence in Probability and Stochastic Equicontinuity.‚Äù Econometrica 59 (4): 1161‚Äì67. https://doi.org/10.2307/2938179.\n\n\nVershynin, Roman. 2018. ‚ÄúHigh Dimensional Probability ‚Äî Lectures 23 and 24.‚Äù https://www.math.uci.edu/~rvershyn/teaching/hdp/hdp.html."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html#approach-stochastic-equicontinuity",
    "href": "notes/technicals/uniform-consistency-proof.html#approach-stochastic-equicontinuity",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "This approach aims to leverage some compactness-kind characterization of the space \\(\\chi\\). Suppose that \\(\\chi\\) is compact. However, that is not enough. To see this, note that if \\(\\chi\\) is compact, we can consider a finite subcover for this, and if we can show that the uniform convergence holds for every subcover, because there are only finitely many, we can take further supremum over them. Therefore, we need to show something like: \\[\n\\sup_{x \\in B(y, \\cdot)} \\vert f_n(x) - f(x)\\vert = o_p(1)\n\\] where \\(B(y, \\cdot)\\) is a ball in \\(\\chi\\) centered around \\(y\\). However, because \\(\\chi\\) is compact, it is also complete, and hence one can actually work with the Cauchy-sequences kind of thing. Therefore, to show the above, one version would be to work with \\[\n\\sup_{x \\in B(y, \\cdot)} \\vert f_n(x) - f_n(y)\\vert = o_p(1)\n\\] This is a version of equicontinuity, but for possibly random functions, hence called stochastic equicontinuity. Also, it needs to be hold only probabilistically for most \\(y\\)s, not for all.\nHere are the final assumptions.\n\n(A1) \\(\\chi\\) is compact.\n(A2) For every \\(\\epsilon, \\eta &gt; 0\\), there exists a random \\(\\Delta_n(\\epsilon, \\eta)\\) and constant \\(n_0(\\epsilon, \\eta)\\) such that for all \\(n \\geq n_0(\\epsilon,\\eta)\\), \\(\\mathbb{P}(\\vert \\Delta_n(\\epsilon, \\eta) \\vert &gt; \\epsilon) \\leq \\eta\\) and for all \\(x \\in \\chi\\), there exists a neighbourhood (open set) \\(N(x, \\epsilon, \\eta)\\) such that \\[\n\\sup_{y, y' \\in N(x, \\epsilon, \\eta)} \\vert f_n(y) - f_n(y') \\vert \\leq \\Delta_n(\\epsilon, \\eta), \\ n \\geq n_0(\\epsilon, \\eta)\n\\]\n\nUnder this assumptions, we can lift the pointwise convergence of \\(f_n(x)\\) to \\(f(x)\\) for each fixed \\(x \\in \\chi\\) to the uniform convergence over all \\(x \\in \\chi\\). The detailed proof can be found in (Newey 1991)."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html#approach-epsilon-net-bound",
    "href": "notes/technicals/uniform-consistency-proof.html#approach-epsilon-net-bound",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "Another interesting technique when the space \\(\\chi\\) is not compact, is to use an \\(\\epsilon\\)-net type bounding argument.\nFirst note that, \\[\n\\mathbb{P}(\\sup_{x \\in \\chi} | f_n(x) - f(x)| &gt; \\delta) = \\mathbb{P}(\\cup_{x \\in \\chi} \\{ |f_n(x) - f(x)| &gt; \\delta \\})\n\\] One can now try to bound this by an union bound, i.e., by \\(|\\chi| \\sup_{x \\in \\chi} \\mathbb{P}(|f_n(x) - f(x) | &gt; \\delta)\\), which is typically \\(|\\chi| \\times o_p(1)\\). However, \\(\\chi\\) is generally an uncountable set, hence the above bound is useless. The strategy is to bound this set by some number of open balls which (like an open cover) and then control this number of open balls with the decay rate of the probability.\nTypically, there are 3 steps of this kind of proof.\n\n\nHere, we use Hoeffding‚Äôs inequality (if the stochastic functions \\(f_n\\) are uniformly bounded) or use Berstein‚Äôs inequality (or even a version of Talagrand‚Äôs inequality), which provides some sort of exponential concentration: For every \\(x \\in \\chi\\), we have \\[\n\\mathbb{P}(|f_n(x) - f(x)| &gt; \\delta) \\leq C e^{-c h(n, \\delta)}\n\\] where \\(h(n,\\delta)\\) is some function of \\(n\\) and \\(\\delta &gt; 0\\). The constant \\(C\\) and \\(c\\) are typically chosen to be independent of \\(x \\in \\chi\\).\n\n\n\nAt this stage, we try to build an \\(\\epsilon\\)-net cover for the space \\(\\chi\\). It means, to find a set \\(S \\subset \\chi\\) of representative points such that for any \\(x \\in \\chi\\), there exists \\(s' \\in S\\) such that \\(|x - s'| &lt; \\epsilon\\).\nLet \\(N_\\epsilon(\\chi)\\) be the \\(\\epsilon\\)-net covering number for the space \\(\\chi\\), which is the size of the smallest such \\(\\epsilon\\)-net set \\(S\\) for \\(\\chi\\). In general, \\(N_\\epsilon(\\chi)\\) can just be a bound on the cardinality of that smallest set, it does not need to be exactly calculated.\n\n\n\nNow for any fixed \\(x \\in \\chi\\), let \\(s \\in S\\) be its representative point in the \\(\\epsilon\\)-net. Then, \\[\n\\begin{align*}\n    | f_n(x) - f(x) |\n    & = | f_n(x) - f_n(s) - (f(x) - f(s)) + f_n(s) - f(s) |\\\\\n    & \\leq |f_n(x) - f_n(s)| + |f(x) - f(s)| + |f_n(s) - f(s)|\\\\\n    & \\leq \\sup_{|x - s| &lt; \\epsilon} |f_n(x) - f_n(s)| + \\sup_{|x - s| &lt; \\epsilon} |f(x) - f(s)| + |f_n(s) - f(s)|\n\\end{align*}\n\\] Often, \\(f_n\\) and \\(f\\) are nice continuous type functions (sometimes even Lipschitz) so that the first two terms are of \\(O(g(\\epsilon))\\) for some function \\(g(\\cdot)\\), and the constant is actually free of the choice of \\(x \\in \\chi\\). This means, \\[\n\\begin{align*}\n\\mathbb{P}(\\sup_{x \\in \\chi} |f_n(x) - f(x)| &gt; \\delta)\n& = \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; \\delta - g(\\epsilon))\\\\\n& \\leq \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; g'(\\delta))\n\\end{align*}\n\\] for some function \\(g'(\\cdot)\\) and appropriately chosen small \\(\\epsilon\\) as a function of \\(\\delta\\). What this step shows is that there is only a little error in replacing the difference between \\(f_n\\) and \\(f\\) at this general point \\(x \\in \\chi\\) to the same difference but now evaluated in one of the points in the \\(\\epsilon\\)-net.\nThis now helps us to write: \\[\n\\begin{align*}\n    \\mathbb{P}(\\sup_{x \\in \\chi} |f_n(x) - f(x)| &gt; \\delta)\n    & = \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; g'(\\delta))\\\\\n    & \\leq N_\\epsilon(\\chi) \\mathbb{P}(|f_n(s) - f(s)| &gt; g'(\\delta))\\\\\n    & \\leq N_\\epsilon(\\chi) \\times C e^{-c h(n, g'(\\delta))}\n\\end{align*}\n\\] Now putting the expression of \\(\\epsilon\\) in terms of \\(\\delta\\) and taking appropriate limit of \\(n\\) typically makes this bound tend to \\(0\\). Note that, here union bound makes sense as we have a finite-set (i.e., the \\(\\epsilon\\)-net).\nMore details can be found in (Erdogdu 2024) and (Vershynin 2018)."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html#approach-radamacher-complexity-bound",
    "href": "notes/technicals/uniform-consistency-proof.html#approach-radamacher-complexity-bound",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "Another way to approach this kind of problem is to hope for a variational-type bound, where the space \\(\\chi\\) is too broad for the covering number to be useful, or the exponential concentration does not hold."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html#references",
    "href": "notes/technicals/uniform-consistency-proof.html#references",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "Erdogdu, Murat A. 2024. ‚ÄúCSC 2532: Statistical Learning Theory (Lecture 05 Notes).‚Äù https://erdogdu.github.io/csc2532/lectures/lecture05.pdf.\n\n\nNewey, Whitney K. 1991. ‚ÄúUniform Convergence in Probability and Stochastic Equicontinuity.‚Äù Econometrica 59 (4): 1161‚Äì67. https://doi.org/10.2307/2938179.\n\n\nVershynin, Roman. 2018. ‚ÄúHigh Dimensional Probability ‚Äî Lectures 23 and 24.‚Äù https://www.math.uci.edu/~rvershyn/teaching/hdp/hdp.html."
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "These are the various notes that I use to learn a new topic / material. These reflect my own understanding of various topics, as guided by the various resources I used to learn them.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n5 words\n\n\n\n\n\n\n\n\n\nStatistics\n\nDepedent Data\n\n\n\n\n\n\n\n\n\nJuly 12, 2025\n\n\n598 words\n\n\n\n\n\nNo matching items\n\nAlso take a look at some of my blogs posts as follows:\n\nAn eight-part tutorial series on Natural Language Processing is available at my substack.\nA six-part tutorial series on the basics of Generative AI is available at my substack.\nA seven-part tutorial series on the basics of Reinforcement Learning is available at my substack.\nA three part series on how to do natural language text processing using R.\n\nText mining in R.\nText Classification in R.\nChangepoint Analysis of Linguistics in R."
  },
  {
    "objectID": "notes/index.html#concept-guides",
    "href": "notes/index.html#concept-guides",
    "title": "Notes",
    "section": "",
    "text": "These are the various notes that I use to learn a new topic / material. These reflect my own understanding of various topics, as guided by the various resources I used to learn them.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n5 words\n\n\n\n\n\n\n\n\n\nStatistics\n\nDepedent Data\n\n\n\n\n\n\n\n\n\nJuly 12, 2025\n\n\n598 words\n\n\n\n\n\nNo matching items\n\nAlso take a look at some of my blogs posts as follows:\n\nAn eight-part tutorial series on Natural Language Processing is available at my substack.\nA six-part tutorial series on the basics of Generative AI is available at my substack.\nA seven-part tutorial series on the basics of Reinforcement Learning is available at my substack.\nA three part series on how to do natural language text processing using R.\n\nText mining in R.\nText Classification in R.\nChangepoint Analysis of Linguistics in R."
  },
  {
    "objectID": "notes/index.html#technical-materials",
    "href": "notes/index.html#technical-materials",
    "title": "Notes",
    "section": "Technical Materials",
    "text": "Technical Materials\nThese are like some of the useful proof techniques, some puzzles and interesting mathematical (or statistical) problems I have collected over the years.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nProof of Uniform Consistency\n\n\n\nProof Technique\n\nConsistency\n\n\n\n\n\n\n\n\n\nNovember 8, 2025\n\n\n1,009 words\n\n\n\n\n\n\n\nProof of Statistical Consistency of an Optimizer\n\n\n\nProof Technique\n\nConsistency\n\n\n\n\n\n\n\n\n\nApril 13, 2024\n\n\n277 words\n\n\n\n\n\nNo matching items\n\nAlso take a look at,\n\nPractice questions and solutions to various assignments of the B.Stat. programme at Indian Statistical Institute, Kolkata.\nPractice questions and solutions to various assignments of the M.Stat. programme at Indian Statistical Institute, Kolkata."
  },
  {
    "objectID": "notes/index.html#reflections",
    "href": "notes/index.html#reflections",
    "title": "Notes",
    "section": "Reflections",
    "text": "Reflections\nThese are my own way of understanding the ways of the world, again guided by various non-fiction books, podcasts and a few philosophy lessons.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/index.html#recommended-resources",
    "href": "notes/index.html#recommended-resources",
    "title": "Notes",
    "section": "Recommended Resources",
    "text": "Recommended Resources\nInternet is full of interesting things. Here‚Äôs a few that I like:\n\nSeeing Theory. http://seeing-theory.brown.edu/ - Interactive Visualization of Introduction of Probability and Statistics.\nThe Idea behind Explorable Explorations: https://blog.ncase.me/explorable-explanations/. And here‚Äôs the list of such explanations: https://explorabl.es/.\nAlexander Bogomolny‚Äôs post on the Area of Circle by Rabbi Abraham bar Hiyya Hanasi - Probably the best proof that I have seen for showing that the area of a circle is half of the radius times the circumference.\nThe Mathematics of Juggling - Allen Knutson."
  },
  {
    "objectID": "notes/concepts/robust-stat.html",
    "href": "notes/concepts/robust-stat.html",
    "title": "Robust Statistics",
    "section": "",
    "text": "Robust statistics notes coming soon.\n\n\n\n Back to top"
  }
]