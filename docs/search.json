[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Open Source Softwares",
    "section": "",
    "text": "Below is a curated collection of software, packages, and tools I have developed or contributed to ‚Äî resulting from various academic and personal projects.\n\n\n\n\nhandanim A Python library for programmatically generating hand-drawn sketch-style animations.\nIdeal for instructional videos, visual explanations, and stylized presentations.\n\nüìò Docs: https://subroy13.github.io/handanim\nüíª GitHub: https://github.com/subroy13/handanim \n\n\n\n\n\n\n\n\nImportant\n\n\n\nI am actively looking for collaboration in the handanim project. If building hand-drawn sketch-style animations videos using reproducible Python code sounds interesting to you, please reach out to me by email! or create an issue in Github\n\n\n\n\ndecompy A Python package implementing several robust matrix decomposition algorithms, supporting outlier-resistant SVD/PCA pipelines.\n\nüì¶ PyPI: https://pypi.org/project/decompy\n\nüíª GitHub: https://github.com/subroy13/decompy\n \n\nroufcp Robust gradual changepoint detection in time series using Rough-Fuzzy set theory. Useful for weak or soft transitions where classic CPD fails.\n\nüì¶ PyPI: https://pypi.org/project/roufcp\n\nüíª GitHub: https://github.com/subroy13/roufcp\n \n\nrsvddpd Robust Singular Value Decomposition using the Minimum Density Power Divergence Estimator (MDPDE).\n\nüì¶ CRAN: https://cran.r-project.org/web/packages/rsvddpd/index.html\nüìò Vignette: https://cran.r-project.org/web/packages/rsvddpd/vignettes/rSVDdpd-intro.html\nüíª GitHub: https://github.com/subroy13/rsvddpd\n | [1] ‚Äú‚Äù\n\ncallgrind-reader A TypeScript library to parse callgrind-like profiling output, extract metrics, and integrate into analysis tools.\n\nüì¶ npm: https://www.npmjs.com/package/callgrind-reader\n\n \n\nmathaddons.sty A LaTeX package for writing research papers containing multiple useful mathematical commands. Click here to download."
  },
  {
    "objectID": "software.html#software-tools-packages",
    "href": "software.html#software-tools-packages",
    "title": "Open Source Softwares",
    "section": "",
    "text": "Below is a curated collection of software, packages, and tools I have developed or contributed to ‚Äî resulting from various academic and personal projects.\n\n\n\n\nhandanim A Python library for programmatically generating hand-drawn sketch-style animations.\nIdeal for instructional videos, visual explanations, and stylized presentations.\n\nüìò Docs: https://subroy13.github.io/handanim\nüíª GitHub: https://github.com/subroy13/handanim \n\n\n\n\n\n\n\n\nImportant\n\n\n\nI am actively looking for collaboration in the handanim project. If building hand-drawn sketch-style animations videos using reproducible Python code sounds interesting to you, please reach out to me by email! or create an issue in Github\n\n\n\n\ndecompy A Python package implementing several robust matrix decomposition algorithms, supporting outlier-resistant SVD/PCA pipelines.\n\nüì¶ PyPI: https://pypi.org/project/decompy\n\nüíª GitHub: https://github.com/subroy13/decompy\n \n\nroufcp Robust gradual changepoint detection in time series using Rough-Fuzzy set theory. Useful for weak or soft transitions where classic CPD fails.\n\nüì¶ PyPI: https://pypi.org/project/roufcp\n\nüíª GitHub: https://github.com/subroy13/roufcp\n \n\nrsvddpd Robust Singular Value Decomposition using the Minimum Density Power Divergence Estimator (MDPDE).\n\nüì¶ CRAN: https://cran.r-project.org/web/packages/rsvddpd/index.html\nüìò Vignette: https://cran.r-project.org/web/packages/rsvddpd/vignettes/rSVDdpd-intro.html\nüíª GitHub: https://github.com/subroy13/rsvddpd\n | [1] ‚Äú‚Äù\n\ncallgrind-reader A TypeScript library to parse callgrind-like profiling output, extract metrics, and integrate into analysis tools.\n\nüì¶ npm: https://www.npmjs.com/package/callgrind-reader\n\n \n\nmathaddons.sty A LaTeX package for writing research papers containing multiple useful mathematical commands. Click here to download."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html",
    "href": "notes/concepts/aws_cloud_basics.html",
    "title": "Introduction of AWS Cloud Computing",
    "section": "",
    "text": "Cloud computing is the on-demand delivery of IT resources over the Internet with pay-as-you-go pricing model.\nThere are 4 aspects in this definition.\n\nOn demand delivery - As per your business needs, get delivery of IT resources instantly, without any additional contract or any heads up to AWS\nIT resources - Whatever business you have, almost all kinds of IT resource that you need would be available\nOver the internet - Provides those resources to you from anywhere in the world (across the globe) to wherever you want to access it to.\nPay as you go pricing - You pay for only what you use, nothing more, nothing less.\n\nAmazon Web Service (AWS) offers various services, popular categories include Compute, Storage, Networking, Security, Machine Learning Models, etc."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#what-is-cloud-computing",
    "href": "notes/concepts/aws_cloud_basics.html#what-is-cloud-computing",
    "title": "Introduction of AWS Cloud Computing",
    "section": "",
    "text": "Cloud computing is the on-demand delivery of IT resources over the Internet with pay-as-you-go pricing model.\nThere are 4 aspects in this definition.\n\nOn demand delivery - As per your business needs, get delivery of IT resources instantly, without any additional contract or any heads up to AWS\nIT resources - Whatever business you have, almost all kinds of IT resource that you need would be available\nOver the internet - Provides those resources to you from anywhere in the world (across the globe) to wherever you want to access it to.\nPay as you go pricing - You pay for only what you use, nothing more, nothing less.\n\nAmazon Web Service (AWS) offers various services, popular categories include Compute, Storage, Networking, Security, Machine Learning Models, etc."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#amazon-compute",
    "href": "notes/concepts/aws_cloud_basics.html#amazon-compute",
    "title": "Introduction of AWS Cloud Computing",
    "section": "Amazon Compute",
    "text": "Amazon Compute\nThere are a few services that falls under the category of Amazon Compute. Among them, the simplest one is ‚ÄúAmazon Elastic Compute Cloud (EC2)‚Äù\nEC2 is a virtualization tool, which creates virtual cores on top of the physical cores of a physical machine. So, it might be that, when you are using EC2 compute, you are sharing the same physical machine with another EC2 instance.\nThere are 5 primary types of EC2 instances available.\n\nGeneral Purpose - These instances provide a balance between computing, storage and network requirements. Used for any general purpose.\nCompute optimized - High performance computing machines. May be used for high performance computations, batch processing, dedicated gaming servers, etc.\nMemory optimized - Used for high internal memory like high RAM, workloads requiring manipulation of large data in memory\nStorage optimized - Used for high iops with the persistent disk or storage, used for distributed file system applications, high frequency transaction writings, etc.\nAccelerated computing - Has GPU or other acceleration devices. Used for numerical computing or training ML or DL algorithms.\n\nEC2 has 5 different types of instances available based on the pricing models available.\n\nOn demand instances - The usual pricing model\nDedicated instances - Does not share a physical host with another instance, runs on a separate physical server.\nReserved Instances - You go for a contract with AWS for 1 year or 3 year commitment and get to use a predetermined number of EC2, then you can get high discounts.\n\nYou can go with standard reserve rates - where you specify the machine type, availability zone, os, etc at the time of purchasing reserved rates.\nYou can go with convertible reserve rates - where you don‚Äôt specify these things.\n\nEC2 instance savings plan - You go for a contract with AWS for 1 year or 3 year commitment, but here you specify the number of hours of monthly usage you will do. Unlike reserved instance, you don‚Äôt need to specify availability zones, ec2 instance types, or OS used.\nSpot Instances are ideal for workloads with flexible start and end times, or that can withstand interruptions. Spot Instances use unused Amazon EC2 computing capacity and offer you cost savings at up to 90% off of On-Demand prices.\n\nAfter you have launched a Spot Instance, if capacity is no longer available or demand for Spot Instances increases, your instance may be interrupted. This might not pose any issues for your background processing job\n\n\n\nAmazon EC2 Scalability\nScalability involves beginning with only the resources you need and designing your architecture to automatically respond to changing demand by scaling out or in.\n\nScale up = Meaning making the current EC2 instance more powerful\nScale out = Increasing the number of EC2 instance.\n\nYou can add automatic scaling capacity by using AWS EC2 Auto Scaling.\n\nThere are two types of scaling:\n\nDynamic scaling - Changing the scaling as per demand.\nPredictive scaling - Changing the number of EC2 as per the prediction of demand ahead of time.\n\nFor enabling auto scaling, you specify 3 things.\n\nMinimum capacity = Minimum number of instances you want to run always..\nDesired capacity = On average what would be the number of instances. Defaults to minimum capacity.\nMaximum capacity = Maximum number of instances you want to run in case demand reaches it peak.\n\n\n\nEven though you have 10 servers running, if there are 10 requests coming in, it does not mean everything gets routed uniformly. Some server may be clogged up with 4 requests at random.\nTo counter this problem, we need Load Balancing.\n\n\nAWS Elastic Load Balancer (ELB)\nIt routes the incoming traffic properly into proper server in an ASG group with the least amount of load.\n\n\n\nMessaging and Queuing\nImagine the frontend is talking to the backend. The ELB is one option to handle the traffic properly. However, if backend process is clogged that means customer is experiencing bad UX, since frontend is also waiting for the response for the same.\nThe way to tackle that is to decouple the architecture. There are 2 ways to do that.\n\nUsing Amazon Simple Queue Service (SQS) - It provides a queue buffer to hold your messages, as long as you want, unless you process or delete it.\n\n\n\nAmazon Simple Notification Service (SNS) - It provides a way to broadcast your request to some specific topics (or channels). The servers listen to those channels (topics) and processes things from these channels one by one.\n\nIn this case, we have a network of servers, every server is a frontend and every server is a backend.\n\n\n\n\n\nAWS Lambda\nIt is a computer service provided by AWS that allows you to run code, without thinking about the underlying infrastructure of the server where it is running. This is hence called ‚Äúserverless‚Äù.\n\nYou just select the runtime of the code you want to run, infrastructure with the compile engine or programming languages are auto installed.\nYour code can be setup to trigger from other events happening in AWS.\nCode can run a maximum of 15 minutes.\nYou pay on the basis of (runtime x the memory usage)\n\n\n\n\nAWS Container Services\n\nContainers are special types of packaged software that package the OS, dependencies, software and code completely.\nAWS has 2 types of container services if you want to run containerized applications.\n\nAWS Elastic Container Service (ECS) - If you want to run Docker\nAWS Elastic Kubernetes Service (EKS) - If you want to run Kubernetes\n\nFor each, you can choose to:\n\nManage your own servers using ASG with a group of EC2 instances.\nAsk AWS to manage everything, using a serverless framework called AWS Fargate .\n\nThis means it is just like Lambda, you only bring your code, AWS takes care of managing and provisioning the instances."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#aws-global-infrastructure",
    "href": "notes/concepts/aws_cloud_basics.html#aws-global-infrastructure",
    "title": "Introduction of AWS Cloud Computing",
    "section": "AWS Global Infrastructure",
    "text": "AWS Global Infrastructure\nIn order to make sure disasters or any kind of physical activities at a data center does not compromise the safety of your company‚Äôs data / IT resource in AWS, AWS provides an infrastructure that scales globally.\nAcross the globe, AWS has different regions, which are geographically separated areas. Examples include us-west-1 (California), us-west-2 (Ohio), Singapore, Australia, Mumbai, etc.\n\nWhat should you consider when selecting a region for your IT infra?\n\nCompliance requirements - If the business you operate in or the country you operate in, requires the customer‚Äôs data never to leave a country‚Äôs boundary, then you have to use specific region.\nProximity - For low latency, you should choose the regions that are closer to your customer bases.\nServices available - Not all AWS services are available everywhere, so check and make sure the region you are using, satisfy all requirements of IT infra.\nCost - Another aspect is that regions has different costs for the same infra, due to the various factors such as availability of physical chips and other things, tax expenditure, datacenter rent fees, etc.\n\nWithin every region, AWS has three or more availability zones (AZ). Each availability zone is a single or a group of data centers.\nThese availability zones are usually tens of miles apart (so that there is a less chance of a natural disaster striking both simultaneously), while they are connected through high speed network to provide one-hundredth of a millisecond latency.\n\nNote: AWS recommends that once you choose a region, you should replicate your IT infra in at least 2 AZs (as a disaster recovery plan).\n\n\nCaching and Data Delivery\nIf your customers are scattered across the world, then only having your IT resource to the regions or AZs is not enough for the latency. For instance, a customer sitting in Malaysia requesting some document from your applications, gets served from Singapore region, which may be hundreds of miles away.\nTo reduce the latency, AWS has ‚Äúedge locations‚Äù, some small scale setup compared to a full fledged data center. These are scattered across the globe in almost all locations.\nThese locations run a service called ‚ÄúAWS CloudFront‚Äù which is a CDN (Content Delivery Network) service that caches the data. The region or AZ where your data resides puts the cached data to the edge location, and when customer requests the data, it shows it from cache (does not need to travel entire distance to the region)\nAlso, if you have a specific data center or on-premise data center available where you must host your infra, but you want AWS to manage them, you can request service from ‚ÄúAWS Outposts‚Äù. This means AWS guys will come and install the necessary infra in your data center, they will maintain it, but they will charge a custom pricing for the same.\n\n\nProvisioning Infrastructure / IT resources from Cloud\nTo provision the IT resources that you need from AWS, internally everything uses AWS API. To get access to these AWS standardized APIs, you can interact with them in 3 major ways.\n\nAWS Management Console - An web-based UI to manage IT resource provisioning\nAWS Command Line Interface - A CLI tool to do the same using your terminal\nAWS SDK - Do the same using programming from your code written in various programming languages\n\nAWS also has their own services that helps you do this.\n\nAWS Elastic Beanstalk - It is a PaaS service, you provide only your code and configuration settings like how much memory and CPU you need.\n\nIt takes care of launching EC2, inside ASG group.\nAdjust capacity and other settings.\nAdd ELB for load balancing\nConnect cloudwatch for logging and health monitoring, etc.\n\nAWS CloudFormation - With AWS CloudFormation, you can treat your infrastructure as code. This means that you can build an environment by writing lines of code(meaning configs in YAML or JSON files) instead of using the AWS Management Console to individually provision resources.\n\nIt ensures that you can use the same configuration YAML or JSON file and redeploy / build your infra exactly as it is in another region.\nIt deploys all resources mentioned in the config parallely, so very fast for deployment."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#aws-networking",
    "href": "notes/concepts/aws_cloud_basics.html#aws-networking",
    "title": "Introduction of AWS Cloud Computing",
    "section": "AWS Networking",
    "text": "AWS Networking\nAWS delivers every IT resource over the Internet. So, security is a concern, since you don‚Äôt want everyone using the internet to access the IT resources you are paying for.\nSo you create a virtual private boundary of the IT resources you have, this is called ‚ÄúAmazon Virtual Private Cloud‚Äù (Amazon VPC). It is a networking service that you can use to establish boundaries around your AWS resources.\nWithin each VPC, you can have multiple logical groups of resources that share networking boundaries. These groups are called ‚Äúsubnets‚Äù (sub-inter-nets)\nNow, by default, your VPC does not allow any traffic. But, you might have 2 use cases:\n\nYou have a web server inside your VPC. You want the public to be able to access that VPC and request your website content.\n\nFor this, you need to attach an Internet Gateway to the VPC.\nThis is like a door to the office room (VPC) where you have your business running.\n\n\n\n\nYou might have only privately secured instances running inside the VPC and you don‚Äôt want the public to access that. However, you want your employees to be able to access that.\n\nSo basically you have a Virtual Private Network (VPN) for your employees.\nAnd you can use the VPN to connect to the VPC.\nFor this connection, you need a Private Gateway attached to the VPC.\nIt encrypts your requests and protects them from bad guys.\n\n\n\nWhen you use Private Gateway, you still use the same optical fibre cable (physical hardware) that others are using, so you do not get any bandwidth benefit, except security.\nIf you want more bandwidth (and exclusive connection between your AWS resources in VPC to your on-premise network), you need to use Amazon DirectConnect. This requires AWS partner to go and physically install a special optical fibre cable from AWS connection to directly to your on-premise network.\n\n\nSubnet Access Controls\nA subnet is a logical group of AWS resources sharing a common network.\nSubnets are of 2 types:\n\nPublic subnets contain resources that need to be accessible by the public, such as an online store‚Äôs website.\nPrivate subnets contain resources that should be accessible only through your private network, such as a database that contains customers‚Äô personal information and order histories.\n\nTo differentiate between subnets and VPC, consider the following:\n\nIn a VPC, subnets can communicate with each other. For example, you might have an application that involves Amazon EC2 instances in a public subnet communicating with databases that are located in a private subnet.\nDifferent VPCs cannot communicate with each other, this means, VPC‚Äôs are isolated environments of resources that do not know the existence of each other.\n\nA packet is a unit of data sent over the internet or a network.\nWhen you send/receive a packet to a subnet, at the subnet level, there is a check. These checks are done using Network Access Control Lists (Network ACL). The default ACL allows all inbound and outbound traffic.\n\nInbound traffic = The packets that want to come into the subnet.\nOutbound traffic = The packets that want to go outside the subnet.\n\nNote that, the Network ACLs are stateless, so they don‚Äôt remember whether a packet has already been sent. It always checks against the list.\n\nIf you want instance-level networking control, you need to use AWS Security Groups (SG). This can be applied to a single EC2 instance or a small group of instances.\nCompared to Network ACL, there are 3 major differences.\n\nIt is stateful, so it remembers that a packet has been sent.\n\n\n\nBy default, it denies all inbound traffic.\nIt does not check the packet for outbound traffic. Compared to that, ACL performs checks for both inbound and outbound traffic.\n\nIn summary, we have the following system.\nVPC has multiple subnets. Each subnet has multiple security groups. Each SG can have one or more (usually one) EC2 instances.\n\n\n\nDomain Management\nFor domain management, AWS has a tool called AWS Route 53. It is a DNS service that takes the name of the website like ‚Äúmywebsite.com‚Äù and uses its address book to find the server‚Äôs IP address ‚Äú234.23.435.11‚Äù\n\nWhen a customer requests a file by typing the website address, AWS uses Route 53 to look up its DNS address. Then it searches the same file in CloudFront edge locations if the file is present in CDN, if not found, the request goes to the ELB and then to the ASG or EC2 instance."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#aws-storage-services",
    "href": "notes/concepts/aws_cloud_basics.html#aws-storage-services",
    "title": "Introduction of AWS Cloud Computing",
    "section": "AWS Storage Services",
    "text": "AWS Storage Services\nWhen you work with an EC2 instance, you get some amount of memory by default. This is the harddisk or SSD attached to the physical host of the machine where the EC2 virtual server is running. This means, the storage is not permanently accessible and is not guaranteed to be persistent between sessions. When you stop the EC2 instance and start another EC2 instance, it may boot up in some other host, hence none of the data that you saved temporarily would be available. Also, AWS performs cleanups of these disks, so it may get deleted as well.\nTo ensure persistent storage, there are different kinds of storages that you can use.\n\nBlock Level Storage\n\nThis kind of storage is architectured as a set of blocks of bytes. When you modify the data in this storage (i.e., edit some files), then the block of bytes which were modified only those are updated.\nThis is similar to the usual harddisk kind of storage we are familiar with.\n\nObject Level Storage\n\nThis storage stores items as Objects, so there is no block.\nEach stored item has 3 things: The object to store, its metadata and a key.\nEvery time you update the data, the entire object data is updated with the help of this key.\n\nFile Level Storage\n\nThis is a block storage\nBut additionally provide shared permissions, like two or multiple compute unit can access the data simultaneously over a network\nThis is a truly linux like file system based storage, you will have your /mnt, /etc, /var, etc paths available.\n\nDatabase\n\nStores special kinds of data that needs additional capabilities of querying and analyzing the data\n\nData Lake\n\nA dump of data storage for historical data,\nUsually these are immutable, but constantly growing in size\nYou need to run business analytics on these.\n\n\nAWS has a solution for each of the type of storage that you need based on your use case.\n\nElastic Block Storage\nElastic Block Storage (EBS) is a simple block storage service, that you can attach to your EC2 instances. You can write or read from it, and the data persists even if your EC2 instance is terminated.\nTo create an EBS volume, you define the configuration (such as volume size and type) and provision it. After you create an EBS volume, it can attach to an Amazon EC2 instance.\n\nEBS only attaches to a single EC2 instance.\nEBS is available only in a single availability zone.\nEBS does not scale automatically, you must define the volume size in its configuration.\n\nHowever, after you create EBS, you can modify its storage size and resize this.\n\n\nAlso, you can take EBS snapshot for make a backup of your EBS volume, in case you need to restore back. EBS snapshot stores only incremental data (i.e., the blocks that are modified or are newly added)\n\n\n\nElastic File Storage\nElastic File Storage (EFS) is a file system based storage service. In comparison to EBS, it has following features:\n\nEFS can be attached to multiple EC2 instances.\nEFS is replicated across multiple AZ\n\nSo within same region, you can access EFS by an EC2 instance from any AZ\n\nOn-premise IT resource can access EFS using AWS DirectConnct\nEFS scales automatically based on the amount of storage you are using and the scaling policy you have in place.\n\n\n\nAmazon Simple Storage Service\nAWS Simple Storage Service (AWS S3) is an object level storage. In object storage, each object consists of data, metadata, and a key.\n\nThe data might be an image, video, text document, or any other type of file.\nMetadata contains information about what the data is, how it is used, the object size, and so on.\nAn object‚Äôs key is its unique identifier.\n\n\nThe maximum file size that you can upload in S3 is 5TB. It has potentially unlimited storage.\nYou first define the storage partition (called buckets) where you want to store these objects. At each bucket level, you can define its access permissions. Objects residing in a bucket inherits its access permissions.\nAlso, using S3 you can have versioning, for each object. Hence for any object, you can do a point-in-time restore to go back to any version that you want. This is possible since it is object level storage, one does not need to revert each incremental changes to restore, one can simply modify the entire object at once.\nS3 has different storage classes depending on:\n\nThe availability of your data\nHow often you plan to retrieve your data\nHow much cost you want to bear.\nS3 Standard (default)\n\nDesigned for frequently accessed data\nStores data into a minimum of 3 AZs, hence 99.(9 many 9)% available (very high reliability).\n\nS3 Standard - Infrequent Access (IA)\n\nSimilar to S3 Standard, but lower storage price and higher retrieval price\n\nS3 One Zone - Infrequent Access\n\nMuch lesser cost\nStores data into a single AZ\n\nS3 Intelligent Tiering\n\nHas a small monitoring/subscription kind of fee.\nIntelligently moves the objects from standard to standard IA if you have not accessed the file in the last 30 days.\nOnce you retrieve from IA, again moves back to standard.\n\nS3 Glacier Instant Retrival\n\nArchival storage\nBut require immediate access, and retrieval within milliseconds\n\nS3 Glacier Flexible Retrieval\n\nArchival storage\nRetrieval takes between 1 to 12 hours. (several hours)\n\nS3 Glacier Deep Archive\n\nArchival storage\nRetrieval takes about 12 hours to 48 hours.\n\nS3 Outposts\n\nCreates S3 buckets in your on-premise AWS outpost environment.\n\n\nYou can also have a lifecycle policy that will automatically move your data from one tier to another, based on the rules you specify.\n\n\nAmazon Database Services\n\nAmazon Relational Database Service (RDS) is a fully managed relational data service.\n\nAutomates tasks such as hardware provisioning, database setup, patching, and backups.\nProvides SQL engine with popular backend for Postgresql, MySql, MariaDB, Oracle, SQL Server\nAmazon also provides its own engine called ‚ÄúAmazon Aurora‚Äù. It is an enterprise-class RDMS compatible with MySql and Postgresql and much faster than their standard versions.\n\nIt is also highly available, provides 6 copies of data across 3 AZs and runs continuous backups to S3.\n\n\nAmazon DynamoDB is a nonrelational database.\n\nYou can key - value pair data.\nIt has very low latency, millisecond latency for fetch and put.\nIt is serverless, performs automatic scaling based on the amount of your data.\nYou dont need to manage anything, just create table and put the data.\n\nAmazon Redshift is a data lake (i.e., the data warehousing) service that you can use to store historical data, that needs less modification but require very sophistical business analytics query running.\n\nAlso, use it if your data is growing day by day.\n\nAmazon DocumentDB is a NoSql document / text based storage, like MongoDB.\nAmazon Neptune is a graph DB service.\n\nYou can use Amazon Neptune to build and run applications that work with highly connected datasets, such as recommendation engines, fraud detection, and knowledge graphs.\n\nAmazon Quantum Ledge Database (QLDB) is a ledger database service, you can use for accounting or finance services. The data here is immutable, hence may be used reliably for auditing purposes.\nAmazon Managed Blockchain is a blockchain based service for AWS. Blockchain is a distributed ledger system that lets multiple parties run transactions and share data without a central authority.\nAmazon ElastiCache is a service that adds caching layer for the RDS to improve the read times of requests.\n\nIt supports Redis and MemCache.\n\nAmazon DynamoDB Accelerator (DAX) is an in-memory cache for Dynamodb. It improves dynamodb reading performance, used for very critical low latency applications.\n\nAmazon also has a database migration service (AWS DMS) which you can use to migrate relational databases.\n\nYou select a source database and a target database\nThe source and target DB can have the same type or different types.\n\nIf they are the same type, it is easy. Same as copying the schema, data types and functions.\nIf they are of different types, there is a transformation layer that converts the schema and the SQL functions from one SQL to another SQL.\nDuring migration, it does not put too much load on the source so that your source DB remains operational, so there is no downtime.\n\nOther use cases of DMS include:\n\nCreate development or test DB identical to production\nDatabase consolidation =&gt; where you copy data from multiple DB into a single DB\nContinuous replication"
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#amazon-security",
    "href": "notes/concepts/aws_cloud_basics.html#amazon-security",
    "title": "Introduction of AWS Cloud Computing",
    "section": "Amazon Security",
    "text": "Amazon Security\nAWS security relies on a principle of shared responsibility model. Basically it means,\n\nAWS is responsible for the ‚Äúsecurity of the cloud‚Äù. This means, security of the datacenter, network, EC2 or other infrastructures, etc.\nCustomers are responsible for the ‚Äúsecurity in the cloud‚Äù. This means, whatever OS you use in your EC2 instances, the application and the data security.\n\n\n\nAWS Identification and Authentication Management (IAM)\nIn AWS, you have different users and each user can have different level of access in your AWS provided IT resources.\n\nRoot User is the user who have all access to all of your IT resources in your AWS account.\n\nAWS recommends that you enable Multi Factor Authentication (MFA) in your root user and also provide a strong password for the login.\n\nIAM User is an user that you create in AWS using AWS IAM Portal. You can provide granular level scope or permission access to the resources this user can use.\nIAM Policy is a document that you use to specify the AWS API access a particular group or user will have.\n\n\nIt has 3 things inside.\n\nEffect: Which is either ‚ÄúAllow‚Äù or ‚ÄúDeny‚Äù\nAction: Which is the name of the API\nResource: The specific resource ARN (Amazon Resource Number) where you can use this API.\n\nFor example, the above policy tells that ‚ÄúList Object permission in s3 bucket AWSDOC-EXAMPLE-BUCKET is allowed.‚Äù\n\nIAM group is a group of users to which you can attach a policy for access management. It means, all user in the same group inherits those permission.\n\n\n\nIAM role is a collection of IAM policy, which you can assume to temporarily gain access to certain level of permission. For example, in an organization, you may have roles for different departments, and each role has different responsibilities due to which they want access to only a set of things in AWS. You can create then role level permission only. Then, as your users change from one role to another, their permission sets can accordingly vary.\n\n\n\nAWS Organizations\nIf you have multiple AWS accounts, (say for different environments such as development and production or testing), you can centrally manage them in a portal called AWS Organization.\nThese include 3 major features:\n\nYou can centrally control the access specifications using Service Control Policies (SCP). This means you can control in each account what all AWS services your users can use.\nYou can have consolidated billing and corresponding recommendations in one single place.\nYou can organize the AWS accounts into groups based on their functioning, and nest them into Org Units (OUs).\n\n\n\n\nCompliance Reports\nBased on the industry you operate in, you may require auditing to be performed. You can access AWS security and compliance reports on-demand for the same from AWS Artifacts.\nAWS Artifacts has 2 kinds of reports.\n\nSuppose that your company needs to sign an agreement with AWS regarding your use of certain types of information throughout AWS services. You can do this through AWS Artifact Agreements.\nNext, suppose that a member of your company‚Äôs development team is building an application and needs more information about their responsibility for complying with certain regulatory standards. You can advise them to access this information in AWS Artifact Reports.\n\nAWS also has a Customer Compliance Center which contains various resources about AWS compliance. In the Customer Compliance Center, you can read customer compliance stories to discover how companies in regulated industries have solved various compliance, governance, and audit challenges.\nYou can also access compliance whitepapers and documentation on topics such as:\n\nAWS answers to key compliance questions\nAn overview of AWS risk and compliance\nAn auditing security checklist\n\nAdditionally, the Customer Compliance Center includes an auditor learning path. This learning path is designed for individuals in auditing, compliance, and legal roles who want to learn more about how their internal operations can demonstrate compliance using the AWS Cloud.\n\n\nDistributed Denial of Service (d-Dos) Attacks\nDenial of Service attacks basically means to bombard your server with unreasonable requests which ends up denying the server usage for valid users of the service.\nFor instance, a particular user can flood lots of spam requests to your server. This means, your server will be processing these spam requests while your actual customer requests won‚Äôt go through. A simple solution to this is blocking the IP address of the spamming user.\nHowever, a carefully planned attack is distributed, which uses some random machines on the internet to perform this Dos attack unknowingly. In this case, you cannot block a singled out IP address.\n\n\nAWS solves this problem by using a service called AWS Shield. It tracks the usage patterns of different requests to understand and flag malicious requests.\n\nThis has two offerings. Standard and Advanced.\nAWS Shield Standard automatically protects all AWS customers at no cost. It protects your AWS resources from the most common, frequently occurring types of DDoS attacks.\nAWS Shield Advanced is a paid service that provides detailed attack diagnostics and the ability to detect and mitigate sophisticated DDoS attacks. It also integrates with other services such as Amazon CloudFront, Amazon Route 53, and Elastic Load Balancing. Additionally, you can integrate AWS Shield with AWS WAF by writing custom rules to mitigate complex DDoS attacks.\n\n\nAnother kind of d-Dos attack is to perform a really slow request (basically the malicious user pretends to have a really crappy network connection). In this case, while your server is processing that request, it cannot tend to other requests which might be in line.\n\nIn this case, AWS has ELB which automatically directs the other good traffic to other servers in the ASG.\n\nAlso note that ELB is a region level infrastructure. So, it means it can scale and accept the other requests from valid customers. In this case, the scale of AWS is a huge advantage since it is expensive to overwhelm region level AWS resource.\n\n\nOne more kind of d-Dos attack uses weather API. Basically, the malicious user asks a question about the weather at some location to the weather API and the return address is specified as your server address. So, the weather API gets the weather details of the location, and floods your server with unreasonable data (which it never asked in the first place).\n\nSince weather API uses a different protocol compared to https, it is blocked by the security group of the EC2 instance.\n\n\n\nEncryption Service\nSince it is customer‚Äôs responsibility to protect and secure the data they have in AWS, AWS offers a solution that helps customer to do this, by encrypting the data ‚Äúin transit‚Äù (when it is going as packet from one part of your IT infra to another) and ‚Äúat rest‚Äù (when it is residing in S3 or EBS).\nAWS Key Management Service (AWS KMS) enables you to perform encryption operations through the use of cryptographic keys. A cryptographic key is a random string of digits used for locking (encrypting) and unlocking (decrypting) data. You can use AWS KMS to create, manage, and use cryptographic keys. You can also control the use of keys across a wide range of services and in your applications\n\n\nFirewall\nSimilar to the security groups, subsets and VPC, AWS also offers a firewall service called AWS WAF (Web Application Firewall) to protect your web application or monitor all network requests in your application.\nSimilar to network ACL, it checks against as web access control (WAC) list to filter out some requests. It integrates with ELB and CloudFront.\nYou can use it to block access from certain IP address, certain regional IP address, etc. Helps you prevent against d-Dos attacks.\n\n\nMore Security Applications\nAWS Inspector is a service that performs automated security and compliance assessments of your web application and your infra. It checks applications for security vulnerabilities and deviations from security best practices, such as open access to Amazon EC2 instances and installations of vulnerable software versions. However, it provides a list of vulnerabilities and recommendations to fix them, but it is customer‚Äôs responsibility to know how to fix them as per the shared responsibility model.\nAWS GuardDuty is a service that continuously monitor your infrastructure, network activity and intelligently detect threats using help of machine learning. It uses data from your VPC logs as well as the DNS logs and analyzes them.\n\nIf GuardDuty detects any threats, you can review detailed findings about them from the AWS Management Console. Findings include recommended steps for remediation. You can also configure AWS Lambda functions to take remediation steps automatically in response to GuardDuty‚Äôs security findings."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#aws-logging-and-monitoring",
    "href": "notes/concepts/aws_cloud_basics.html#aws-logging-and-monitoring",
    "title": "Introduction of AWS Cloud Computing",
    "section": "AWS Logging and Monitoring",
    "text": "AWS Logging and Monitoring\nFor logging and monitoring, AWS has offered a service called Cloudwatch. Cloudwatch tracks your application logs and various metrics on your application health.\nCloudWatch uses metrics to represent the data points for your resources. AWS services send metrics to CloudWatch. CloudWatch then uses these metrics to create graphs automatically that show how performance has changed over time.\nCloudWatch has multiple interesting features:\n\nCloudwatch provides a consolidation logging feature from your EC2 instances, you can filter and view logs.\nIt has Cloudwatch Alarms which is something like a notification service that automatically sends you alerts if some metrics are over or under a predefined threshold.\n\nFor example, if your EC2 CPU usage is under say 5%, it can sends an alarm to a SNS topic, asking you to may be stop the instance since it is not being used.\n\nCloudwatch Dashboard feature enables you to look at health and metrics of all your AWS resources from a single central location.\n\n\nAnother logging and monitoring offering from AWS is Amazon CloudTrail. It is a service that logs every API request made to AWS for auditing purposes. The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, and more. You can think of CloudTrail as a ‚Äútrail‚Äù of breadcrumbs (or a log of actions) that someone has left behind them. This means, you can view an entire history of your cloud interactions (or interactions of any user) by applying proper filters.\n\nEvents are typically updated in CloudTrail within 15 minutes after an API call.\n\nWithin CloudTrail, you can also enable CloudTrail Insights. This optional feature allows CloudTrail to automatically detect unusual API activities in your AWS account. For example, CloudTrail Insights might detect that a higher number of Amazon EC2 instances than usual have recently launched in your account. You can then review the full event details to determine which actions you need to take next."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#aws-pricing",
    "href": "notes/concepts/aws_cloud_basics.html#aws-pricing",
    "title": "Introduction of AWS Cloud Computing",
    "section": "AWS Pricing",
    "text": "AWS Pricing\n\nFree Tier and Cost Optimization\nAWS has a free tier. It comes in 3 forms.\n\nSome things are always free. Like the first 1 million lambda calls every month.\n12 months free. Like specific amounts of S3 storage.\nTrials. This trial period differs service by service, and usually ranges between 30-day trial to 90 day-trial periods.\n\nAWS costs are built on 3 principles.\n\nPay exactly for what you use.\nPay less when you reserve, like reserved EC2 instances.\nPay less when you use more with volume-based discounts. More storage in S3 allows you to quality for more discounts.\n\nYou can take advantage of this by using AWS organizations and enabling consolidated billing. In this case, even if your individual accounts are not eligible for the discounts, the discounts may be application all accounts taken together.\n\n\nFor example, suppose the above is your S3 usage in your 3 accounts. By using AWS organizations, your total exceeds the 10TB limit and you get high discount for the additional 4TB data stored.\nYou can use the AWS Pricing Calculator to get an estimate of your cost, and plan your resources accordingly before deploying them in the Cloud.\n\n\nAWS Cost Management Tools\nAWS has various cost management tools as follows:\n\nBilling Dashboard, provides a consolidated view of your usages and billing, and future month forecasts.\n\n\n\nAccess AWS Budgets which can help you plan your service usages, and send you alerts when your usage or forecasted usage goes beyond what is set.\n\nIt updates 3 times a day.\n\n\n\n\nFinally, AWS Cost explorer helps you create reports and drill down and do interactive analysis on your costs for your AWS account."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#aws-support",
    "href": "notes/concepts/aws_cloud_basics.html#aws-support",
    "title": "Introduction of AWS Cloud Computing",
    "section": "AWS Support",
    "text": "AWS Support\n\nAWS Trusted Advisor\nThis is a system provided by AWS that monitors your AWS infrastructure in real time and provides recommendations about how you can improve them, in accordance with the industry and AWS best practices. It performs these automated checks in 5 main categories:\n\nCost Optimization (are you paying for unnecessary things that you are not utilizing, like ec2 instances not being used)\nFault tolerance (is your application protected against AZ failures?)\nPerformance (is your application latency okay, ways you can improve performance, are you using CloudFront for CDN?)\nSecurity (are you using proper VPC and security groups?)\nService Limits (are you close to the service limits of AWS, for example, in a particular region, you can have at most 5 VPC in your account. Are you close to that?)\n\nThese recommendations are provided to you in form of a dashboard and checklist with recommendation actions to perform.\n\nHere,\n\nGreen checks mean no problem.\nOrange triangle means you need to do some investigation based on your use-case.\nThe red circle means recommendation actions you need to take ASAP.\n\n\n\nAWS Support Plans\nAWS has 5 different categories of Support Plans\n\nBasic support is free and enabled for all AWS customers.\n\nIt includes access to AWS docs, whitepapers and other support communities.\nYou can contact AWS for billing questions and an increase of service limits.\nYou have access to only limited AWS Trusted Advisor checks.\n24/7 access to customer support.\n\nDeveloper support is a paid monthly subscription plan that enables:\n\nBest practice guidance.\nUnlimited AWS technical support\nBuilding block architecture support, guidance on AWS offerings, features, etc.\nEmail access to customer support with 24-hour response time.\n\nBusiness support enables:\n\nAccess to use-case specific guidance.\nAll AWS Trusted advisor checks.\nSupport response time of 48 hours.\nLimited support on third-party software installation in EC2 or other application stack components.\nDirect phone access to customer support, 4 hours response time normally, and 1 hour response time if your production system is down or impaired.\n\nEnterprise On-Ramp support includes:\n\nA concierge support team for billing and account assistance.\nCost optimization workshop every year.\nA pool of Technical Account Managers (TAM) to provide guidance and coordinate access to programs and AWS experts.\n30 minutes or less response time for business-critical issues.\n\nEnterprise Support includes:\n\nTraining and game days to drive innovation.\nA designated TAM.\n15 minutes or less response time for business-critical issues.\n\n\n\n\nAWS Marketplace\nAWS Marketplace is a digital catalog that includes thousands of software listings from independent software vendors. You can use AWS Marketplace to find, test, and buy software that runs on AWS.\nThe following are broad categories of 3rd party softwares available on AWS marketplace."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#cloud-migration",
    "href": "notes/concepts/aws_cloud_basics.html#cloud-migration",
    "title": "Introduction of AWS Cloud Computing",
    "section": "Cloud Migration",
    "text": "Cloud Migration\n\nCloud Adoption Framework (CAF)\nIf you have your IT resources are set up on-premise, and you need to migrate that to the AWS cloud, AWS has a framework to help you get started. This is called AWS Cloud Adoption Framework.\nAt the highest level, AWS CAF organizes the cloud migration guidance into 6 different perspectives in your organization. Each perspective addresses distinct responsibilities.\n\nBusiness perspective ensures that the IT aligns with the business needs and the investments towards cloud adoption in IT link to the key business metrics / results.\n\nUsed to create a strong business case for the cloud adoption, why it is needed and ensures that cloud offerings align with your business goals.\n\nPeople perspective ensures that your organization employees has the necessary technical skills for this cloud adoption. It evaluates the organizational structure, the different roles and identify gaps between skills and works on staffing / training as needed.\nThe governance perspective focuses on understanding how to update staff skills and processes necessary to ensure business governance in the cloud. It focuses on changes in processes to align IT strategy on cloud adoption to align with business strategy.\nPlatform perspective includes strategy for principles and patterns for implementing/architecting new solutions on the cloud.\n\nSolution architects need to understand the relationships between IT systems and understand how cloud can bring value.\n\nSecurity perspective ensures that the organization meets the necessary security standards for visibility, audibility, control and agility.\nOperations perspective helps you to enable, run, use, operate and recover IT workloads at the agreed-upon level with your business stakeholders.\n\nFor a better understanding, please check out the whitepaper.\nhttps://docs.aws.amazon.com/pdfs/whitepapers/latest/overview-aws-cloud-adoption-framework/overview-aws-cloud-adoption-framework.pdf\nNow that you started your migration journey, there are 6 strategies that you can use to migrate your IT workload from on-premise to a cloud.\n\nRehosting or ‚ÄúLift and Shift‚Äù involves moving the application without any change to cloud. You just host your code entirely in EC2 instance.\nReplatforming or ‚ÄúLife, Tinker and Shift‚Äù involves making a few cloud optimizations without changing the core architecture or any code of your application. Like instead of using legacy DB by managing your own EC2 server, you can adopt the AWS provided RDS.\nRefactoring or ‚Äúre-architecting‚Äù involves completely redesign your application using cloud optimized architectures. It should be driven by strong business needs.\nRepurchasing involves moving from a traditional licensing software to a SaaS model, (may be provided by a cloud)\nRetaining consists of keeping applications that are either critical to be moved to cloud, so some code / application which may be deprecated within few months. In this case, there is no tangible benefit to cloud migration for these applications, so they are retained in source environment.\nRetiring means the process of removing applications that are no longer needed, so no need to migrate.\n\n\n\nAWS Snow Family\nImagine you have about 1PB of data present in your on-premise data center. If we want to move the data to AWS S3 using a direct connect bandwidth with 1GBps speed, it will take 100 days to move the data.\nToo much!\nSo, AWS has certain physical devices that can be shipped to you, you connect it to your data center, copy the data and send that device back to AWS. There are 3 options for these devices.\n\n\nAWS Snowcone is a small, rugged, and secure edge computing and data transfer device. It features 2 CPUs, 4 GB of memory, and up to 14 TB of usable storage.\nAWS Snowball offers two types of devices:\n\nSnowball Edge Storage Optimized devices are well suited for large-scale data migrations and recurring transfer workflows, in addition to local computing with higher capacity needs.\n\nStorage: 80 TB of hard disk drive (HDD) capacity for block volumes and Amazon S3 compatible object storage, and 1 TB of SATA solid state drive (SSD) for block volumes.\nCompute: 40 vCPUs, and 80 GiB of memory to support Amazon EC2 sbe1 instances (equivalent to C5).\n\nSnowball Edge Compute Optimized provides powerful computing resources for use cases such as machine learning, full motion video analysis, analytics, and local computing stacks.\n\nStorage: 80-TB usable HDD capacity for Amazon S3 compatible object storage or Amazon EBS compatible block volumes and 28 TB of usable NVMe SSD capacity for Amazon EBS compatible block volumes.\nCompute: 104 vCPUs, 416 GiB of memory, and an optional NVIDIA Tesla V100 GPU. Devices run Amazon EC2 sbe-c and sbe-g instances, which are equivalent to C5, M5a, G3, and P3 instances.\n\n\nAWS Snowmobile is an exabyte-scale data transfer service used to move large amounts of data to AWS. You can transfer up to 100 petabytes of data per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi trailer truck.\n\n\n\nAWS Well-Architected Framework\nThe AWS Well-Architected framework helps you understand how to design and operate reliable, secure, efficient and cost-effective systems in the AWS Cloud.\nReference: AWS Well-Architected Framework\nIt is based on 6 pillars.\n\n\nOperational excellence is the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures.\n\nDesign principles for operational excellence in the cloud include performing operations as code, annotating documentation, anticipating failure, and frequently making small, reversible changes.\n\nThe security pillar is the ability to protect information, systems and assets.\nThe reliability pillar is the ability of a system to recover from infra disruptions, and dynamically perform scaling to meet demand.\nPerformance efficiency is the ability to use the computing resources efficiently.\n\nExamples include using serverless architecture whenever possible.\nDesigning systems to be global in minutes.\n\nCost optimization is the ability to run systems to deliver business value at the lowest price possible\nSustainability is the ability to maximize utilization, reduce energy consumption, meet sustainability goals, reduce downstream impact of your cloud workloads."
  },
  {
    "objectID": "notes/concepts/aws_cloud_basics.html#other-aws-services",
    "href": "notes/concepts/aws_cloud_basics.html#other-aws-services",
    "title": "Introduction of AWS Cloud Computing",
    "section": "Other AWS Services",
    "text": "Other AWS Services\nAWS has offerings related to AI models. For example:\n\nConvert speech to text with Amazon Transcribe\nDiscover patterns in text with Amazon Comprehend\nIdentify fraud transactions / online activities with Amazon Fraud Detector.\nBuild voice and text chatbots with Amazon Lex.\nUse foundational LLM as an API with Amazon Bedrock\nBuild, train and deploy customized ML models using Amazon Sagemaker\nAmazon Augmented AI (A2I) allows you to conduct a human review of ML systems to guarantee precision.\nRead and parse text from image or PDF using Amazon Textract.\nUse AWS Kendra to build a search functionality.\nUse AWS Polly to convert text to voice.\nUse AWS Translate for language translation services.\nAmazon Athena is a serverless querying system to query your extracted data catalogue from S3 using simple SQL like statements.\nAWS Glue is an ETL (extract, transform and load) service which discovers, prepares and moves the data from multiple sources (amazon S3, Dyanmodb, etc.) to the specific data lake or warehousing system.\n\n\n\nAWS Data Exchange is a tool to find, subscribe to and use third-party data in the cloud.\nAmazon EMR (Elastic MapReduce) is a big data analytics platform and create petabyte-scale big data applications.\nAmazon Kinesis is a tool to process and analyze streaming data with low latencies.\n\nKinesis Data Streams is a serverless streaming data service that captures streaming data, and sends to EC2 / business analysis software.\nKinesis Data Firehose is an ETL service to capture and send data to storage (S3, redshift, opensearch, etc.)\nSimilar to Kinesis Data Streams, but specialized for video processing.\n\nAmazon QuickSight is a business analytics service. Create dashboards, visualizations, and embedded analytics in reports.\nAmazon SES (Simple Email Service) provides reliable low-cost emailing solutions using SES API.\nAWS Billing Conductor is a tool to simplify and customize reporting in the bills using organization groups or groups of AWS accounts.\nAWS Batch is a fully managed service that lets you run batch computing workloads at any scale.\nAWS Lightsail is like Elastic Beanstalk but for small business applications, with pre-configured environments. Build and personalise your blog, e-commerce or personal website.\nAWS Local Zones: Run applications that require single-digit millisecond latency or local data processing by bringing AWS infrastructure closer to your end users and business centers.\n\nCompared to CloudFront, it is more capable allowing the AWS infra to be deployed, rather than only caching.\n\nAWS Wavelength is a 5G computing service that embeds AWS compute and storage services within 5G networks, providing mobile edge computing infrastructure for developing, deploying, and scaling ultra-low-latency applications.\n\n\nThis is useful in cars (automatic cars) which can connect to the internet in the 5G web area and connect to AWS computing for the ML models.\n\nAmazon Elastic Container Registry (Amazon ECR) is AWS‚Äôs own dockerhub, a repository to store container images.\nCustomer Engagements:\n\nAWS Activate for Startups is an initiative to help startups reduce their AWS bills.\nAWS IQ is a community platform to search AWS certified professionals.\nAWS Managed Services (AMS) helps you adopted AWS as scale and operate more efficiently and securely.\nAWS Sales Support for sales requests\nAWS Compliance support for support related to audit and compliance.\nAWS Technical support for service related technical issues.\n\nUnavailable under the basic support plan.\n\nBilling or Account support for assistance with billing-related queries.\n\nAWS Systems Manager and AWS AppConfig helps creating dynamic configurations for softwares / applications to change behaviour quickly without deployment.\nAWS Cloud9 is a cloud based IDE, lets you code using a browser.\nAWS Cloudshell is a cloud based shell tool for working with AWS CLI commands.\nAWS Codeartifact is a software package management tool (like pypi or npm)\nAWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests and produces ready-to-deploy software applications.\nAWS CodeDeploy is a service that automates the deployments to different compute platforms, like EC2, AWS lambda, ECS, etc.\nAWS X-ray is a debug tool that traces user requests through your application and provides insights into performance, security and ocst.\nAWS Health Dashboard\n\nYour personal account health dashboard shows events that impacts your services in all AWS regions.\nAWS Health Dashboard (without login) - shows all public events that impacts all regions of AWS reach.\n\nAWS Config to track auditing changes for a particular AWS resource over time.\nThe AWS Well-Architected Tool is designed to help you review the state of your applications and workloads against architectural best practices, identify opportunities for improvement, and track progress over time."
  },
  {
    "objectID": "notes/concepts/aws_data_analytics.html",
    "href": "notes/concepts/aws_data_analytics.html",
    "title": "AWS Cloud Data Analytics Guide",
    "section": "",
    "text": "There are a few primary definitions:\n\nAnalysis is detailed examination of something in order to understand its nature of determine its essentials features / properties.\nData Analysis is the process of compiling, processing and analyzing data to perform the analysis, and use the analysis extracted properties to make a business decision.\n\nMeta analysis is, in contrast, analysis performed through collection, processing and analyzing existing researches.\n\nAnalytics is a systematic analysis of the data (i.e., a framework of how to perform data analysis)\n\nIt tells you what to look for\nWhat step should you do next\nHow to ensure the next step you are doing is scientifically valid and meaningful, so that logical conclusion can be drawn.\n\nData Analytics is the specific analytical process (i.e., one particular choice of framework) which is being applied.\n\nWe should give a clearer definition here.\n\nData Analysis is to analyze the data to derive meaningful insights from the data. It works one a single dataset.\nData Analytics is broader, it performs multiple data analysis techniques on different data, explore their relationships and finally provides some business value.\n\nThus, an effective data analytics solution should combine 3 main features:\n\nScalability: Analyze small or vast data sets of different types.\nSpeed: Analysis performed in near real time or fast as new data arrives.\nGoal: Analysis should be good enough to be able to derive business decisions from and yield high value returns.\n\nThus, a component of data analysis solution require 4 parts.\n\n\nIngest or Collect the data in various forms.\nStore the data as necessary.\nProcess or analyze the data in fast speed.\nConsume the analyzed or processed data in terms of query / dashboard / reports to provide business value or insights.\n\n\n\nData Analytics come in 4 different forms.\n\nDescriptive - It is an exploratory data analysis part, performs summarization using charts, etc. It answers the question ‚ÄúWhat happened?‚Äù\nDiagnostic - It aims to answer the question ‚ÄúWhy it happened?‚Äù. It looks for past historical data, and compares and finds patterns and disparities, and performs correlation analysis.\nPredictive - It aims to answer ‚ÄúWhat might happen in future?‚Äù. Basically, it uses time series analysis to discover historical trend, and predict the future trends.\nPrescriptive - It recommends actionable insights to the stakeholders based on the predictive analytics results, it answers questions like ‚ÄúWhat should we do to make the future look better?‚Äù\n\n\n\n\nAs businesses begin to implement data analysis solutions, challenges arise. These challenges are based on the characteristics of the data and analytics required for their use case. These challenges come in various forms, in five Vs.\n\nVolume: The data could be extremely large.\nVariety: The data may come in various forms such as structured, semi-structured or unstructured form, from different sources.\nVelocity: The data must be process fast with low latency, in near real time.\nVeracity: The data must be validated for accuracy, and any outlier must be removed or handled properly. The solution should be able to fix the errors if possible.\nValue: The processed and analyzed data must be able to provide business insights and value for decision making.\n\nMost of the machine learning system, focuses only on the veracity anhd value part. But in AWS, we should be able to provide a comprehensive treatment using cloud machineries.\n\nNot all organizations experience challenges in every area. Some organizations struggle with ingesting large volumes of data rapidly. Others struggle with processing massive volumes of data to produce new predictive insights. Still, others have users that need to perform detailed data analysis on the fly over enormous data sets. Before beginning your data analysis solution, you must first check which of these 5 Vs are present in the business problem and design your solution accordingly."
  },
  {
    "objectID": "notes/concepts/aws_data_analytics.html#what-is-data-analytics",
    "href": "notes/concepts/aws_data_analytics.html#what-is-data-analytics",
    "title": "AWS Cloud Data Analytics Guide",
    "section": "",
    "text": "There are a few primary definitions:\n\nAnalysis is detailed examination of something in order to understand its nature of determine its essentials features / properties.\nData Analysis is the process of compiling, processing and analyzing data to perform the analysis, and use the analysis extracted properties to make a business decision.\n\nMeta analysis is, in contrast, analysis performed through collection, processing and analyzing existing researches.\n\nAnalytics is a systematic analysis of the data (i.e., a framework of how to perform data analysis)\n\nIt tells you what to look for\nWhat step should you do next\nHow to ensure the next step you are doing is scientifically valid and meaningful, so that logical conclusion can be drawn.\n\nData Analytics is the specific analytical process (i.e., one particular choice of framework) which is being applied.\n\nWe should give a clearer definition here.\n\nData Analysis is to analyze the data to derive meaningful insights from the data. It works one a single dataset.\nData Analytics is broader, it performs multiple data analysis techniques on different data, explore their relationships and finally provides some business value.\n\nThus, an effective data analytics solution should combine 3 main features:\n\nScalability: Analyze small or vast data sets of different types.\nSpeed: Analysis performed in near real time or fast as new data arrives.\nGoal: Analysis should be good enough to be able to derive business decisions from and yield high value returns.\n\nThus, a component of data analysis solution require 4 parts.\n\n\nIngest or Collect the data in various forms.\nStore the data as necessary.\nProcess or analyze the data in fast speed.\nConsume the analyzed or processed data in terms of query / dashboard / reports to provide business value or insights.\n\n\n\nData Analytics come in 4 different forms.\n\nDescriptive - It is an exploratory data analysis part, performs summarization using charts, etc. It answers the question ‚ÄúWhat happened?‚Äù\nDiagnostic - It aims to answer the question ‚ÄúWhy it happened?‚Äù. It looks for past historical data, and compares and finds patterns and disparities, and performs correlation analysis.\nPredictive - It aims to answer ‚ÄúWhat might happen in future?‚Äù. Basically, it uses time series analysis to discover historical trend, and predict the future trends.\nPrescriptive - It recommends actionable insights to the stakeholders based on the predictive analytics results, it answers questions like ‚ÄúWhat should we do to make the future look better?‚Äù\n\n\n\n\nAs businesses begin to implement data analysis solutions, challenges arise. These challenges are based on the characteristics of the data and analytics required for their use case. These challenges come in various forms, in five Vs.\n\nVolume: The data could be extremely large.\nVariety: The data may come in various forms such as structured, semi-structured or unstructured form, from different sources.\nVelocity: The data must be process fast with low latency, in near real time.\nVeracity: The data must be validated for accuracy, and any outlier must be removed or handled properly. The solution should be able to fix the errors if possible.\nValue: The processed and analyzed data must be able to provide business insights and value for decision making.\n\nMost of the machine learning system, focuses only on the veracity anhd value part. But in AWS, we should be able to provide a comprehensive treatment using cloud machineries.\n\nNot all organizations experience challenges in every area. Some organizations struggle with ingesting large volumes of data rapidly. Others struggle with processing massive volumes of data to produce new predictive insights. Still, others have users that need to perform detailed data analysis on the fly over enormous data sets. Before beginning your data analysis solution, you must first check which of these 5 Vs are present in the business problem and design your solution accordingly."
  },
  {
    "objectID": "notes/concepts/aws_data_analytics.html#volume",
    "href": "notes/concepts/aws_data_analytics.html#volume",
    "title": "AWS Cloud Data Analytics Guide",
    "section": "Volume",
    "text": "Volume\nOne of the first things we require for performing data analysis is storage. Global data creation is projected to grow to 180 zettabytes (1000 GB = 1 petabyte, 1000PB = 1 exabyte, 1000 EB = 1 zetabyte) by 2025.\nThere are usually 3 kinds of data you will have in terms of storage burdening.\n\nTransactional data - Very important, like your user details, customer details, purchases, etc.\nTemporary data - moves you make in a game, website scrolling data, Browser cache, etc.\nObjects - Emails, text messages, social media content, videos, etc. that you cannot store as transactional, but require an object level storage.\n\nThere are mainly 3 types of data in terms of storage schema.\n\nStructured data is organized and are stored in the form of rows and columns of tables.\n\nExamples are Relational databases\n\nSemi-structured data is like a key-value pair, with proper groupings / tags inside a file.\n\nExamples include CSV, XML, JSON, etc.\n\nUnstructured data is data without a specific consistent structure.\n\nLogs files\nText files\nAudio or video, etc.\n\n\nMost of the data present in business is unstructured.\n\n\nAWS S3\nThe most popular, versatile solution for storage in AWS is Simple Storage Service (S3). AWS S3 is a file system like object storage. Basically, it is like a key-value store:\n\nKeys are the paths of the file / object.\nValue is the object data itself.\n\nEvery put call to the existing overlapping key will update the entire object at once, with no block level updation.\nS3 is highly available and highly durable storage.\n\nIt has unlimited scalability.\nIt is natively online, accessed over HTTP requests, so multiple machines can access the data simultaneously.\nAWS provides built in encryption security.\n99.(9 many 9s)% durability.\n\nIn AWS S3, there are a few concepts.\n\nBuckets are the first concept. It is like system disk. Each bucket may contain different objects and are used for different purposes. The access pattern of the objects are configured at the bucket level.\nIn each object stored inside a bucket, there is an associated metadata. The metadata is simply the prefix (the folder path) in the bucket, the namee of the object (or the object key) and a version number.\n\n\nAn object key is the unique identifier for an object in a bucket. Because the combination of a bucket, key, and version ID uniquely identifies each object, you can think of Amazon S3 as a basic data map between ‚Äúbucket + key + version‚Äù and the object itself. Every object in Amazon S3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and (optionally) version.\nUsing S3 provides many benefits, few are:\n\nIt makes the storage decoupled from the processing / compute nodes.\nIt is a centralized place for all of your data.\nAWS provides built in integration with clusterless and serverless computing.\nS3 has a standard API using which you can access objects like network requests.\nBuilt in security and encryption.\n\n\n\nTypes of Data Stores\nIn Data Analytics literature, there are a few types of data stores based on their purpose.\n\nData Lake is the storage of data where you can store unstructured, structured or semi-structured data for almost everything. Basically all of your business application dumps their data into a data lake. Starting from data lake, we add ingestion and other processing logic to convert this data into something useful.\n\nA data lake is a centralized repository that allows you to store structured, semistructured, and unstructured data at any scale.\nIt gives you a single source of truth for any data.\nThe data lake should also contain an index or tags for the data, producing a catalogue for all data that exists in the data lake.\nTo implement data lake, one can use AWS S3.\n\nIntegration with AWS Glue can provide the metadata and cataloguing service for all data present in S3.\nOne can also use AWS LakeFormation (currently in preview) to set up a secure data lake in days. It also performs analysis using Machine Learning to understand the structure / format of the data, and create schema or field description for cataloguing purpose.\n\n\n\n\nData lakes are an increasingly popular way to store and analyze both structured and unstructured data. If you want to build your own custom Amazon S3 data lake, AWS Glue can make all your data immediately available for analytics without moving the data.\n\nData Warehouse is a central repository to contain all your structured data from many different sources. These data are processed and ready to be ingested to business reporting tools and decision making.\n\nThis data is kept transformed, cleaned, aggregated and prepared beforehand using data analytics tools.\nThe data is structured here, using a relational database. Hence, before storing the data, one must define the schema and the constraints on the stored data.\nThese are kept in an efficient way to minimize read operations and deliver query results at blazing speeds to thousands of users concurrently.\nYou can use AWS Redshift for a data warehouse solution in AWS.\n\nIt is 10x faster than existing other data warehouse solutions.\nEasy to setup, deploy and manage.\nAWS provides built in security.\n\n\n\n\n\nData Mart is a subset of data warehouse specific to a particular department or part of your organization. This is a restricted access subset of your data warehouse. Small transformations are allowed for creation of data mart.\n\n\n\n\n\n\n\n\n\n\nCharacteristics\nData Warehouse\nData Lake\n\n\n\n\nData\nRelational data from transactional systems, operational databases, and line-of-business applications\nRelational and non-relational data from IoT devices, websites, mobile apps, social media, and enterprise applications\n\n\nSchema\nDefined before loading (schema-on-write)\nApplied during analysis (schema-on-read)\n\n\nPrice / Performance\nFastest query performance using higher-cost storage\nImproving query performance using low-cost storage\n\n\nData Quality\nHighly curated, serving as the trusted single source of truth\nAny data, raw or curated\n\n\nUsers\nBusiness analysts\nData scientists, data engineers, and analysts (with curated data)\n\n\nAnalytics\nBatch reporting, BI, dashboards, and visualizations\nMachine learning, predictive analytics, data exploration, and profiling\n\n\n\nAmazon EMRFS\nAmazon EMR provides an alternative to HDFS: the EMR File System (EMRFS). EMRFS can help ensure that there is a persistent ‚Äúsource of truth‚Äù for HDFS data stored in Amazon S3. When implementing EMRFS, there is no need to copy data into the cluster before transforming and analyzing the data as with HDFS. EMRFS can catalogue data within a data lake on Amazon S3. The time that is saved by eliminating the copy step can dramatically improve the performance of the cluster."
  },
  {
    "objectID": "notes/concepts/aws_data_analytics.html#velocity",
    "href": "notes/concepts/aws_data_analytics.html#velocity",
    "title": "AWS Cloud Data Analytics Guide",
    "section": "Velocity",
    "text": "Velocity\nWhen businesses need rapid insights from the data collected, they have a velocity requirement.\nData Processing in the context of the velocity problem means two things.\n\nData collection must be rapid so that systems can ingest high volume of data or data streams rapidly.\nData Processing must be fast to provide insights for the required level of latency.\n\n\nTypes of Data Processing\nThere are 2 major types of data processing.\n\nBatch processing - Refers to the system when the data collection may be done separately from the data analysis tasks. Once a certain amount of data is collected, then only data analysis tasks kick in.\n\nScheduled Batch processing - means it runs on a specific regular schedule, and has predictable workloads.\nPeriodic Batch processing - means it runs only when a certain amount of data is collected, which may not be on a regular interval. So the workload is unpredictable.\n\nStream processing - Refers to the system when data collection and analysis is usually coupled. The insights must be delivered very fast as the data is consumed.\n\nReal-time - Insights must be delivered within milliseconds, like autonomous cars.\nNear real-time - Insights must be delivered within minutes.\n\n\n\n\n\n\n\n\n\n\n\nBatch Data Processing\nStream Data Processing\n\n\n\n\nData scope\nProcesses all or most of the data in a dataset\nProcesses data in a rolling time window or only the most recent records\n\n\nData size\nLarge batches of data\nIndividual records or micro-batches of a few records\n\n\nLatency\nMinutes to hours\nSeconds or milliseconds\n\n\nAnalysis\nComplex analytics\nSimple response functions, aggregates, rolling metrics\n\n\n\n\n\nDistributed Processing\nThere are many distributed processing frameworks. The most popular ones are\n\nApache Hadoop\nApache Spark\n\n\nApache Hadoop\nApache Hadoop is a distributed computing system that is designed on the principle of delegating the data processing workload to several servers, and workload configuration managed by a single master server. Each of these servers are called nodes.\nAt its core, Hadoop implements a specialized file system called Hadoop Distributed File System (HDFS), that stores data across multiple nodes, making redundant data replication to make it failsafe.\n\nIn Hadoop, most of the servers are configured as Data Nodes, these process and stores the data. And the few nodes will be NameNodes, these are where the memory management happens, it stores the maps of where the actual data resides.\nAny client connects the NameNode for file metadata or file modifications.\nBut performs the I/O of actual file updation in DataNodes.\nLet‚Äôs say, a client issues a write request to Name Node. The node performs initial checks to make sure the file already does not exist, and the client has the permissions. Then it returns the location where to write each block of the data.\n\nFor each block, client performs I/O directly to that block.\n\nAs soon as the client finishes writing the data block, the Data Node starts copying the data block to another Data Node for redundancy. Note that, client is not involved anymore, so the copying happens internally.\n\nThe name nodes will contain the mapping of all the data nodes where the particular data block resides.\nHadoop comprises 4 modules.\n\nHDFS - The Hadoop‚Äôs native file system.\nYet Another Resource Navigator (YARN) - a module for scheduling tasks in the Name Nodes.\nHadoop Map Reduce - The main computing module that allows programs to break large data processing tasks into smaller ones and run them in parallel on multiple servers.\nHadoop Common or Hadoop Core - The core java libraries\n\nHadoop provides the following features:\n\nIt has built in security, runs encryption on data in transit and at rest.\nThe map reduce module only has simple tasks, machine learning cannot run.\nHadoop uses external memory, so integrates well with network based storage like AWS S3. This is provided in AWS EMR (Elastic MapReduce)\nSince Hadoop uses network memory access, usually slow, but capable to large amount of scaling.\nHadoop is very affordable for processing big data.\nHadoop is used with batch processing mostly.\n\n\n\nApache Spark\nSpark works on the principle of accessing local memory in RAM instead of using network storage. Spark does not provide any native file system, you can integrate it with any distributed file system.\nSpark has the following components:\n\nSpark Core contains basic functionalities like memory management, task scheduling, etc.\nSpark SQL allows you to process data in Spark‚Äôs distributed storage.\nSpark streaming and Structured streaming allow Spark to stream data efficiently in real-time by separating data into tiny continuous blocks.\nMachine Learning Library (MLlib) provides built in machine learning algorithms with distributed algorithms.\nGraphX allows you to visualize and analyze data with graphs.\n\nFollowing are the few features:\n\nSpark stores and process data on internal memory.\nUsually more expensive that Hadoop.\nScaling is difficult, and requires vertical scaling by adding more RAM.\nHas built-in machine learning algorithm.\nSpark has only basic security, it is not opinionated about that.\nProcesses data very fast, ideal for the real-time or near real-time processing workloads.\n\n\n\n\nBatch Data Processing\nFor batch data processing, we can use AWS Elastic MapReduce (AWS EMR) with Hadoop and Apache Spark integrated on top of it.\nSo, this could be one architecture for a data analytics solution.\n\nThe architecture diagram below depicts the same data flow as above but uses AWS Glue for aggregated ETL (heavy lifting, consolidated transformation, and loading engine). AWS Glue is a fully managed service, as opposed to Amazon EMR, which requires management and configuration of all of the components within the service.\n\n\n\nStream Data Processing\nFor processing streaming data, AWS has a service group called Kinesis, it has multiple services inside the group. The entire Kinesis group of services is serverless in nature.\n\nAmazon Kinesis Data Streams - is a service that collects and ingests gigabytes per second data and can send it to your processing services, like EMR or EC2 or AWS Lambda, etc.\n\nKinesis requires you to install a kinesis agent software in your source of data generation platform.\nThe configuration of the agent requires the URL of the Kinesis Data Stream created, and it publishes the event to the Data Streams.\n\n\n\n\nAmazon Kinesis Video Streams - collects huge amount of streaming video from camera or other devices and sends them to proper processing services.\n\nUsed for video surveillance, self-driving car, smart home, etc.\nSends to custom Sagemaker endpoint, or AWS Rekognition, etc.\n\n\n\n\nAmazon Kinesis Firehose - is an ETL (Extract Transform Load) service that ingests the data similar to Kinesis streaming services, performs simple transformations / filters if necessary and directly stores them into a specific destination configured.\n\nIts source can be some SNS topic, or some Kinesis Data Stream or direct PUT API using AWS SDK.\nThe destination can be AWS S3, Redshift, Opensearch, etc.\nWe can customize the transformations to apply ML models as well.\n\nAmazon Managed Service for Apache Flink - this is a managed service where without using any programming language or explicit coding, you can perform the transform operation using SQL statements.\n\nIts source is a Kinesis Data Stream, or some S3 and the destination is similarly another Kinesis Data Stream or S3.\nUseful for performing aggregation-type logic.\n\n\n\nAnother way to stream data is to use Amazon Managed Service for Kafka (Amazon MSK). This is similar to the broadcasting strategy by SNS topic, but serverless setup without self-managing the resources (consumer/producer) in the SNS.\n\nMSK is better in that it manages the partitions in SNS automatically. For instance, if you have 10 kinds of topics, then you need 10 SNS channels. Here, you need to set up one AWS MSK with a proper key partition.\nSNS is easier to use compared to MSK."
  },
  {
    "objectID": "notes/concepts/aws_data_analytics.html#variety",
    "href": "notes/concepts/aws_data_analytics.html#variety",
    "title": "AWS Cloud Data Analytics Guide",
    "section": "Variety",
    "text": "Variety\nWhen the data is coming from different sources, in many different formats, you have a variety problem.\nAll kinds of data can be summarized into 3 different types.\n\nStructured Data - The data stored into rows and columns like a spreadsheet. The easiest to analyze. These are usually stored in RDBMS.\nSemi-structured Data - The data stored in JSON, CSV or XML files, in a specific format, but easily adaptable to whole bunch of different schemas. These are usually stored in NoSQL databases. (However, NoSQL does not mean that it cannot be queried by SQL, it simply means that it is not only SQL). These are usually stored in DynamoDB / specialized databases.\nUnstructured Data - The hardest to analyze. Could be anything from videos, images, documents, text files, logs, that does not have any specific standardized formats to be parsed. These are usually stored in S3.\n\n\nData Storage and Efficiency\nWhen storing structured data, there can be two competing interests. You need to do write fast / you need to do read fast. When the size of database is huge, you cannot do both efficiently, you need to sacrifice one for the other.\nSo we have 2 kinds of systems:\n\nOLTP (Online Transactional Protocol) system which says that the write should be fast. In these systems, the read queries are usually very simple GET, given a filter, find me this row.\n\nIn this case, we do row based partitioning and indexing.\nThese are optimized for random reads and writes\nThere can be only low or medium level compression of data.\nIt is best for returning the full row of data given a filter.\nOn disk, these values are stored in row by row, in consecutive physical memory addresses.\n\nOLAP (Online Analytical Protocol) system which says that data read should be fast. In these systems, the read queries are generally aggregate queries used for dashboarding.\n\nWe do column based partitioning or columnar storage based indexing.\nThese are optimized for sequential reads and writes.\nIt is best for returning data on aggregate level queries, only for a few important columns that are selected.\nOne can achieve high level of compression. Due to the nature of aggregate queries, compressed values can be fetched and the aggregation logic can be computed after uncompressing the data.\nThe data is stored on disk in a column by column manner.\n\n\n\n\nAWS Available Data Storage Services\nHere is a list of different AWS services for data storage of different types.\n\n\n\n\n\n\n\nDatabases\nDescription\n\n\n\n\nAmazon Aurora\nHigh-performance, highly available, scalable proprietary serverless relational database with full MySQL and PostgreSQL compatibility\n\n\nAmazon RDS\nManaged relational database service in the cloud with multiple engine options\n\n\nAmazon Redshift\nCloud-based data warehouse with ML optimizations for best price‚Äìperformance at any scale\n\n\nAmazon DynamoDB\nFast, flexible, and highly scalable NoSQL database\n\n\nAmazon ElastiCache\nFully managed, cost-optimized, highly scalable caching service for real-time performance\n\n\nAmazon MemoryDB for Redis\nRedis-compatible, durable, in-memory database for ultra-fast performance\n\n\nAmazon DocumentDB (MongoDB-compatible)\nFully managed, scalable JSON document database\n\n\nAmazon Keyspaces (for Apache Cassandra)\nServerless, scalable, highly available Cassandra-compatible database service\n\n\nAmazon Neptune\nHigh-availability, scalable, serverless graph database\n\n\nAmazon Timestream\nFast, scalable, serverless time-series database\n\n\nAmazon QLDB\nFully managed, cryptographically verifiable ledger database\n\n\nAWS Database Migration Service (DMS)\nAutomated managed migration and replication service for moving databases and analytics workloads to AWS with minimal downtime and no data loss"
  },
  {
    "objectID": "notes/concepts/aws_data_analytics.html#veracity",
    "href": "notes/concepts/aws_data_analytics.html#veracity",
    "title": "AWS Cloud Data Analytics Guide",
    "section": "Veracity",
    "text": "Veracity\nVercity means to ensure the truthfulness of the data that it has not been tampered with. You must ensure that you maintain a high level of certainty that the data you are analyzing is trustworthy. Data integrity is all about making sure your data is trustworthy, the entire data chain is secure and free from compromise.\n\nA data analyst might be called to perform data integrity checks. During this process they look for potential sources of data integrity problems.\nData can come from both internal and external sources.\n\nIt is highly unlikely that they will influence data generated outside of the organization. For these external sources of data, we might need to perform some ETL steps to identify issues (random entries, incomplete or inconsistent data, outliers, etc.) and cleanse the data.\nHowever, within the organization, they might have the ability to make recommendations on improvements for the data sources they will be interacting with.\n\nData analysts may need to determine the integrity of the data sources and make adjustments to account for any integrity deficiencies.\n\nAs opposed to the popular ETL approach, there is another approach called ELT (Extract Load and Transform). In modern cloud-based environments, the extract, load, and transform (ELT) approach loads data as it is. It then transforms it at a later stage, depending on the use case and analytics requirements.\nBasically it means:\n\nCollect and extract the raw data.\nLoad it into its natural storage type like a data lake or data warehouse.\nTransform the data from data storage as necessary later for business analytics requirements.\n\nIn this ELT process, the transformations happen in the target data warehouse system itself, not at the source or any other processing units.\n\n\n\n\nData Lifecycles\nData goes through various stages while performing data analytics. We need to ensure that the data integrity is remained throughout this data lifecycle stage.\n\nCreation\n\nData gets created at source\nNeeds to ensure data generated is correct\nMeans needs to run software-based audits.\n\nAggregation\n\nStream data processing performs simple continuous aggregation\nErrors are usually reduced due to aggregation methods used\nHowever, data is generally misrepresented / wrong aggregation is performed.\nNeed to ensure correct data transformation and aggregation.\n\nStorage\n\nThe aggregated data gets stored in data storage for business decisions\nNeeds auditing, encryption at rest.\nNeeds proper data labelling so that schema information is correct.\n\nAccess\n\nUsers access the aggregated data for their business intelligence needs.\nData should be read-only.\nMonitoring for unusual access patterns.\n\nSharing\n\nThis is the reporting phase.\nThis is where the most accuracy and veracity concern lies in\n\nArchiving\n\nOnce the data loses its immediate value, it should be archived.\nArchival repository should have very restricted access and should be read-only.\nData security is of primary concern.\n\nDisposal\n\nData needs to be destroyed at some point for safety, compliance and regulations.\nPrevents leakage of past sensitive information.\n\n\n\n\nAWS Services for Veracity\nAWS has multiple services for ensuring that businesses meet their veracity needs.\n\nAWS Glue is a serverless data integration service:\n\nIt can discover the data present in data lake or AWS S3.\nIt manages the metadata and schema associated with the data stored in various storages.\nThe AWS Glue Data Catalog is your persistent metadata store for all your data assets, regardless of where they are located.\nThe Data Catalog contains table definitions, job definitions, schemas, and other control information to help you manage your AWS Glue environment.\nIt automatically computes statistics and registers partitions to make queries against your data efficient and cost-effective.\nIt also maintains a comprehensive schema version history so you can understand how your data has changed over time.\nAWS Glue also has crawlers, that can crawl your data and automatically discover schemas of the data.\nAWS Glue also performs data cleaning tasks. AWS Glue helps clean and prepare your data for analysis without you having to become an ML expert.\n\nIts FindMatches feature deduplicates and finds records that are imperfect matches of each other. For example, use FindMatches to find duplicate records in your database of restaurants, such as when one record lists ‚ÄúJoe‚Äôs Pizza‚Äù at ‚Äú121 Main St.‚Äù and another shows a ‚ÄúJoseph‚Äôs Pizzeria‚Äù at ‚Äú121 Main.‚Äù FindMatches will ask you to label sets of records as either ‚Äúmatching‚Äù or ‚Äúnot matching.‚Äù\nThe system will then learn your criteria for calling a pair of records a ‚Äúmatch‚Äù and will build an ETL job that you can use to find duplicate records within a database or match records across two databases.\n\n\nYou can manage data quality in datasets with AWS Glue Data Quality. This service analyzes your chosen dataset and recommends data quality rules that you can optimize.\n\nYou can use Data Quality Definition Language (DQDL) to add these quality checks and actions to your existing data pipeline, by adding them in AWS Glue Data Catalogue.\n\n\n\nNote: AWS Glue is a serverless data pipeline with lots of built-in tools, but it does not allow customized code. If you want to use custom code in ETL job, go with either Amazon EMR (for batch processing) or Amazon Kinesis (for stream processing loads)\n\nAWS Glue DataBrew is a visual data preparation tool that can help you clean, normalize your dataset using an excel-like visual tool.\n\nIt has a diverse range of pre-built data transformation tools (over 250+), that can help you clean data without writing any code.\nYou can automate filtering anomalies, converting data to standard formats and correcting invalid values, and other tasks.\n\nAmazon DataZone is a data management service that makes it easier for an organization to catalogue discover, share and govern data stored across AWS, on-premise or 3rd party services.\n\nThis is an elaborate data-sharing tool, you can control who has access to what part of the data. Administrators and data stewards who oversee your organization‚Äôs data assets can manage and govern access to data using fine-grained controls.\nGovern and share data seamlessly across organizational boundaries.\nCreate a data access portal within your organization. An integrated analytics portal gives you a personalized view of all your data while enforcing your governance and compliance policies at scale.\nAmazon DataZone provides the following features:\n\nSearch for published data and request access to work on projects.\nCollaborate with project teams through data assets.\nManage and monitor data assets across projects.\nEnsure the right data is accessed with a governed workflow.\nAccess analytics with a personalized view for data assets through a web-based application or API."
  },
  {
    "objectID": "notes/concepts/aws_data_analytics.html#value",
    "href": "notes/concepts/aws_data_analytics.html#value",
    "title": "AWS Cloud Data Analytics Guide",
    "section": "Value",
    "text": "Value\nBefore making business decisions, it is important to derive value from the data stored in the data warehouse.\nThere are two key ways to extract value from data:\n\nQuery: Queries are the process of extracting, filtering and customizing your data.\nReport: Reports are the presentation of these query results in a form of actionable insights.\n\nReports can come in form of tables, charts and figures.\nVisualization makes complex data more accessible and understandable, helping users quickly identify trends, patterns, and anomalies.\nYou can break reports into pages or views.\nThese pages should have a single theme for all the report elements within them. Provide filters that the report consumer can apply to either the whole page or to individual elements within the page.\n\n\nThere are 3 kinds of reports.\n\nStatic: Reports presented in forms of PDF or Powerpoint slides, the data does not change.\nInteractive:\n\nThese types of reports generally fall under the heading of self-service business intelligence.\nThese reports often take on a print-based report style but have the advantage that consumers can apply filters to charts and graphs, change the scales, and even group and sort values within the reports.\nA consumer can then tell their own story using the foundation laid by the report builder\n\nDashboards:\n\nThis type of visualization is another very popular reporting tool.\nWhether dashboards are interactive depends on the software used.\nConsumers find the greatest benefit in dashboards when they focus on high-level roll-ups of key business factors.\n\n\n\nAWS Services for Value\nTo derive value from data present in a data warehouse, we have different services available.\n\nAmazon QuickSight is a generative business intelligence (BI) and analytics tool. (Similar to Power BI, but AWS specific).\n\nConveniently create interactive dashboards, paginated reports, email reports, embedded analytics, and use natural language queries.\nSupports integration with several data sources, including Amazon S3, Amazon Redshift, Amazon RDS, Amazon Athena, and third-party databases.\nCan connect to on-premise data sources.\nCleans, transforms and shapes data before creating the visualizations\nCombine multiple visualizations into interactive dashboards.\nIntegrates with Amazon SageMaker to incorporate ML models directly into visualizations.\n\nAWS Sagemaker is a machine learning platform to build, train, test and deploy custom-made machine learning models.\n\nSageMaker also provides tools to monitor the performance of deployed models, and track metrics, and set up alarms\nIt also has a range of built-in ML algorithms ready to be deployed and used. This service is called AWS Sagemaker Jumpstart.\nYour model is deployed as a highly available and scalable endpoint.\n\n\n\n\nAWS Bedrock is an API service to access foundational models for generative AI, including text-to-text and text-to-image models.\nAmazon Athena is a serverless analytics service that can query data stored in AWS S3 using SQL like language.\n\nProvides a streamlined, flexible way to analyze petabytes of data without the need to set up and manage infrastructure\nAthena is designed for interactive analytics, running queries, and getting results in real time. This is especially valuable for one-time queries and exploratory data analysis.\nSaves query history and results, making it convenient to review past queries, analyze past performance, and troubleshoot discrepancies\nIntegrates out-of-the-box with AWS Glue\nControls access to data by using IAM policies"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "These are the various notes that I use to learn a new topic / material. These reflect my own understanding of various topics, as guided by the various resources I used to learn them.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n5 words\n\n\n\n\n\n\n\n\n\nStatistics\n\nDepedent Data\n\n\n\n\n\n\n\n\n\nJuly 12, 2025\n\n\n598 words\n\n\n\n\n\n\n\n\n\nData Science\n\nSoftware\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nNovember 24, 2024\n\n\n1,821 words\n\n\n\n\n\n\n\n\n\nSoftware\n\nCloud Computing\n\nData Science\n\n\n\n\n\n\n\n\n\nMarch 13, 2024\n\n\n5,441 words\n\n\n\n\n\n\n\n\n\nSoftware\n\nCloud Computing\n\n\n\n\n\n\n\n\n\nFebruary 19, 2024\n\n\n8,245 words\n\n\n\n\n\nNo matching items\n\nAlso take a look at some of my blogs posts as follows:\n\nAn eight-part tutorial series on Natural Language Processing is available at my substack.\nA six-part tutorial series on the basics of Generative AI is available at my substack.\nA seven-part tutorial series on the basics of Reinforcement Learning is available at my substack.\nA three part series on how to do natural language text processing using R.\n\nText mining in R.\nText Classification in R.\nChangepoint Analysis of Linguistics in R."
  },
  {
    "objectID": "notes/index.html#concept-guides",
    "href": "notes/index.html#concept-guides",
    "title": "Notes",
    "section": "",
    "text": "These are the various notes that I use to learn a new topic / material. These reflect my own understanding of various topics, as guided by the various resources I used to learn them.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n5 words\n\n\n\n\n\n\n\n\n\nStatistics\n\nDepedent Data\n\n\n\n\n\n\n\n\n\nJuly 12, 2025\n\n\n598 words\n\n\n\n\n\n\n\n\n\nData Science\n\nSoftware\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nNovember 24, 2024\n\n\n1,821 words\n\n\n\n\n\n\n\n\n\nSoftware\n\nCloud Computing\n\nData Science\n\n\n\n\n\n\n\n\n\nMarch 13, 2024\n\n\n5,441 words\n\n\n\n\n\n\n\n\n\nSoftware\n\nCloud Computing\n\n\n\n\n\n\n\n\n\nFebruary 19, 2024\n\n\n8,245 words\n\n\n\n\n\nNo matching items\n\nAlso take a look at some of my blogs posts as follows:\n\nAn eight-part tutorial series on Natural Language Processing is available at my substack.\nA six-part tutorial series on the basics of Generative AI is available at my substack.\nA seven-part tutorial series on the basics of Reinforcement Learning is available at my substack.\nA three part series on how to do natural language text processing using R.\n\nText mining in R.\nText Classification in R.\nChangepoint Analysis of Linguistics in R."
  },
  {
    "objectID": "notes/index.html#technical-materials",
    "href": "notes/index.html#technical-materials",
    "title": "Notes",
    "section": "Technical Materials",
    "text": "Technical Materials\nThese are some of the useful proof techniques that I like and a bunch of technical materials that I have collected over the years.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nProof of Uniform Consistency\n\n\n\nProof Technique\n\nConsistency\n\n\n\n\n\n\n\n\n\nNovember 8, 2025\n\n\n1,009 words\n\n\n\n\n\n\n\nProof of Statistical Consistency of an Optimizer\n\n\n\nProof Technique\n\nConsistency\n\n\n\n\n\n\n\n\n\nApril 13, 2024\n\n\n277 words\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/index.html#teaching-materials",
    "href": "notes/index.html#teaching-materials",
    "title": "Notes",
    "section": "Teaching Materials",
    "text": "Teaching Materials\n\n10+2 Level\n\nA short notes on Riemann Integration.\nPractice Problem Sets for 10+2 Level Statistics (West Bengal State Board).\n\nProblem Set 1\nProblem Set 2\n\nPractice Problem Sets for B.Stat. Entrace Exam for Indian Statistical Institute.\n\nMCQ Set 1\nMCQ Set 2\nMCQ Set 3\nMCQ Set 4\nMCQ Set 5\nSAQ Set 1\nSAQ Set 2\nSAQ Set 3\n\n\n\n\nUndergraduate Level\n\nPractice questions and solutions to various assignments of the B.Stat. programme at Indian Statistical Institute, Kolkata.\nA short lecture slide deck on Linear Algebra and its applications.\n\n\n\nGraduate Level\n\nPractice questions and solutions to various assignments of the M.Stat. programme at Indian Statistical Institute, Kolkata."
  },
  {
    "objectID": "notes/index.html#reflections",
    "href": "notes/index.html#reflections",
    "title": "Notes",
    "section": "Reflections",
    "text": "Reflections\nThese are my own way of understanding the ways of the world, again guided by various non-fiction books, podcasts and a few philosophy lessons.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/index.html#recommended-resources",
    "href": "notes/index.html#recommended-resources",
    "title": "Notes",
    "section": "Recommended Resources",
    "text": "Recommended Resources\nInternet is full of interesting things. Here‚Äôs a few that I like:\n\nSeeing Theory. http://seeing-theory.brown.edu/ - Interactive Visualization of Introduction of Probability and Statistics.\nThe Idea behind Explorable Explorations: https://blog.ncase.me/explorable-explanations/. And here‚Äôs the list of such explanations: https://explorabl.es/.\nAlexander Bogomolny‚Äôs post on the Area of Circle by Rabbi Abraham bar Hiyya Hanasi - Probably the best proof that I have seen for showing that the area of a circle is half of the radius times the circumference.\nThe Mathematics of Juggling - Allen Knutson."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html",
    "href": "notes/technicals/uniform-consistency-proof.html",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "Let us assume that \\(f\\) be a function and \\(f_n\\) be its empirical counterpart (possibly stochastic). Let \\(x \\in \\chi\\) be some input variable, \\(\\chi\\) is some abstract space.\nSuppose using empirical process (or law of large numbers, etc.) we can somehow show that for every fixed \\(x \\in \\chi\\), \\[\n\\vert f_n(x) - f(x) \\vert = o_p(1)\n\\] However, we need a stronger result that establish this uniformly over all choices of \\(x \\in \\chi\\), i.e., we require \\[\n\\sup_{x \\in \\chi} \\vert f_n(x) - f(x)\\vert = o_p(1)\n\\]\nThere are few typical ways to approach this problem.\n\n\nThis approach aims to leverage some compactness-kind characterization of the space \\(\\chi\\). Suppose that \\(\\chi\\) is compact. However, that is not enough. To see this, note that if \\(\\chi\\) is compact, we can consider a finite subcover for this, and if we can show that the uniform convergence holds for every subcover, because there are only finitely many, we can take further supremum over them. Therefore, we need to show something like: \\[\n\\sup_{x \\in B(y, \\cdot)} \\vert f_n(x) - f(x)\\vert = o_p(1)\n\\] where \\(B(y, \\cdot)\\) is a ball in \\(\\chi\\) centered around \\(y\\). However, because \\(\\chi\\) is compact, it is also complete, and hence one can actually work with the Cauchy-sequences kind of thing. Therefore, to show the above, one version would be to work with \\[\n\\sup_{x \\in B(y, \\cdot)} \\vert f_n(x) - f_n(y)\\vert = o_p(1)\n\\] This is a version of equicontinuity, but for possibly random functions, hence called stochastic equicontinuity. Also, it needs to be hold only probabilistically for most \\(y\\)s, not for all.\nHere are the final assumptions.\n\n(A1) \\(\\chi\\) is compact.\n(A2) For every \\(\\epsilon, \\eta &gt; 0\\), there exists a random \\(\\Delta_n(\\epsilon, \\eta)\\) and constant \\(n_0(\\epsilon, \\eta)\\) such that for all \\(n \\geq n_0(\\epsilon,\\eta)\\), \\(\\mathbb{P}(\\vert \\Delta_n(\\epsilon, \\eta) \\vert &gt; \\epsilon) \\leq \\eta\\) and for all \\(x \\in \\chi\\), there exists a neighbourhood (open set) \\(N(x, \\epsilon, \\eta)\\) such that \\[\n\\sup_{y, y' \\in N(x, \\epsilon, \\eta)} \\vert f_n(y) - f_n(y') \\vert \\leq \\Delta_n(\\epsilon, \\eta), \\ n \\geq n_0(\\epsilon, \\eta)\n\\]\n\nUnder this assumptions, we can lift the pointwise convergence of \\(f_n(x)\\) to \\(f(x)\\) for each fixed \\(x \\in \\chi\\) to the uniform convergence over all \\(x \\in \\chi\\). The detailed proof can be found in (Newey 1991).\n\n\n\nAnother interesting technique when the space \\(\\chi\\) is not compact, is to use an \\(\\epsilon\\)-net type bounding argument.\nFirst note that, \\[\n\\mathbb{P}(\\sup_{x \\in \\chi} | f_n(x) - f(x)| &gt; \\delta) = \\mathbb{P}(\\cup_{x \\in \\chi} \\{ |f_n(x) - f(x)| &gt; \\delta \\})\n\\] One can now try to bound this by an union bound, i.e., by \\(|\\chi| \\sup_{x \\in \\chi} \\mathbb{P}(|f_n(x) - f(x) | &gt; \\delta)\\), which is typically \\(|\\chi| \\times o_p(1)\\). However, \\(\\chi\\) is generally an uncountable set, hence the above bound is useless. The strategy is to bound this set by some number of open balls which (like an open cover) and then control this number of open balls with the decay rate of the probability.\nTypically, there are 3 steps of this kind of proof.\n\n\nHere, we use Hoeffding‚Äôs inequality (if the stochastic functions \\(f_n\\) are uniformly bounded) or use Berstein‚Äôs inequality (or even a version of Talagrand‚Äôs inequality), which provides some sort of exponential concentration: For every \\(x \\in \\chi\\), we have \\[\n\\mathbb{P}(|f_n(x) - f(x)| &gt; \\delta) \\leq C e^{-c h(n, \\delta)}\n\\] where \\(h(n,\\delta)\\) is some function of \\(n\\) and \\(\\delta &gt; 0\\). The constant \\(C\\) and \\(c\\) are typically chosen to be independent of \\(x \\in \\chi\\).\n\n\n\nAt this stage, we try to build an \\(\\epsilon\\)-net cover for the space \\(\\chi\\). It means, to find a set \\(S \\subset \\chi\\) of representative points such that for any \\(x \\in \\chi\\), there exists \\(s' \\in S\\) such that \\(|x - s'| &lt; \\epsilon\\).\nLet \\(N_\\epsilon(\\chi)\\) be the \\(\\epsilon\\)-net covering number for the space \\(\\chi\\), which is the size of the smallest such \\(\\epsilon\\)-net set \\(S\\) for \\(\\chi\\). In general, \\(N_\\epsilon(\\chi)\\) can just be a bound on the cardinality of that smallest set, it does not need to be exactly calculated.\n\n\n\nNow for any fixed \\(x \\in \\chi\\), let \\(s \\in S\\) be its representative point in the \\(\\epsilon\\)-net. Then, \\[\n\\begin{align*}\n    | f_n(x) - f(x) |\n    & = | f_n(x) - f_n(s) - (f(x) - f(s)) + f_n(s) - f(s) |\\\\\n    & \\leq |f_n(x) - f_n(s)| + |f(x) - f(s)| + |f_n(s) - f(s)|\\\\\n    & \\leq \\sup_{|x - s| &lt; \\epsilon} |f_n(x) - f_n(s)| + \\sup_{|x - s| &lt; \\epsilon} |f(x) - f(s)| + |f_n(s) - f(s)|\n\\end{align*}\n\\] Often, \\(f_n\\) and \\(f\\) are nice continuous type functions (sometimes even Lipschitz) so that the first two terms are of \\(O(g(\\epsilon))\\) for some function \\(g(\\cdot)\\), and the constant is actually free of the choice of \\(x \\in \\chi\\). This means, \\[\n\\begin{align*}\n\\mathbb{P}(\\sup_{x \\in \\chi} |f_n(x) - f(x)| &gt; \\delta)\n& = \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; \\delta - g(\\epsilon))\\\\\n& \\leq \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; g'(\\delta))\n\\end{align*}\n\\] for some function \\(g'(\\cdot)\\) and appropriately chosen small \\(\\epsilon\\) as a function of \\(\\delta\\). What this step shows is that there is only a little error in replacing the difference between \\(f_n\\) and \\(f\\) at this general point \\(x \\in \\chi\\) to the same difference but now evaluated in one of the points in the \\(\\epsilon\\)-net.\nThis now helps us to write: \\[\n\\begin{align*}\n    \\mathbb{P}(\\sup_{x \\in \\chi} |f_n(x) - f(x)| &gt; \\delta)\n    & = \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; g'(\\delta))\\\\\n    & \\leq N_\\epsilon(\\chi) \\mathbb{P}(|f_n(s) - f(s)| &gt; g'(\\delta))\\\\\n    & \\leq N_\\epsilon(\\chi) \\times C e^{-c h(n, g'(\\delta))}\n\\end{align*}\n\\] Now putting the expression of \\(\\epsilon\\) in terms of \\(\\delta\\) and taking appropriate limit of \\(n\\) typically makes this bound tend to \\(0\\). Note that, here union bound makes sense as we have a finite-set (i.e., the \\(\\epsilon\\)-net).\nMore details can be found in (Erdogdu 2024) and (Vershynin 2018).\n\n\n\n\nAnother way to approach this kind of problem is to hope for a variational-type bound, where the space \\(\\chi\\) is too broad for the covering number to be useful, or the exponential concentration does not hold.\n\n\n\n\n\nErdogdu, Murat A. 2024. ‚ÄúCSC 2532: Statistical Learning Theory (Lecture 05 Notes).‚Äù https://erdogdu.github.io/csc2532/lectures/lecture05.pdf.\n\n\nNewey, Whitney K. 1991. ‚ÄúUniform Convergence in Probability and Stochastic Equicontinuity.‚Äù Econometrica 59 (4): 1161‚Äì67. https://doi.org/10.2307/2938179.\n\n\nVershynin, Roman. 2018. ‚ÄúHigh Dimensional Probability ‚Äî Lectures 23 and 24.‚Äù https://www.math.uci.edu/~rvershyn/teaching/hdp/hdp.html."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html#approach-stochastic-equicontinuity",
    "href": "notes/technicals/uniform-consistency-proof.html#approach-stochastic-equicontinuity",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "This approach aims to leverage some compactness-kind characterization of the space \\(\\chi\\). Suppose that \\(\\chi\\) is compact. However, that is not enough. To see this, note that if \\(\\chi\\) is compact, we can consider a finite subcover for this, and if we can show that the uniform convergence holds for every subcover, because there are only finitely many, we can take further supremum over them. Therefore, we need to show something like: \\[\n\\sup_{x \\in B(y, \\cdot)} \\vert f_n(x) - f(x)\\vert = o_p(1)\n\\] where \\(B(y, \\cdot)\\) is a ball in \\(\\chi\\) centered around \\(y\\). However, because \\(\\chi\\) is compact, it is also complete, and hence one can actually work with the Cauchy-sequences kind of thing. Therefore, to show the above, one version would be to work with \\[\n\\sup_{x \\in B(y, \\cdot)} \\vert f_n(x) - f_n(y)\\vert = o_p(1)\n\\] This is a version of equicontinuity, but for possibly random functions, hence called stochastic equicontinuity. Also, it needs to be hold only probabilistically for most \\(y\\)s, not for all.\nHere are the final assumptions.\n\n(A1) \\(\\chi\\) is compact.\n(A2) For every \\(\\epsilon, \\eta &gt; 0\\), there exists a random \\(\\Delta_n(\\epsilon, \\eta)\\) and constant \\(n_0(\\epsilon, \\eta)\\) such that for all \\(n \\geq n_0(\\epsilon,\\eta)\\), \\(\\mathbb{P}(\\vert \\Delta_n(\\epsilon, \\eta) \\vert &gt; \\epsilon) \\leq \\eta\\) and for all \\(x \\in \\chi\\), there exists a neighbourhood (open set) \\(N(x, \\epsilon, \\eta)\\) such that \\[\n\\sup_{y, y' \\in N(x, \\epsilon, \\eta)} \\vert f_n(y) - f_n(y') \\vert \\leq \\Delta_n(\\epsilon, \\eta), \\ n \\geq n_0(\\epsilon, \\eta)\n\\]\n\nUnder this assumptions, we can lift the pointwise convergence of \\(f_n(x)\\) to \\(f(x)\\) for each fixed \\(x \\in \\chi\\) to the uniform convergence over all \\(x \\in \\chi\\). The detailed proof can be found in (Newey 1991)."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html#approach-epsilon-net-bound",
    "href": "notes/technicals/uniform-consistency-proof.html#approach-epsilon-net-bound",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "Another interesting technique when the space \\(\\chi\\) is not compact, is to use an \\(\\epsilon\\)-net type bounding argument.\nFirst note that, \\[\n\\mathbb{P}(\\sup_{x \\in \\chi} | f_n(x) - f(x)| &gt; \\delta) = \\mathbb{P}(\\cup_{x \\in \\chi} \\{ |f_n(x) - f(x)| &gt; \\delta \\})\n\\] One can now try to bound this by an union bound, i.e., by \\(|\\chi| \\sup_{x \\in \\chi} \\mathbb{P}(|f_n(x) - f(x) | &gt; \\delta)\\), which is typically \\(|\\chi| \\times o_p(1)\\). However, \\(\\chi\\) is generally an uncountable set, hence the above bound is useless. The strategy is to bound this set by some number of open balls which (like an open cover) and then control this number of open balls with the decay rate of the probability.\nTypically, there are 3 steps of this kind of proof.\n\n\nHere, we use Hoeffding‚Äôs inequality (if the stochastic functions \\(f_n\\) are uniformly bounded) or use Berstein‚Äôs inequality (or even a version of Talagrand‚Äôs inequality), which provides some sort of exponential concentration: For every \\(x \\in \\chi\\), we have \\[\n\\mathbb{P}(|f_n(x) - f(x)| &gt; \\delta) \\leq C e^{-c h(n, \\delta)}\n\\] where \\(h(n,\\delta)\\) is some function of \\(n\\) and \\(\\delta &gt; 0\\). The constant \\(C\\) and \\(c\\) are typically chosen to be independent of \\(x \\in \\chi\\).\n\n\n\nAt this stage, we try to build an \\(\\epsilon\\)-net cover for the space \\(\\chi\\). It means, to find a set \\(S \\subset \\chi\\) of representative points such that for any \\(x \\in \\chi\\), there exists \\(s' \\in S\\) such that \\(|x - s'| &lt; \\epsilon\\).\nLet \\(N_\\epsilon(\\chi)\\) be the \\(\\epsilon\\)-net covering number for the space \\(\\chi\\), which is the size of the smallest such \\(\\epsilon\\)-net set \\(S\\) for \\(\\chi\\). In general, \\(N_\\epsilon(\\chi)\\) can just be a bound on the cardinality of that smallest set, it does not need to be exactly calculated.\n\n\n\nNow for any fixed \\(x \\in \\chi\\), let \\(s \\in S\\) be its representative point in the \\(\\epsilon\\)-net. Then, \\[\n\\begin{align*}\n    | f_n(x) - f(x) |\n    & = | f_n(x) - f_n(s) - (f(x) - f(s)) + f_n(s) - f(s) |\\\\\n    & \\leq |f_n(x) - f_n(s)| + |f(x) - f(s)| + |f_n(s) - f(s)|\\\\\n    & \\leq \\sup_{|x - s| &lt; \\epsilon} |f_n(x) - f_n(s)| + \\sup_{|x - s| &lt; \\epsilon} |f(x) - f(s)| + |f_n(s) - f(s)|\n\\end{align*}\n\\] Often, \\(f_n\\) and \\(f\\) are nice continuous type functions (sometimes even Lipschitz) so that the first two terms are of \\(O(g(\\epsilon))\\) for some function \\(g(\\cdot)\\), and the constant is actually free of the choice of \\(x \\in \\chi\\). This means, \\[\n\\begin{align*}\n\\mathbb{P}(\\sup_{x \\in \\chi} |f_n(x) - f(x)| &gt; \\delta)\n& = \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; \\delta - g(\\epsilon))\\\\\n& \\leq \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; g'(\\delta))\n\\end{align*}\n\\] for some function \\(g'(\\cdot)\\) and appropriately chosen small \\(\\epsilon\\) as a function of \\(\\delta\\). What this step shows is that there is only a little error in replacing the difference between \\(f_n\\) and \\(f\\) at this general point \\(x \\in \\chi\\) to the same difference but now evaluated in one of the points in the \\(\\epsilon\\)-net.\nThis now helps us to write: \\[\n\\begin{align*}\n    \\mathbb{P}(\\sup_{x \\in \\chi} |f_n(x) - f(x)| &gt; \\delta)\n    & = \\mathbb{P}(\\sup_{s \\in S} |f_n(s) - f(s)| &gt; g'(\\delta))\\\\\n    & \\leq N_\\epsilon(\\chi) \\mathbb{P}(|f_n(s) - f(s)| &gt; g'(\\delta))\\\\\n    & \\leq N_\\epsilon(\\chi) \\times C e^{-c h(n, g'(\\delta))}\n\\end{align*}\n\\] Now putting the expression of \\(\\epsilon\\) in terms of \\(\\delta\\) and taking appropriate limit of \\(n\\) typically makes this bound tend to \\(0\\). Note that, here union bound makes sense as we have a finite-set (i.e., the \\(\\epsilon\\)-net).\nMore details can be found in (Erdogdu 2024) and (Vershynin 2018)."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html#approach-radamacher-complexity-bound",
    "href": "notes/technicals/uniform-consistency-proof.html#approach-radamacher-complexity-bound",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "Another way to approach this kind of problem is to hope for a variational-type bound, where the space \\(\\chi\\) is too broad for the covering number to be useful, or the exponential concentration does not hold."
  },
  {
    "objectID": "notes/technicals/uniform-consistency-proof.html#references",
    "href": "notes/technicals/uniform-consistency-proof.html#references",
    "title": "Proof of Uniform Consistency",
    "section": "",
    "text": "Erdogdu, Murat A. 2024. ‚ÄúCSC 2532: Statistical Learning Theory (Lecture 05 Notes).‚Äù https://erdogdu.github.io/csc2532/lectures/lecture05.pdf.\n\n\nNewey, Whitney K. 1991. ‚ÄúUniform Convergence in Probability and Stochastic Equicontinuity.‚Äù Econometrica 59 (4): 1161‚Äì67. https://doi.org/10.2307/2938179.\n\n\nVershynin, Roman. 2018. ‚ÄúHigh Dimensional Probability ‚Äî Lectures 23 and 24.‚Äù https://www.math.uci.edu/~rvershyn/teaching/hdp/hdp.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Subhrajyoty Roy",
    "section": "",
    "text": "I am a Postdoctoral Researcher in the Department of Statistics and Data Science at Washington University in St.¬†Louis.\nYou may be interested in my Research or the open-source software tools I have created (or maintaining), or learning about me in general. I also have some notes here based on my understanding of various topics, including statistics, personal finance, and even my take on the philosophy of life.\nFor more formal reasons, my Curriculum Vitae may be more useful.\n\n\n Back to top"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My current research interest concentrates on Robust Statistics, Dependent Data Analysis and development of Statistical Methods for Deep Learning.\n\n\n\nStatistical Methods for Deep Learning\n\nDeep learning models have found super-accurate inferences and profound applications in many fields. Unfortunately, we understand very little about them: There‚Äôs always concerns about safety, ethics and reliability. There‚Äôs been a proliferation of statistical ideas that combines year-old techniques with these modern blackboxes to assure trustworthiness and help us understand them more.\n\n\nBonnerjee, Soham, Sayar Karmakar, and Subhrajyoty Roy.\n‚ÄúWISER: Segmenting watermarked region ‚Äì an epidemic change-point perspective.‚Äù\narXiv preprint arXiv:2509.21160 (2025).\nOpen Access Link\n\n\n\nDependent Data Analysis\n\nObservations are often correlated with each other, sometimes these correlation occur temporally, or spatially, or both. Dependent Data Analysis aims to obtain valid inference guarantees for these kinds of datasets.\n\n\nDeb, Soudeep, Claudia Neves, and Subhrajyoty Roy. ‚ÄúNonparametric quantile regression for spatio-temporal processes.‚Äù arXiv preprint arXiv:2405.13783 (2024). Open Access Link\nBhaduri, Ritwik, Subhrajyoty Roy, and Sankar K. Pal. ‚ÄúRough-Fuzzy CPD: a gradual change point detection algorithm.‚Äù Journal of Data, Information and Management 4, no. 3 (2022): 243‚Äì266. Link | Open Access Link\n\n\n\nRobust Statistics\n\nRobust Statistics aims to deliver guaranteed inferences even when your data may contain outliers, or the assumptions underlying your inference strategy are slightly violated.\n\n\nRoy, Subhrajyoty, Abhik Ghosh, and Ayanendranath Basu. ‚ÄúRobust Rank Estimation for Noisy Matrices.‚Äù arXiv preprint arXiv:2510.19583 (2025). Open Access Link\nJana, Suryasis, Subhrajyoty Roy, Ayanendranath Basu, and Abhik Ghosh.\n‚ÄúAsymptotic breakdown point analysis of the minimum density power divergence estimator under independent non-homogeneous setups.‚Äù\narXiv preprint arXiv:2508.12426 (2025).\nOpen Access Link\nRoy, Subhrajyoty. ‚ÄúRobust Matrix Factorization using the Density Power Divergence and its Applications‚Äù. ISI PhD Thesis TH646 (2025). Open Access Link\nRoy, Subhrajyoty, Supratik Basu, Abhik Ghosh, and Ayanendranath Basu.\n‚ÄúGeneralized Alpha-Beta Divergence and Associated Entropy Measures.‚Äù\narXiv preprint arXiv:2507.04637 (2025).\nOpen Access Link\nRoy, Subhrajyoty, Abir Sarkar, Abhik Ghosh, and Ayanendranath Basu. ‚ÄúAsymptotic Breakdown Point Analysis for a General Class of Minimum Divergence Estimators.‚Äù Bernoulli (2025+) (Accepted). Open Access Link\nRoy, Subhrajyoty, Ayanendranath Basu, and Abhik Ghosh.\n‚ÄúRobust Principal Component Analysis using Density Power Divergence.‚Äù Journal of Machine Learning Research 25, no. 324 (2024): 1‚Äì40. Link | Open Access Link\nPyne, Arijit, Subhrajyoty Roy, Abhik Ghosh, and Ayanendranath Basu.\n‚ÄúRobust and efficient estimation in ordinal response models using the density power divergence.‚Äù\nStatistics 58, no. 3 (2024): 481‚Äì520.\nLink | Open Access Link\nRoy, Subhrajyoty, Abhik Ghosh, and Ayanendranath Basu.\n‚ÄúRobust singular value decomposition with application to video surveillance background modelling.‚Äù\nStatistics and Computing 34, no. 5 (2024): 178.\nLink | Open Access Link | Supplementary Material\n\n\n\nApplications and Exploratory Data Analysis\n\nIn addition of these broad topics of interests, I have been fortunated to be part of some collaborative application-oriented work, that do not fit into one of the above categories.\n\n\nRoy, Subhrajyoty.\n‚ÄúTrustworthy Dimensionality Reduction.‚Äù\narXiv preprint arXiv:2405.05868 (2024).\nOpen Access Link\nGhatak, Anirban, Shivshanker Singh Patel, Soham Bonnerjee, and Subhrajyoty Roy.\n‚ÄúA generalized epidemiological model with dynamic and asymptomatic population.‚Äù\nStatistical Methods in Medical Research 31, no. 11 (2022): 2137‚Äì2163.\nLink | Open Access Link\nRoy, Subhrajyoty, Debasis Sengupta, Kalyan Rudra, and Udit Surya Saha.\n‚ÄúAnalysis of Pollution Patterns in Regions of Kolkata.‚Äù\nCalcutta Statistical Association Bulletin 72, no. 2 (2020): 133‚Äì170.\nLink\nDalal, Abhinandan, Diganta Mukherjee, and Subhrajyoty Roy.\n‚ÄúThe Information Content of Taster‚Äôs Valuation in Tea Auctions of India.‚Äù\narXiv preprint arXiv:2005.02814 (2020).\nOpen Access Link\nMukherjee, Diganta, Abhinandan Dalal, and Subhrajyoty Roy.\n‚ÄúFeasibility of Transparent Price Discovery in Tea through Auction in India.‚Äù\nCommodity Insights Yearbook MCX (2019): 44‚Äì52.\nLink | Open Access Link\nBhaduri, Ritwik, Soham Bonnerjee, and Subhrajyoty Roy.\n‚ÄúOnset detection: A new approach to QBH system.‚Äù\narXiv preprint arXiv:1908.07409 (2019).\nOpen Access Link\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "research/talks.html",
    "href": "research/talks.html",
    "title": "Talks & Seminars",
    "section": "",
    "text": "Below is a curated list of invited lectures, conference presentations, public lectures, and special talks delivered over the years."
  },
  {
    "objectID": "research/talks.html#section",
    "href": "research/talks.html#section",
    "title": "Talks & Seminars",
    "section": "2025",
    "text": "2025\n\nLocalized Detection of Authenticity in Mixed Source Texts via an Epidemic Change-Point Perspective\nInvited Talk\nGraduate Student Seminar, Washington University in St.¬†Louis, USA.\nSlide deck\n\n\nGeneralized Alpha-Beta Divergence, its Properties and Associated Entropy\nContributory Talk\nInternational Conference on Robust Statistics (ICORS) 2025, Stresa, Piedmont, Italy.\nSlide deck"
  },
  {
    "objectID": "research/talks.html#section-1",
    "href": "research/talks.html#section-1",
    "title": "Talks & Seminars",
    "section": "2024",
    "text": "2024\n\nRobust Principal Component Analysis using Density Power Divergence\nBest Paper Nominee\nIISA Conference 2024, Cochin University of Science and Technology.\nSlide deck\n\n\nRobust Matrix Factorization using Density Power Divergence and its Applications\nPublic Lecture\nDoctoral Research Lecture, Indian Statistical Institute, Kolkata.\nSlide deck\n\n\nA Novel and Scalable Background Modelling Algorithm for Video Surveillance Data\nContributory Talk\nIMS Asia Pacific Rim Meeting (APRM 2024), University of Melbourne.\nSlide deck\n\n\nA Review of Robust Location and Scatter Estimators\nGuest Lecture\nRobust Statistics Course (M.Stat.‚Äô22 & M.Stat.‚Äô24), Indian Statistical Institute, Kolkata.\nSlide deck"
  },
  {
    "objectID": "research/talks.html#section-2",
    "href": "research/talks.html#section-2",
    "title": "Talks & Seminars",
    "section": "2021",
    "text": "2021\n\nAlgorithmic Fairness of Statistical Decision Systems\nMemorial Lecture\nPrasanta Chandra Mahalanobis Memorial Lecture, Indian Statistical Institute, Kolkata.\nSlide deck"
  },
  {
    "objectID": "research/talks.html#section-3",
    "href": "research/talks.html#section-3",
    "title": "Talks & Seminars",
    "section": "2019",
    "text": "2019\n\nVisualising Multivariate Data using t-SNE\nMemorial Lecture\nD. Basu Memorial Lecture, Indian Statistical Institute, Kolkata.\nSlide deck"
  },
  {
    "objectID": "research/rsvddpd.html",
    "href": "research/rsvddpd.html",
    "title": "Supplementary Material for rSVDdpd",
    "section": "",
    "text": "A basic algorithmic task in automated video surveillance is to separate background and foreground objects. Camera tampering, noisy videos, low frame rate, etc., pose difficulties in solving the problem. A general approach that classifies the tampered frames, and performs subsequent analysis on the remaining frames after discarding the tampered ones, results in loss of information. If the ultimate goal is to perform background modelling, foreground object detection or motion detection etc., then it would be useful to have an algorithm that robustly performs background foreground separation from video surveillance data. Thus, considerable effort has been expended to solve this problem.\n\nThere is a class of statistical background modelling that assumes that the intensity of the pixel values corresponding to the background content is distributed according to a known probability distribution (Gaussian or Mixture of Gaussian). These algorithms are fast, but their performances are not so great.\nBackground modelling based on deep neural network has also been performed. Usually a convolutional neural network with hundreds, thousands or millions of parameters with several layers of connection is built. These are fast for the inference, their performances are great, but they require an enormous amount of data and time for training.\nRecent approaches towards background modelling assumes a decomposition of the video data into a low rank matrix (corresponding to the background content) and a sparse or noisy matrix (corresponding to the foreground content). Matrix decomposition algorithms like robust PCA have been found to be useful here. However, these algorithms have high computational complexity, and performs a convex optimization problem which do not scale well for large scale real life video surveillance data. Also, an integral component of these foreground detection algorithms is singular value decomposition which is nonrobust.\n\nIn this paper, we aim to introduce a scalable, fast, efficient and robust singular value decomposition technique based on the popular minimum density power divergence estimator which naturally extends to a background modelling algorithm. We call this algorithm rSVDdpd. We also demonstrate the superiority of our proposed algorithm on a benchmark dataset (Background Models Challenge or BMC Dataset) and a new real-life video surveillance dataset (University of Houston Camera Tampering or UHCT Dataset) in presence of camera tampering."
  },
  {
    "objectID": "research/rsvddpd.html#extended-abstract",
    "href": "research/rsvddpd.html#extended-abstract",
    "title": "Supplementary Material for rSVDdpd",
    "section": "",
    "text": "A basic algorithmic task in automated video surveillance is to separate background and foreground objects. Camera tampering, noisy videos, low frame rate, etc., pose difficulties in solving the problem. A general approach that classifies the tampered frames, and performs subsequent analysis on the remaining frames after discarding the tampered ones, results in loss of information. If the ultimate goal is to perform background modelling, foreground object detection or motion detection etc., then it would be useful to have an algorithm that robustly performs background foreground separation from video surveillance data. Thus, considerable effort has been expended to solve this problem.\n\nThere is a class of statistical background modelling that assumes that the intensity of the pixel values corresponding to the background content is distributed according to a known probability distribution (Gaussian or Mixture of Gaussian). These algorithms are fast, but their performances are not so great.\nBackground modelling based on deep neural network has also been performed. Usually a convolutional neural network with hundreds, thousands or millions of parameters with several layers of connection is built. These are fast for the inference, their performances are great, but they require an enormous amount of data and time for training.\nRecent approaches towards background modelling assumes a decomposition of the video data into a low rank matrix (corresponding to the background content) and a sparse or noisy matrix (corresponding to the foreground content). Matrix decomposition algorithms like robust PCA have been found to be useful here. However, these algorithms have high computational complexity, and performs a convex optimization problem which do not scale well for large scale real life video surveillance data. Also, an integral component of these foreground detection algorithms is singular value decomposition which is nonrobust.\n\nIn this paper, we aim to introduce a scalable, fast, efficient and robust singular value decomposition technique based on the popular minimum density power divergence estimator which naturally extends to a background modelling algorithm. We call this algorithm rSVDdpd. We also demonstrate the superiority of our proposed algorithm on a benchmark dataset (Background Models Challenge or BMC Dataset) and a new real-life video surveillance dataset (University of Houston Camera Tampering or UHCT Dataset) in presence of camera tampering."
  },
  {
    "objectID": "research/rsvddpd.html#codes",
    "href": "research/rsvddpd.html#codes",
    "title": "Supplementary Material for rSVDdpd",
    "section": "Codes",
    "text": "Codes\n\nThe proposed rSVDdpd algorithm is made available in an R package rsvddpd.\nA simple introductory tutorial for the package is also available in form of a vignette Introduction to rSVDdpd.\nSome of the sample codes are available here."
  },
  {
    "objectID": "research/rsvddpd.html#figures",
    "href": "research/rsvddpd.html#figures",
    "title": "Supplementary Material for rSVDdpd",
    "section": "Figures",
    "text": "Figures\nThe enlarged version of the figures presented in the paper are available here.\n\nBMC Dataset\nThe Background Modelling Challenger (BMC) Dataset is taken from http://backgroundmodelschallenge.eu/ containing some simulated video surveillance data with proper ground truth of foreground and background content. Here we demonstrate the original video, the ground truth mask along with the estimated foregrounds from different algorithms as follows.\n\nSVD: Singular Value Decomposition of the Video Data Matrix\nrSVDdpd: The proposed Robust SVD using Density Power Divergence Algorithm. Reference https://arxiv.org/abs/2109.10680\nRobust PCA: Robust Principal Component Analysis. Reference https://arxiv.org/abs/0912.3599.\nInexact ALM: Inexact Augmented Lagrangian Method for Robust PCA. Reference https://doi.org/10.1080/10556788.2012.700713.\nSRPCP: Sparse Regularized Principal Component Pursuit. Reference http://code.ucsd.edu/~pcosman/Liu_2017-151.pdf.\nVB: Variational Bayesian Method for Robust PCA. Reference https://doi.org/10.1109/TSP.2012.2197748.\nOP: Outlier Pursuit Algorithm. Reference https://doi.org/10.1109/TIT.2011.2173156.\nGoDec: Go Decomposition Algorithm. Reference https://dl.acm.org/doi/10.5555/3104482.3104487.\n\nFollowing are the descriptions of the videos present in the dataset along with the experimental results.\n\n\n\n\n\n\nImportant\n\n\n\nThe videos and the GIF have been compressed at an increased frame per second to save storage. Please convert it to a reduced frame per second video clip for better inspection.\n\n\n\n\n\n\nVideo\nVideo Background\nNoise\nLink\n\n\n\n\nVideo 112\nA street with moving cars\nNoiseless\nDownload\n\n\nVideo 122\nA rotary with moving cars\nNoiseless\nDownload\n\n\nVideo 212\nA street with moving cars\nSlight imperceptible noise in pixel values\nDownload\n\n\nVideo 222\nA rotary with moving cars\nSlight imperceptible noise in pixel values\nDownload\n\n\nVideo 312\nA street with moving cars\nVarying illumination due to movement of the sun\nDownload\n\n\nVideo 322\nA rotary with moving cars\nVarying illumination due to movement of the sun\nDownload\n\n\nVideo 412\nA street with moving cars\nCloud and fog for a brief period\nDownload\n\n\nVideo 422\nA rotary with moving cars\nCloud and fog for a brief period\nDownload\n\n\nVideo 512\nA street with moving cars\nWindy movement of the trees\nDownload\n\n\nVideo 522\nA rotary with moving cars\nWindy movement of the trees\nDownload\n\n\n\n\n\n\nUHCT Dataset\nUniversity of Houston Camera Tampering is a dataset collected from University of Houston‚Äôs surveillance video footage data for two consecutive days across two cameras in the UH campus. Some frames of the videos have been synthentically tampered with noisy artifacts to emulate camera tampering. The dataset is taken from UHCTD: A Comprehensive Dataset for Camera Tampering Detection.\n\n\n\n\nVideo\nDescription\nOriginal Video Link\nResults Link\n\n\n\n\nStream 1\nA small part of the video surveillance footage in Day 2 from Camera B in the UHCT Dataset. Some of the frames have been tampered with random image partially.\nView\nDownload\n\n\nStream 2\nA small part of the nighttime video surveillance footage in Day 1 from Camera A in the UHCT Dataset. Some of the frames have been tampered with random image partially.\nView\nDownload\n\n\nStream 3\nA small part of a daytime video surveillance footage in Day 1 from Camera A in the UHCT Dataset. Some of the frames have been tampered with random image partially. There is also presence of change in intensity and illumination due to the position of the sun.\nView\nDownload\n\n\nStream 4\nA small part of a daytime video surveillance footage in Day 2 from Camera A in the UHCT Dataset. After some time, the camera has been tampered by moving it to point in a different direction. A reliable background modelling algorithm should be sensitive to this rapid change in background and indicate camera tampering.\nView\nDownload"
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "Tasks to-do",
    "section": "",
    "text": "Tasks to-do\n[ ] - Optimize the images for the notes. Too large size.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Indian Statistical Institute, Kolkata | India PhD in Statistics | Aug 2021 - June 2025 During my PhD, I was fortunate to work with Ayanendranath Basu and Abhik Ghosh. My doctoral thesis is about developing fast and scalable algorithms for robust SVD and robust PCA using the minimum density power divergence.\n\nIf you are interested in applying these methods, see this R package or this Python package.\nIf you are more into maths, may be take a look at my PhD thesis here.\n\nIndian Statistical Institute, Kolkata | India M.Stat. | Aug 2019 - July 2021\nIndian Statistical Institute, Kolkata | India B.Stat. | Aug 2016 - July 2019"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "Indian Statistical Institute, Kolkata | India PhD in Statistics | Aug 2021 - June 2025 During my PhD, I was fortunate to work with Ayanendranath Basu and Abhik Ghosh. My doctoral thesis is about developing fast and scalable algorithms for robust SVD and robust PCA using the minimum density power divergence.\n\nIf you are interested in applying these methods, see this R package or this Python package.\nIf you are more into maths, may be take a look at my PhD thesis here.\n\nIndian Statistical Institute, Kolkata | India M.Stat. | Aug 2019 - July 2021\nIndian Statistical Institute, Kolkata | India B.Stat. | Aug 2016 - July 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nWashington Univeristy in St.¬†Louis | Postdoctoral Researcher | Aug 2025 - current\nSysCloud Inc. | Principal Information Researcher | July 2022 - June 2025\n\n\n\n\n\n\nNote\n\n\n\nThe job of an Information Researcher to research, design and build automated systems using algorithms, machine learning, and deep learning techniques. You can think of this as a convex combination of data scientist, solution architect and full-stack developer.\n\n\nSysCloud Inc. | Data Scientist | July 2021 - June 2022"
  },
  {
    "objectID": "about.html#places-i-have-been",
    "href": "about.html#places-i-have-been",
    "title": "About Me",
    "section": "Places I have been",
    "text": "Places I have been\nMost of my travel revolves around visiting academic conferences. So far, I have been to 4 out of 7 continents, and I hope to get to all 7 someday! üó∫Ô∏è‚ÅÄ‡™ú‚úàÔ∏é"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About Me",
    "section": "Hobbies",
    "text": "Hobbies\n\nPre-Stable Diffusion Era\nOne of my hobby was to build various 3D art forms using softwares like Bryce 3D, Daz Studio, Blender, etc. It was really fascinating to me to see how mathematical ideas (e.g.¬†coordinate geometries) are applied in the 3D modelling world. Here‚Äôs a beginner friendly tutorial on this topic.\nHere are some outputs that came out of these activities."
  },
  {
    "objectID": "notes/technicals/optimizer-consistency-proof.html",
    "href": "notes/technicals/optimizer-consistency-proof.html",
    "title": "Proof of Statistical Consistency of an Optimizer",
    "section": "",
    "text": "Suppose you have a function \\(Q(\\theta)\\) and its empirical counterpart is given by \\(Q_n(\\theta)\\). Let \\[\n\\theta^* = \\arg\\max_{\\theta \\in \\Theta} Q(\\theta), \\ \\quad\n\\hat{\\theta}_n = \\arg\\max_{\\theta \\in \\Theta} Q_n(\\theta)\n\\]\n(where one can replace the above by argmin as well). Then, one typical use-case is to show that \\(\\hat{\\theta}_n\\) is close to \\(\\theta^\\ast\\).\nOne trick to do that is as follows:\n\\[\n\\begin{align*}\n    Q(\\theta^*) - Q(\\hat{\\theta}_n)\n    & = Q(\\theta^*) - Q_n(\\theta^*) + Q_n(\\hat{\\theta}_n) - Q(\\hat{\\theta}_n) + (Q_n(\\theta^*) - Q_n(\\hat{\\theta}_n))\\\\\n    & \\leq Q(\\theta^*) - Q_n(\\theta^*) + Q_n(\\hat{\\theta}_n) - Q(\\hat{\\theta}_n)\\\\\n    & \\qquad \\text{ since the last term is negative by definition of } \\hat{\\theta}_n\\\\\n    & \\leq \\vert Q(\\theta^*) - Q_n(\\theta^*) \\vert + \\vert Q_n(\\hat{\\theta}_n) - Q(\\hat{\\theta}_n) \\vert\\\\\n    & 2 \\sup_{\\theta \\in \\Theta} \\vert Q(\\theta) - Q_n(\\theta)\\vert\n\\end{align*}\n\\]\nOften, one can show that this bound is very small (because \\(Q_n\\) is the empirical version of \\(Q\\)), all we need is to strengthen the usual bound to uniform convergence over \\(\\theta\\).\nAt this point, we have \\[\nQ(\\theta^*) - Q_n(\\hat{\\theta}_n) = o_p(1) \\ (\\approx \\text{small})\n\\] as \\(n \\to \\infty\\). This means, if \\(Q\\) is even locally convex near its unique global minima, then that would imply \\(\\hat{\\theta}_n\\) must be close to \\(\\theta^\\ast\\). Note that the significance here is than we do not have to produce any assumption of convexity (or something similar) to the random function \\(Q_n(\\cdot)\\), instead focus on only the deterministic function \\(Q(\\cdot)\\) instead.\nThe typical assumption required here is that there should be no sequence of local minimas of \\(Q\\) that can approximate the global minimum \\(Q(\\theta^*)\\) without approaching \\(\\theta^*\\). See (Van der Vaart 2000) for a more formal statement of this assumption.\n\n\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press."
  },
  {
    "objectID": "notes/technicals/optimizer-consistency-proof.html#references",
    "href": "notes/technicals/optimizer-consistency-proof.html#references",
    "title": "Proof of Statistical Consistency of an Optimizer",
    "section": "",
    "text": "Van der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press."
  },
  {
    "objectID": "notes/concepts/spatial-stat.html",
    "href": "notes/concepts/spatial-stat.html",
    "title": "Spatial Statistics",
    "section": "",
    "text": "Often we find ourselves with a dataset which has a location / spatial information that should be part of the modelling. This can be useful to model the dependence structure of datasets, and this dependence structure can be positively / negatively correlated.\n\nAir Pollution Measurements: Air pollution levels of nearby locations are positively correlated.\nPlant growths: Plants in the nearby locations compete with each other for the same resources, growth of one kind of plant may be negatively correlated with the growth of another.\n\n\n\n\nWhen we observe say \\(Y_n = (Y(s_1), \\dots, Y(s_n))\\) where \\(s_1, \\dots, s_n\\) are the spatial locations, think of this as a single realization of a \\(n\\)-variable random vector, instead of the usual \\(n\\) replications of a univariate random vector. It is not clear at this point, whether any inference is possible based on a single sample observation[1].\nWhen time series methodologies are concerned, the sample indices have a natural ordering, hence \\(Y_t\\) depends only on the past values \\(\\{ Y_s : s &lt; t \\}\\). However, for the spatial case, say even when agricultural plots are arranged in a line to produce a natural ordering, the value \\(Y_{t}\\) may depend on both \\(Y_{t-1}\\) and \\(Y_{t+1}\\).\n\n\n\n\nIn the spatial data, there are two things in concern.\n\nThere is a spatial horizon \\(S\\). From this spatial horizon, some locations \\(s_1, \\dots, s_n\\) are where the samples are observed.\nFor each of these locations, some variables \\(Y(s)\\) is observed.\n\nTherefore, we assume the existence of a stochastic process \\(\\{  Y(s): s \\in S \\}\\).\nThere are 3 major types of spatial data based on the characteristics of this stochastic process.\n\nPoint Patterns: The location themselves are realizations of some stochastic process, e.g., events of earthquakes / volcano.\n\nThe main question here is to try to find patterns / clusters on which locations these events happen or are they happen randomly throughout the spatial horizon.\nUsually, if we just observe the locations alone, these are called unmarked point process. Example is location of volcanos.\nIf we also observe some variables associated with these event locations, these are called marked point process. Example is along with the location of earthquake, we also measure its intensity.\n\nGeostatistical Data: The underlying stochastic process for the \\(Y\\)-variable is defined on the continuous domain \\(S\\). However, due to fixed sampling design, we observe \\(Y(s)\\) only at few designated locations, e.g.¬†\\(Y(s_1), \\dots, Y(s_n)\\).\n\nThe aim is to use the observed values to predict the continuous surface \\(Y(s)\\), and use the neighbouring correlation to improve the prediction.\nExample is to predict the underground oil reserves based on the quantity of oil at few mining locations.\n\nLattice Data: The underlying sampling frame is a fixed designed areal units. In each of these areal units, some \\(Y\\)-observations are made. These areal units could be subplots, counties, census tracts, etc.\n\nExample is agricultural plots are sub-divided into subplots, where different types of crop-yields are observed.\nIf it is census-tract data, then often the aim is to do some kind of regression of one spatial-process on another to explain its variation.\nAnother aim is to predict the observation (called ‚Äúkriging‚Äù) at the unobserved lattice areal units based on the observed values."
  },
  {
    "objectID": "notes/concepts/spatial-stat.html#how-spatial-analysis-is-different",
    "href": "notes/concepts/spatial-stat.html#how-spatial-analysis-is-different",
    "title": "Spatial Statistics",
    "section": "",
    "text": "When we observe say \\(Y_n = (Y(s_1), \\dots, Y(s_n))\\) where \\(s_1, \\dots, s_n\\) are the spatial locations, think of this as a single realization of a \\(n\\)-variable random vector, instead of the usual \\(n\\) replications of a univariate random vector. It is not clear at this point, whether any inference is possible based on a single sample observation[1].\nWhen time series methodologies are concerned, the sample indices have a natural ordering, hence \\(Y_t\\) depends only on the past values \\(\\{ Y_s : s &lt; t \\}\\). However, for the spatial case, say even when agricultural plots are arranged in a line to produce a natural ordering, the value \\(Y_{t}\\) may depend on both \\(Y_{t-1}\\) and \\(Y_{t+1}\\)."
  },
  {
    "objectID": "notes/concepts/spatial-stat.html#types-of-spatial-data",
    "href": "notes/concepts/spatial-stat.html#types-of-spatial-data",
    "title": "Spatial Statistics",
    "section": "",
    "text": "In the spatial data, there are two things in concern.\n\nThere is a spatial horizon \\(S\\). From this spatial horizon, some locations \\(s_1, \\dots, s_n\\) are where the samples are observed.\nFor each of these locations, some variables \\(Y(s)\\) is observed.\n\nTherefore, we assume the existence of a stochastic process \\(\\{  Y(s): s \\in S \\}\\).\nThere are 3 major types of spatial data based on the characteristics of this stochastic process.\n\nPoint Patterns: The location themselves are realizations of some stochastic process, e.g., events of earthquakes / volcano.\n\nThe main question here is to try to find patterns / clusters on which locations these events happen or are they happen randomly throughout the spatial horizon.\nUsually, if we just observe the locations alone, these are called unmarked point process. Example is location of volcanos.\nIf we also observe some variables associated with these event locations, these are called marked point process. Example is along with the location of earthquake, we also measure its intensity.\n\nGeostatistical Data: The underlying stochastic process for the \\(Y\\)-variable is defined on the continuous domain \\(S\\). However, due to fixed sampling design, we observe \\(Y(s)\\) only at few designated locations, e.g.¬†\\(Y(s_1), \\dots, Y(s_n)\\).\n\nThe aim is to use the observed values to predict the continuous surface \\(Y(s)\\), and use the neighbouring correlation to improve the prediction.\nExample is to predict the underground oil reserves based on the quantity of oil at few mining locations.\n\nLattice Data: The underlying sampling frame is a fixed designed areal units. In each of these areal units, some \\(Y\\)-observations are made. These areal units could be subplots, counties, census tracts, etc.\n\nExample is agricultural plots are sub-divided into subplots, where different types of crop-yields are observed.\nIf it is census-tract data, then often the aim is to do some kind of regression of one spatial-process on another to explain its variation.\nAnother aim is to predict the observation (called ‚Äúkriging‚Äù) at the unobserved lattice areal units based on the observed values."
  },
  {
    "objectID": "notes/concepts/practical-data-science.html",
    "href": "notes/concepts/practical-data-science.html",
    "title": "Practical Data Science",
    "section": "",
    "text": "Beyond the usual training of neural networks, there are few major learning variants:\n\nTransfer Learning: Here, you take a pre-trained model, and train only a few top layers to ensure that it adapts to the specific tasks. For example, a pre-trained model capable to comprehension of English language, could be used via transfer learning to summarize paragraphs, or for question-answering.\nFine tuning: Again we take a pretrained model, but train all the weights (or parameters) but only for a small scale dataset (less number of epoch) to make it adhere to the nuisances of that dataset. A pre-trained large image classification model can be fine-tuned to classify between different types of insects and their species, which may be useful for a group of entomologists.\nMulti-task Learning: Here, we take atleast two different tasks, and we want to keep the idea of shared parameters like transfer learning. But, instead of training for one task at a time, we combine the losses arising from two different tasks, and then train the entire network (like fine tuning). The network architecture looks like it has a few shared parameter at the bottom layers, but suddenly it splits off into branches, each adapted to a specific task, and uses different parameters and different set of top layers.\n\n\nAdditionally, there are also different variants that are emerging due to specific needs.\n\nFederated Learning:\n\nModern devices like mobiles and other IoT devices collects lots of data. But most of these data are private.\nDue to privacy concerns, the ML model training cannot use whole of the data as they do not remain in a central place.\nInstead of bringing the data to the models, we bring the model to the data.\nEach IoT device downloads a part of the model, trains it on the private data by computing a few gradient steps, and send back to server.\nThe server aggregates the gradient steps from billions of users, and updates the entire model from an aggregated source of data.\n\nActive Learning:\n\nSuppose we want to build a supervised system, but we do not have a training data with labels, only unlabelled data is present.\nThis means, we need to build the system step-by-step where each step should ideally improve the model.\nActive Learning refers to a paradigm where the sampling is done simulteneously with model training. What this means is the following:\n\nStart with a small human annotated datasets. Large part of the data remains unlabelled.\nTrain a model using that annotated part. We understand that this model is going to be inaccurate. Let it be. Also create some sort of uncertainty measure.\nUse the model to predict the labels / make inference on rest of the dataset, along with confidence level.\nWhichever samples have low confidence level, for them, ask the human annotator to label them.\nRetrain the model on this new dataset.\n\nWhile combining the low-confidence data with the seed data, we can also use the high-confidence data. The labels would be the model‚Äôs predictions. This variant of active learning is called ‚ÄúCo-operative learning‚Äù."
  },
  {
    "objectID": "notes/concepts/practical-data-science.html#learning-paradigms",
    "href": "notes/concepts/practical-data-science.html#learning-paradigms",
    "title": "Practical Data Science",
    "section": "",
    "text": "Beyond the usual training of neural networks, there are few major learning variants:\n\nTransfer Learning: Here, you take a pre-trained model, and train only a few top layers to ensure that it adapts to the specific tasks. For example, a pre-trained model capable to comprehension of English language, could be used via transfer learning to summarize paragraphs, or for question-answering.\nFine tuning: Again we take a pretrained model, but train all the weights (or parameters) but only for a small scale dataset (less number of epoch) to make it adhere to the nuisances of that dataset. A pre-trained large image classification model can be fine-tuned to classify between different types of insects and their species, which may be useful for a group of entomologists.\nMulti-task Learning: Here, we take atleast two different tasks, and we want to keep the idea of shared parameters like transfer learning. But, instead of training for one task at a time, we combine the losses arising from two different tasks, and then train the entire network (like fine tuning). The network architecture looks like it has a few shared parameter at the bottom layers, but suddenly it splits off into branches, each adapted to a specific task, and uses different parameters and different set of top layers.\n\n\nAdditionally, there are also different variants that are emerging due to specific needs.\n\nFederated Learning:\n\nModern devices like mobiles and other IoT devices collects lots of data. But most of these data are private.\nDue to privacy concerns, the ML model training cannot use whole of the data as they do not remain in a central place.\nInstead of bringing the data to the models, we bring the model to the data.\nEach IoT device downloads a part of the model, trains it on the private data by computing a few gradient steps, and send back to server.\nThe server aggregates the gradient steps from billions of users, and updates the entire model from an aggregated source of data.\n\nActive Learning:\n\nSuppose we want to build a supervised system, but we do not have a training data with labels, only unlabelled data is present.\nThis means, we need to build the system step-by-step where each step should ideally improve the model.\nActive Learning refers to a paradigm where the sampling is done simulteneously with model training. What this means is the following:\n\nStart with a small human annotated datasets. Large part of the data remains unlabelled.\nTrain a model using that annotated part. We understand that this model is going to be inaccurate. Let it be. Also create some sort of uncertainty measure.\nUse the model to predict the labels / make inference on rest of the dataset, along with confidence level.\nWhichever samples have low confidence level, for them, ask the human annotator to label them.\nRetrain the model on this new dataset.\n\nWhile combining the low-confidence data with the seed data, we can also use the high-confidence data. The labels would be the model‚Äôs predictions. This variant of active learning is called ‚ÄúCo-operative learning‚Äù."
  },
  {
    "objectID": "notes/concepts/practical-data-science.html#training-techniques",
    "href": "notes/concepts/practical-data-science.html#training-techniques",
    "title": "Practical Data Science",
    "section": "Training Techniques",
    "text": "Training Techniques\n\nCost of Training Large Models\nLet us ask the question:\n\nHow much memory does it take to train a \\(n\\) billion parameter model?\n\nThe key trick is to remember this approximate conversion table:\n\n\\(10^3\\) bytes = 1 thousand bytes = 1 KB\n\\(10^6\\) bytes = 1 million bytes = 1 MB\n\\(10^9\\) bytes = 1 billion bytes = 1 GB\n\nLet us now look at the amount of storage we need:\n\nIf each parameter has float16 representation, they take 16 bits or 2 bytes. Hence, we need \\(2n\\) billion bytes or \\(2n\\) GB of RAM to load the model.\nIf we are training, each parameter will have its own gradient value that we need to store, resulting in another \\(2n\\) GB of storage.\nIf we use Adam optimizer, then we have a formula like:\n\n\\[\n\\hat{\\theta}^{(t+1)} = \\hat{\\theta}^{(t)} - \\alpha \\frac{\\partial L}{\\partial \\theta}\\vert_{\\theta = \\hat{\\theta}^{(t)}} + \\beta \\frac{m_t}{\\sqrt{v_t}}\n\\]\nThese \\(m_t\\) and \\(v_t\\) are momentum and variance parameters, and since we are doing division and taking square roots, we will need more precision for them. Standard is to use float32 for these variables. Hence, storing momentum requires \\(4n\\) GB, and so is variance.\nTogether we need \\((2 + 2 + 4 + 4)n = 12n\\) GB of storage for full-precision training. But for testing, we need only \\(2n\\) GB of storage.\n\n\nFine-tuning Strategies for Large Language Models\nThere have been proposals of various fine-tuning strategies for large language models, that does not require the whole \\(12n\\) GB of storage in your RAM.\n\nLoRA: LoRA (Low Rank Adaptation) is a fine-tuning technique where instead of modifying the parameters (or weights) for the LLM directly, we add a low-rank adjustment on top of it. Say, at a particular layer of the large language model, we need a trainable weights \\(W\\) of dimensions \\(h_1 \\times h_2\\), where \\(h_1\\) and \\(h_2\\) are large. Then, with the fine-tuning, we replace it by \\[\nW + AB^t\n\\]\n\nwhere \\(A\\) is of dimension \\(h_1 \\times r\\) and \\(B\\) is of dimension \\(r \\times h_2\\) and \\(r\\) is a much smaller number than \\(h_1\\) and \\(h_2\\). The matrices \\(A\\) and \\(B\\) are trainable.\n\n\n\n\n\n\nCaution\n\n\n\nA challenge is that both the matrices \\(A\\) and \\(B\\) are not identifiable, for example, \\(AB = cA \\times c^{-1}B\\) for any \\(c \\neq 0\\). Usually, to avoid this, one approach is to always keep the \\(A\\) matrix normalized so that \\(||A||_2 = 1\\).\n\n\n\nLoRA-FA: LoRA with Frozen A. In this case, to avoid the identifiability issue, one may take \\(A\\) to be fixed as the matrix comprising of first \\(r\\) left singular vectors of \\(W\\) corresponding to the largest (in magnitude) \\(r\\) singular values of \\(W\\). Then, only the matrix \\(B\\) is trained.\nVeRA: VeRA (Vector based Random Matrix Adaptation) is a technique where we parametrize the weight matrix \\(W\\) as\n\n\\[\nW = ADB^t + E\n\\]\nwhere \\(A\\) and \\(B\\) are the top \\(r\\) left and right singular vectors and are kept fixed. The diagonal matrix \\(D\\) and error matrix \\(E\\) are trainable. A sparsity structure can be adapted to \\(E\\) to reduce computational cost of training a large number of parameters.\n\n\n\nOptimization using Momentum with Gradient Descent\nSuppose that we have a loss function \\(L(\\theta)\\) that we want to minimize with respect to \\(\\theta\\). Typical gradient descent updates look like:\n\\[\n\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla L(\\theta^{(t)})\n\\]\nTheoretically, under suitably small step sizes (i.e., \\(\\alpha\\)), gradient descent always improves the objective, and converges always to a local minimum. Furthermore, with some additional assumptions, the rate of convergence can be proven to be exponential.\nHowever, in many practical scenarios, you will see that the drop in objective value is significant at first, but becomes lesser and lesser as things approach minima: and the pace of convergence slows down very much. This happens because near the minima, we would have \\(\\nabla L(\\theta) \\approx 0\\), which essentially says that the typical gradient descent will only take very timid steps. But imagine you are going down the hill (the most common analogue how you would describe a gradient descent algorithm) by taking the steepest road, and you are now near the foot of the hill. Would you still look for the steepest road around there, or would you continue down the same way? The momentum says that you can use the previously computed gradients (which essentially gives you some direction where you came from) to help your descent steps navigate the way. This idea is summarized into a moving average type formula:\n\\[\n\\theta^{(t+1)}\n= \\theta^{(t)} - \\alpha \\nabla L(\\theta^{(t)}) - \\alpha \\sum_{k=1}^t \\beta^k \\nabla L(\\theta^{(t-k)})\n\\]\nAt this point, you may be concerned with the fact that implementing this rule will require one to store all the previously computed gradients, i.e., \\(\\nabla L(\\theta^{(t-k)})\\) for \\(k = 1, 2, \\dots, t\\) which can be very challenging and quickly run into memory issues. However, simple algebraic calculations can reduce the above daunting equation into two simpler nice equations to avoid this problem:\n\\[\n\\begin{align*}\nz^{(t+1)} & = \\beta z^{(t)} + \\nabla L(\\theta^{(t)})\\\\\n\\theta^{(t+1)} & = \\theta^{(t)} - \\alpha z^{(t+1)}\n\\end{align*}\n\\]\nThis is essentially the momentum trick. A nice visualization along with a detailed example is available by Goh (2017) on this topic.\n\n\nGradient Accumulation\nGradient accumulation is a simple technique by which you can perform neural network training using smaller batches but effectively get the benefit of using a bigger batch size.\nIn standard training, you might want a batch size of 64 for stable convergence. However, loading 64 complex examples (like high-res images or long text sequences) into your GPU‚Äôs VRAM at once might cause it to crash. You are forced to use a smaller batch size (e.g., 8), but this can make your gradient updates ‚Äúnoisy‚Äù and unstable, leading to poor training performance.\nGradient accumulation solves this by breaking the large batch down but deferring the update step. Instead of updating the model weights after every small batch of 8, you run the small batch, calculate the gradients, and add them to a ‚Äúbucket‚Äù of gradients. You repeat this \\(8\\) times (\\(8\\) batches \\(\\times\\) \\(8\\) samples = \\(64\\) samples). Only after the \\(8\\)-th small batch do you actually update the model weights using the total accumulated gradients. A pytorch implementation of this would look like this:\naccumulation_steps = 8             # Define how many steps to wait\n\nfor i, (data, label) in enumerate(loader):\n    output = model(data)           # Forward\n    loss = criterion(output, label)\n    loss = loss / accumulation_steps # Normalize loss to account for the accumulation\n    loss.backward()                # Backward (grads are added to existing grads)\n\n    # Only update weights every 4 steps\n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()           # Update weights using accumulated grads\n        optimizer.zero_grad()      # Clear grads"
  },
  {
    "objectID": "notes/concepts/practical-data-science.html#topics",
    "href": "notes/concepts/practical-data-science.html#topics",
    "title": "Practical Data Science",
    "section": "Topics",
    "text": "Topics\n\nModel Parallelism\n\n\nDropout Layers Discussion\n\n\nKnowledge Distillation\n\n\nActivation Pruning\n\n\nML Models Testing + Deployment Strategy\n\n\nCross-validation Techniques\n\n\nHuman Labelling for Baselining\n\n\nLeaky Variables\n\n\nFeature Engineering Techniques\n\nCyclical Features\nCategorical Data Encoding Techniques\n\n\n\nFeature Importance\nTalk about Predictive Power Score\nIdentify Drift using Proxy Labelling\n\n\nModel Simplification\nHow to convert a random forest model to a single decision tree?"
  },
  {
    "objectID": "notes/concepts/practical-data-science.html#acknowledgements",
    "href": "notes/concepts/practical-data-science.html#acknowledgements",
    "title": "Practical Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany materials covered here are taken from the website by Chawla (2025)."
  },
  {
    "objectID": "notes/concepts/practical-data-science.html#references",
    "href": "notes/concepts/practical-data-science.html#references",
    "title": "Practical Data Science",
    "section": "References",
    "text": "References\n\n\nChawla, Avi. 2025. ‚ÄúDaily Dose of Data Science.‚Äù Daily Dose of Data Science. https://www.dailydoseofds.com/.\n\n\nGoh, Gabriel. 2017. ‚ÄúWhy Momentum Really Works.‚Äù Distill. https://doi.org/10.23915/distill.00006."
  }
]