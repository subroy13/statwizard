<!DOCTYPE html>
<html lang="en">
<head>
    
    
    
    <link rel="preconnect" href="https://www.googletagmanager.com">
    <link rel="preconnect" href="https://www.google-analytics.com">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-97N9TLJ517"></script>
    <script async>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-97N9TLJ517');
    </script>
    

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="StatWizard: Access beginner-friendly blog posts covering various applications of statistics, data science, computer programming and finance. Also hosts the personal website of the author, Subhrajyoty Roy.">
    <link rel="icon" href="/svg/avatar.svg">
    <title>
        
Texture Networks

    </title>

    

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>


<style>
   
  @font-face {
    font-family: 'Roboto';
    font-style: italic;
    font-weight: 100;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOiCnqEu92Fr1Mu51QrEzAdLw.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: italic;
    font-weight: 500;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOjCnqEu92Fr1Mu51S7ACc6CsQ.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: italic;
    font-weight: 900;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOjCnqEu92Fr1Mu51TLBCc6CsQ.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: normal;
    font-weight: 100;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOkCnqEu92Fr1MmgVxIIzI.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: normal;
    font-weight: 500;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmEU9fBBc4.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: normal;
    font-weight: 900;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmYUtfBBc4.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }
</style>



<script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">




<link rel="stylesheet" type="text/css" href="/css/index.e4514e42feee311b0cc9bf34f860b62c.css" />






<script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>


<style>
  html {
      scroll-behavior: smooth;
  }
</style>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/>

     
     
</head>
<body>
    <div class="app-container min-h-[100vh] flex flex-col justify-between">
        <nav class="bg-white shadow-xl z-50">
  
  <div
    class="hidden mx-auto max-w-7xl py-2 px-4 md:px-6 lg:px-8 md:flex flex-row items-center"
  >
    <div class="w-[200px] flex items-center">
      <a
        class="uppercase font-extrabold font-mono text-2xl flex flex-row justify-center items-center gap-2"
        href="/"
      >
        <img
          src="/images/logo-wide-resized.png"
          height="50px"
          width="100px"
          class="inline-block"
          alt="StatWizard Logo"
        />
      </a>
    </div>
    <div class="w-full flex flex-row justify-center items-center gap-4">
      
      <a
        href="/"
        class="px-4 py-2 border rounded-lg hover:bg-gray-100"
      >
        Home
      </a>
      
      <a
        href="/posts"
        class="px-4 py-2 border rounded-lg hover:bg-gray-100"
      >
        Posts
      </a>
      
      <a
        href="/aboutme"
        class="px-4 py-2 border rounded-lg hover:bg-gray-100"
      >
        About Me
      </a>
      
      <a
        href="/#contact-me"
        class="px-4 py-2 border rounded-lg hover:bg-gray-100"
      >
        Contact Me
      </a>
      
    </div>
    <div class="w-[200px] flex flex-row justify-around items-center">
      
      <a
        href="mailto:subhrajyotyroy@gmail.com"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fas fa-envelope for link mailto:subhrajyotyroy@gmail.com"
        target="_blank"
      >
        <i class="fas fa-envelope fa-lg"></i>
      </a>
      
      <a
        href="https://www.facebook.com/subroy13/"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fab fa-facebook for link https://www.facebook.com/subroy13/"
        target="_blank"
      >
        <i class="fab fa-facebook fa-lg"></i>
      </a>
      
      <a
        href="https://github.com/subroy13"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fab fa-github for link https://github.com/subroy13"
        target="_blank"
      >
        <i class="fab fa-github fa-lg"></i>
      </a>
      
      <a
        href="https://www.linkedin.com/in/subroy13"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fab fa-linkedin-in for link https://www.linkedin.com/in/subroy13"
        target="_blank"
      >
        <i class="fab fa-linkedin-in fa-lg"></i>
      </a>
      
      <a
        href="#"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fab fa-instagram for link #"
        target="_blank"
      >
        <i class="fab fa-instagram fa-lg"></i>
      </a>
      
    </div>
  </div>

  
  <div
    class="flex py-4 px-2 max-w-7xl flex-row justify-end items-center md:hidden"
  >
    <div class="w-full flex justify-center items-center">
      <p class="uppercase font-extrabold font-mono text-2xl">
        StatWizard
      </p>
    </div>
    <div class="w-[30px] mr-4 relative">
      <button
        type="button"
        id="mobile-menu-button"
        class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:bg-gray-700 hover:text-white focus:outline-none"
        aria-controls="mobile-menu"
        aria-expanded="false"
      >
        <span class="sr-only">Open main menu</span>
        
        <svg
          id="mobile-menu-close-icon"
          class="bg-white text-neutral-800 outline-none block h-6 w-6"
          fill="none"
          viewBox="0 0 24 24"
          stroke-width="1.5"
          stroke="currentColor"
          aria-hidden="true"
        >
          <path
            stroke-linecap="round"
            stroke-linejoin="round"
            d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"
          />
        </svg>
        
        <svg
          id="mobile-menu-open-icon"
          class="bg-white text-neutral-800 outline-none hidden h-6 w-6"
          fill="none"
          viewBox="0 0 24 24"
          stroke-width="1.5"
          stroke="currentColor"
          aria-hidden="true"
        >
          <path
            stroke-linecap="round"
            stroke-linejoin="round"
            d="M6 18L18 6M6 6l12 12"
          />
        </svg>
      </button>

      <div
        id="mobile-menu"
        class="absolute end-0 mt-4 -mr-4 bg-white w-[90vw] hidden"
      >
        <ul class="flex flex-col">
          
          <li
            class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100"
          >
            <a href="/" class=""> Home </a>
          </li>
          
          <li
            class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100"
          >
            <a href="/posts" class=""> Posts </a>
          </li>
          
          <li
            class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100"
          >
            <a href="/aboutme" class=""> About Me </a>
          </li>
          
          <li
            class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100"
          >
            <a href="/#contact-me" class=""> Contact Me </a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</nav>


        <div class="content-container">
            



    <div class="relative h-[50vh] max-h-[300px] w-full bg-cover bg-center bg-no-repeat bg-fixed rounded-b-md" 
        style = "background-image: url('/posts/texture-network-style-transfer/featured.webp')">
        
            <div class = "absolute top-0 left-0 bg-neutral-800/100 w-fit text-white p-1 ml-1 font-xs rounded-md">
                <p>Cover image taken from <a href="https://heartbeat.fritz.ai/art-soul-part-1-a-style-transfer-website-based-on-tkinter-and-django-7a897741618">post</a> by Nikita Sharma</p>
            </div>
        
    </div>



<div class="mt-4 flex flex-col gap-4 mx-2 px-2 md:mx-4 md:px-8">
    <h1 class="text-3xl text-center text-neutral-600 font-bold md:px-8 md:pt-6">
        Texture Networks
    </h1>
    <div class="m-2 p-4 rounded-lg shadow-lg border-2">
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="flex flex-col justify-center items-center gap-2">   
                <div class="mt-1 flex flex-row text-neutral-800 gap-4">
                    <p class="font-bold">Date:</p>
                    <p class="font-normal">13 January, 2021</p>
                </div> 
                <p class="mt-1 text-neutral-600">
                    <span class="font-bold">32</span> minutes read
                </p>
                <div class="my-1">
                    
                        <a href="/tags/image-processing/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Image Processing
                        </a>
                    
                        <a href="/tags/deep-learning/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Deep Learning
                        </a>
                    
                </div>    
            </div>
            <div class="flex flex-col justify-start gap-2">
                <h1 class="text-lg font-bold">Prerequisites</h1>
                <ul class="space-y-2 text-left">
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Python Programming - </span>
                        
                            <span class="text-green-600 font-semibold">Beginner</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Tensorflow and Keras - </span>
                        
                            <span class="text-yellow-600 font-semibold">Intermediate</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Neural Networks - </span>
                        
                            <span class="text-green-600 font-semibold">Beginner</span>
                        
                    </li>
                    
                </ul>
            </div>    
        </div>
        <p class="mt-4 p-4 text-sm">
            <span class="font-semibold">Summary: </span> In this post, I analyse Texture Network, a special type of neural network to synthesize textures and performing Neural Style Transfer, the art of mixing the content of a picture with the style from another picture.
        </p>
    </div>
    <div class="grid grid-cols-1 md:grid-cols-4 gap-2">
        <div class="relative col-span-1">
            <div class="sticky top-0 left-0 w-full pl-4 pt-4">
                <h1 class="text-bold text-xl">Table of Contents</h1>
                <div class="prose">
                    <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-a-tensor">What is a tensor?</a></li>
    <li><a href="#what-an-image-is-do-to-with-tensor">What an Image is do to with Tensor?</a></li>
  </ul>

  <ul>
    <li><a href="#choice-of-descriptor-network">Choice of Descriptor Network</a></li>
    <li><a href="#creating-generator-network">Creating Generator Network</a></li>
    <li><a href="#why-do-we-need-convolution">Why do we need Convolution?</a></li>
    <li><a href="#designing-convolutional-block">Designing Convolutional Block</a></li>
    <li><a href="#designing-join-block">Designing Join Block</a></li>
    <li><a href="#completing-the-generator">Completing the Generator</a></li>
  </ul>

  <ul>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
                </div>
            </div>
        </div>
        <div class="col-span-3">
            <div class="text-justify break-words w-full">
                <div class="prose" style="max-width: none;">
                    <p>This is a rather long description, but it is about something that really interests me, about how to mix the content of a picture with the style from another picture, called Neural Style Transfer. <strong>Texture Networks</strong> is a Neural Network approach devised by Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi and Victor Lempitsky in 2016, which is extremely useful to synthesize new types of textures (which is extensively used in the production of clothing with exclusive designs), as well as being able to work as a style transfer mechanism.</p>
<h1 id="what-is-neural-style-transfer">What is Neural Style Transfer?</h1>
<p>I think the answer to this question is better to show visually, rather than talking about it.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='out1.png'>
</img>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='out2.png'>
</img>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='out3.png'>
</img>
<p>So, I think now one should get the idea. Neural Style transfer combines the aesthetics of an image on to another image (here the image of the girl named Karya, which has been provided by <a href="https://github.com/DmitryUlyanov/texture_nets">Dmitry Ulyanov&rsquo;s Github</a>), retaining the content of the image (i.e. retaining the girl in the stylized output). Note that, the effect is particularly visible in 2nd and 3rd images, whereas, for the first image, the style aspect is greatly emphasized.</p>
<p>I am going to discuss exactly how I created those stylized images, and hopefully, after reading this, you would be able to reproduce similar results with images of your choice.</p>
<h1 id="prerequisites">Prerequisites</h1>
<p>I am not talking about things you need to know beforehand to understand the intricate details of the mechanism, but the software requirements that I will be using to create something similar to texture networks. So, at the very beginning, I import all the required packages in <code>python</code>. Also, I shall be using an <strong>NVidia GeForce GTX 1060Ti</strong> Graphics card with CUDA computing capability 6.1, which is not at par with the GPU devices used at a professional level, but this speeds up the computations by a lot rather than using a CPU.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> time
<span style="color:#f92672">import</span> functools
<span style="color:#f92672">import</span> PIL.Image
<span style="color:#f92672">import</span> IPython.display <span style="color:#f92672">as</span> display
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tf<span style="color:#f92672">.</span>__version__
</code></pre></div><pre><code>'2.1.0-rc0'
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tf<span style="color:#f92672">.</span>test<span style="color:#f92672">.</span>gpu_device_name()
</code></pre></div><pre><code>'/device:GPU:0'
</code></pre>
<h1 id="making-some-utility-functions">Making Some Utility Functions</h1>
<p>Before proceeding with describing how the Texture Network is created, I would create some utility functions to help us later. For starters, these utility functions will allow us to load images from a path, and visualize an image given its tensor.</p>
<h2 id="what-is-a-tensor">What is a tensor?</h2>
<p>Let us revisit some high school mathematics for a bit. We know that matrix is a 2-dimensional array of numbers, if simply put. <a href="https://en.wikipedia.org/wiki/Tensor">Tensor</a> is a generalization of that, it is an n-dimensional array.</p>
<p>Now, when a mathematician introduces matrix, it is essentially an efficient way of representing linear functions from a vector space to another vector space. A <strong>Vector Space</strong> is a space comprised of vectors, and a vector is something that satisfies some mathematical properties. But, we don&rsquo;t need that. Think of vectors in the most simple way, it is something that has a magnitude and a direction, like speed.</p>
<p>Coming back to tensor, it is introduced as an efficient representation of <strong>Multilinear Maps</strong> between vector spaces. Let $V_1, V_2, \dots V_n, W$ be some vector spaces, then, a function $f : V_1 \times V_2 \times\dots V_n \rightarrow W$ is said to be a multilinear map, if $f(v_1, v_2, \dots v_i, \dots v_n)$ is linear in $v_i$ given all other arguments $v_1, \dots v_{i-1}, v_{i+1}, \dots v_n$ are fixed. And such linearity holds of any of its arguments. So, if you are comfortable with the mathematics of Linear Algebra and Matrices, then you would clearly understand that tensor is just a multidimensional generalization of matrices.</p>
<h2 id="what-an-image-is-do-to-with-tensor">What an Image is do to with Tensor?</h2>
<p>Now, to understand how tensor comes into play to define images, one needs to understand the mechanism of how an image is stored digitally. For this, consider a black grid like chessboard, but all cells are coloured white. Now, you start colouring some cells to black, and then you would be able to generate some pictures with tons of block like artifacts. The following example from <a href="http://logicalzero.com/gamby/reference/image_formats.html">logicalzero.com</a> shows such a smiley face just colouring a 8x8 grid.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='smiley.png'>
</img>
<p>A smiley was okay, but it was not very appealing. Now, since we have a 8x8 grid, and each of the cells can be coloured in 2 ways, black or white. Hence, by simple combinatorics, this generates $2^{256}$ possible images, out of this 8x8 grid, which is about $1.15 \times 10^{77}$. That&rsquo;s a lot! However, not all such combinations will result in visually appealing images, something that we can actually call as a potential image with our natural sense. So, among these vast majority of combinations, only a few will make up images, that our brain can visualize and understand as an image.</p>
<p>However, if we wish to create more complicated images, we need a bigger grid. The reason being that these 8x8 grid cannot be used to approximate complicated curves in the image we encounter in daily life. For instance, increasing the number of cells in the grid, we can create an image of a panda.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='nonogram.png'>
</img>
<p><em>Note: This image turns out to be a solution of a puzzle called <a href="https://en.wikipedia.org/wiki/Nonogram">Nonogram</a>, which is also called as Picross or Visual Crosswords or Japanese Crosswords.</em></p>
<p>Using finer grids actually results in a better picture, as you can see. Digital black and white images are represented using the technique described above, and each of the grid cell is called a <a href="https://en.wikipedia.org/wiki/Pixel">Pixel</a>. Now, note that, we can represent this grid using a 2-dimensional matrix of 0&rsquo;s and 1&rsquo;s, where a white pixel would be represented as 0 and a black pixel would be repesented as 1. For instance, the smiley image can be matrixified like this:</p>
<div class="p-4 grid grid-cols-1 md:grid-cols-2 gap-4">
    <img src="smiley.png"></img>
    <div class = "w-full">
        $$\begin{bmatrix}
        0 & 0 & 1 & 1 & 1 & 1 & 0 & 0\\\\
        0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\\\
        1 & 0 & 1 & 0 & 1 & 0 & 0 & 1\\\\
        1 & 0 & 1 & 0 & 1 & 0 & 0 & 1\\\\
        1 & 0 & 0 & 0 & 0 & 1 & 0 & 1\\\\
        1 & 0 & 1 & 1 & 1 & 0 & 0 & 1\\\\
        0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\\\
        0 & 0 & 1 & 1 & 1 & 1 & 0 & 0\\\\
        \end{bmatrix}$$
    </div>
</div>
<p>Now, we shall use colour images in this context of Neural Style Transfer. To represent a colour image, we require 3 such matrices. One for Red channel, one for Blue channel and another for Green Channel. Also, the elements of the matrices will be allowed to take values between 0 and 255 (to be represented by 8 digit binary numbers) or to take any real value between 0 and 1, representing the denisty of the colour. For instance, in the above black and white images, we can put the value 0.5 in some elements to represent that those pixels should be coloured using gray, which is a colour midway between black and white. Hence, allowing floating point values would ensure a richer distribution of images.</p>
<p>Coming back to the link between images and tensor, an image is represented by 3 such matrices, in combination, a 3-dimensional tensor, which the dimension or shape being (3, height of the image, width of the image), where 3 being number of channels.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tensor_to_image</span>(tensor):
    tensor <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>clip_by_value(tensor, clip_value_min<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, clip_value_max<span style="color:#f92672">=</span><span style="color:#ae81ff">255.0</span>)
    tensor <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(tensor, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>uint8)   <span style="color:#75715e"># convert tf array to np array of integers</span>
    <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>ndim(tensor)<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">3</span>:
        <span style="color:#66d9ef">assert</span> tensor<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>  <span style="color:#75715e"># asserts that the BATCH_SIZE = 1</span>
        tensor <span style="color:#f92672">=</span> tensor[<span style="color:#ae81ff">0</span>]   <span style="color:#75715e"># take the first image</span>
    <span style="color:#66d9ef">return</span> PIL<span style="color:#f92672">.</span>Image<span style="color:#f92672">.</span>fromarray(tensor)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_img</span>(path_to_img, rescale <span style="color:#f92672">=</span> False):
    <span style="color:#75715e"># we rescale the image to max dimension 256 for fasters processing</span>
    max_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>    
    img <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>io<span style="color:#f92672">.</span>read_file(path_to_img)   <span style="color:#75715e"># read the image</span>
    img <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>decode_image(img, channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)    <span style="color:#75715e"># decode into image content</span>
    img <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>convert_image_dtype(img, tf<span style="color:#f92672">.</span>float32)    <span style="color:#75715e"># convert to float</span>
    <span style="color:#66d9ef">if</span> rescale:
        img <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(img, tf<span style="color:#f92672">.</span>constant([max_dim, max_dim]))
    <span style="color:#66d9ef">else</span>:
        shape <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>shape(img)[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], tf<span style="color:#f92672">.</span>float32)   
        <span style="color:#75715e"># get the shape of image, cast it to float type for division, expect the last channel dimension</span>
        long_dim <span style="color:#f92672">=</span> max(shape)
        scale <span style="color:#f92672">=</span> max_dim <span style="color:#f92672">/</span> long_dim    <span style="color:#75715e"># scale accordingly</span>
        new_shape <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(shape <span style="color:#f92672">*</span> scale, tf<span style="color:#f92672">.</span>int32)   <span style="color:#75715e"># cast the new shape to integer</span>
        img <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(img, new_shape)   <span style="color:#75715e"># resize image</span>
    img <span style="color:#f92672">=</span> img[tf<span style="color:#f92672">.</span>newaxis, :]   <span style="color:#75715e"># newaxis builts a new batch axis in the image at first dimension</span>
    <span style="color:#66d9ef">return</span> img
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">style_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;../input/artistic-style-transfer/pattern-rooster.jpg&#39;</span>
content_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;../input/artistic-style-transfer/celebGAN_male.png&#39;</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">content_image <span style="color:#f92672">=</span> load_img(content_path, rescale <span style="color:#f92672">=</span> True)
tensor_to_image(content_image <span style="color:#f92672">*</span> <span style="color:#ae81ff">255.0</span>)
</code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='index_13_0.png'>
</img>
<p>This is the content image, on which we shall apply some style. Note that, the loaded images have elements between 0 and 1, while the <code>tensor_to_img</code> function takes in a tensor with values between 0 and 255, hence we need to multiply all the elements by 255 to convert it to the crucial range where it can be visualized.</p>
<p>Surprisingly, the image I am using is actually an image generated via Neural Network called <a href="https://arxiv.org/abs/1710.10196">Progressive GAN</a>, which is really interesting work by Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, who trained a network on <strong>Celeb A</strong> dataset to generate images of new celebrities, who do not exist in real life.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">style_image <span style="color:#f92672">=</span> load_img(style_path, rescale <span style="color:#f92672">=</span> False)
tensor_to_image(style_image <span style="color:#f92672">*</span> <span style="color:#ae81ff">255.0</span>)
</code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='index_15_0.png'>
</img>
<p>This is the style image that we can going to use. So, we think, the final image would look like the image of the artificial celebrity, tessalatted like the style image.</p>
<h1 id="architecture-of-texture-network">Architecture of Texture Network</h1>
<p>Texture Network comprised of two main components.</p>
<ol>
<li>A Generator Network.</li>
<li>A Descriptor Network.</li>
</ol>
<p>A <strong>Generator Network</strong> is a neural network which takes input of the content image and some random noise, and output our desired stylized image.</p>
<p>A <strong>Descriptor Network</strong> is a neural network which takes input of the desired stylized image, and then try to figure out the underlying style and content of the stylized image, and try to match it with the original style image and content image.</p>
<p>If you are familiar with <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Networks</a> (GAN), then you would find a lot similarity of Texture Network with GANs. However, unlike to the case of GAN, here, the descriptor network will not be trained, but will be used to measure the style and content of the stylized images generated by Generator Network.</p>
<p>To understand why such a descriptor network is needed at all, consider an image of a person. If one shifts the image just by one pixel to the left, then using a simple squared error loss between the original image and shifted image would become large, however, from our perception, both images would look identical. Hence, to actually compare the two images, we specifically need to compare high level representations of the images, which will be provided by the descriptor network.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='texnet.png'>
</img>
<h2 id="choice-of-descriptor-network">Choice of Descriptor Network</h2>
<p>We use <a href="https://arxiv.org/abs/1409.1556">VGG19</a> as our descriptor network as it is a Very Deep Convolutional Networks for Large-Scale Image Recognition. It was developed by Karen Simonyan, Andrew Zisserman in 2014, and is trained with the ImageNet dataset, comprising of millions of images. Hence, high level features of this network will be an acurate representation of the style and content of images.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">vgg <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>applications<span style="color:#f92672">.</span>VGG19(include_top<span style="color:#f92672">=</span>False, weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;imagenet&#39;</span>)   <span style="color:#75715e"># Load VGG19 pretrained Network from Keras</span>
<p><span style="color:#66d9ef">print</span>()
<span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> vgg<span style="color:#f92672">.</span>layers:
<span style="color:#66d9ef">print</span>(layer<span style="color:#f92672">.</span>name)   <span style="color:#75715e"># print layer names so that we can reference them later</span>
</code></pre></div><pre><code>Downloading data from <a href="https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5">https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5</a>
80142336/80134624 [==============================] - 1s 0us/step</p>
<p>input_1
block1_conv1
block1_conv2
block1_pool
block2_conv1
block2_conv2
block2_pool
block3_conv1
block3_conv2
block3_conv3
block3_conv4
block3_pool
block4_conv1
block4_conv2
block4_conv3
block4_conv4
block4_pool
block5_conv1
block5_conv2
block5_conv3
block5_conv4
block5_pool
</code></pre></p>
<p>Note that, VGG19 has a sturcture called a Convolutional Block. Each such block consists of 5 layers of neuron, the first 4 layers being convolutional layers, and the last layers being a pooling layer. The top layer which was excluded from the loaded model is specifically a hidden dense layer that connects the final pooled layer to the output layer, which outputs the classification of the Imagenet image. However, we do not need this final hidden dense layer.</p>
<h2 id="creating-generator-network">Creating Generator Network</h2>
<p>To build the generator network, we need something called a Circular Convolution. To understand various convolutional arithmetic properly, I would recommend checking out the following resources.</p>
<ol>
<li><a href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a> contains a simple animation showing convolutions with different parameters.</li>
<li><a href="https://ezyang.github.io/convolution-visualizer/index.html">https://ezyang.github.io/convolution-visualizer/index.html</a> has a really good interactive environment where you can set the parameters and take look at the corresponding convolutional operation.</li>
</ol>
<p>Coming back to circular convolution, it is a simple 2-dimensional convolution with with a particular type of padding called <strong>Circular Padding</strong>. It essentially wraps the image from top to bottom and from left to right.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">periodic_padding</span>(x, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
    <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">    x: shape (batch_size, d1, d2)
</span><span style="color:#e6db74">    return x padded with periodic boundaries. i.e. torus or donut
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    d1 <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#75715e"># dimension 1: height</span>
    d2 <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">2</span>] <span style="color:#75715e"># dimension 2: width</span>
    p <span style="color:#f92672">=</span> padding
    <span style="color:#75715e"># assemble padded x from slices</span>
    <span style="color:#75715e">#            tl,tc,tr</span>
    <span style="color:#75715e"># padded_x = ml,mc,mr</span>
    <span style="color:#75715e">#            bl,bc,br</span>
    top_left <span style="color:#f92672">=</span> x[:, <span style="color:#f92672">-</span>p:, <span style="color:#f92672">-</span>p:] <span style="color:#75715e"># top left</span>
    top_center <span style="color:#f92672">=</span> x[:, <span style="color:#f92672">-</span>p:, :] <span style="color:#75715e"># top center</span>
    top_right <span style="color:#f92672">=</span> x[:, <span style="color:#f92672">-</span>p:, :p] <span style="color:#75715e"># top right</span>
    middle_left <span style="color:#f92672">=</span> x[:, :, <span style="color:#f92672">-</span>p:] <span style="color:#75715e"># middle left</span>
    middle_center <span style="color:#f92672">=</span> x <span style="color:#75715e"># middle center</span>
    middle_right <span style="color:#f92672">=</span> x[:, :, :p] <span style="color:#75715e"># middle right</span>
    bottom_left <span style="color:#f92672">=</span> x[:, :p, <span style="color:#f92672">-</span>p:] <span style="color:#75715e"># bottom left</span>
    bottom_center <span style="color:#f92672">=</span> x[:, :p, :] <span style="color:#75715e"># bottom center</span>
    bottom_right <span style="color:#f92672">=</span> x[:, :p, :p] <span style="color:#75715e"># bottom right</span>
    top <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>concat([top_left, top_center, top_right], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
    middle <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>concat([middle_left, middle_center, middle_right], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
    bottom <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>concat([bottom_left, bottom_center, bottom_right], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
    padded_x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>concat([top, middle, bottom], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">return</span> padded_x
</code></pre></div><p>In the function <code>periodic_padding</code>, we give several input images, however, with only one channel for each image. Here, the input is a tensor of the shape (number of images, height of the image, width of the image). Hence, we can think of the input as if we are passing many 2-dimensional matrices of shape (height of the image, width of the image). We also pass the amount of padding that we want. Let us see what it outputs, when we pass a single 2D matrix as follows;</p>
<p>$$\begin{bmatrix}
1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9
\end{bmatrix}$$</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">a <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([[[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">6</span>], [<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">9</span>]]])
periodic_padding(a)
</code></pre></div><pre><code class="language-console" data-lang="console">    &lt;tf.Tensor: shape=(1, 5, 5), dtype=int32, numpy=
    array([[[9, 7, 8, 9, 7],
            [3, 1, 2, 3, 1],
            [6, 4, 5, 6, 4],
            [9, 7, 8, 9, 7],
            [3, 1, 2, 3, 1]]], dtype=int32)&gt;
</code></pre><p>Note that, the output tensor has size 5x5, which it obtained by one unit of padding in top, bottom, left and right. Also note that, to the left of 1, we have 3, hence it is as if the rightmost column of the original matrix is wrapped to the left side of the matrix. Similar wrapping is also seen in vertical direction.</p>
<p>However, for an image, we need to perform this circular padding for each of the channel. Hence, we first split up the 3 channels of the image, then append circular padding for each one, and finally combine them together. This is done through <code>CircularPadding</code> function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">CircularPadding</span>(inputs, kernel_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Prepares padding for Circular convolution&#34;&#34;&#34;</span>
    <span style="color:#75715e"># split all the filters</span>
    n_filters_in <span style="color:#f92672">=</span> inputs<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
    input_split <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>split(inputs, n_filters_in, axis <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    output_split <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> part <span style="color:#f92672">in</span> input_split:
        part <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>squeeze(part, axis <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        outs <span style="color:#f92672">=</span> periodic_padding(part, padding <span style="color:#f92672">=</span> int(kernel_size <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>))
        outs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>expand_dims(outs, axis <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        output_split<span style="color:#f92672">.</span>append(outs)
    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>concat(output_split, axis <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</code></pre></div><img 
    class='max-w-lg mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='convolution.png'>
</img>
<p>The diagram above shows the main idea of a convolutional layer. Let us say, we have an image, represented by a 5x5x3 tensor (pretty bad for visualizing, but pretty good for understanding), and we wish to perform convolution of this image with a 3x3 kernel. Then the convolution is basically a weighted combination of all the neighbouring pixels from all the layers. To understand mathematically, let us introduce some notations.</p>
<p>Let $X$ denote the tensor image, and $X_{ijc}$ denote the value of the $(i, j)$-th pixel at the $c$-th channel. Now, let us say we wish to find out the value of the convolution at $(i_0, j_0)$ cell. Then, the convolution is defined as;</p>
<p>$$H(i_0, j_0) = b + \sum_{(i, j) \in N(i_0, j_0)}\sum_{c} X_{ijc} K((i - i_0), (j - j_0), c)$$</p>
<p>where $K(\cdot, \cdot, \cdot)$ is kernel weights i.e. some parameters of the network which the network is going to learn. Also, the parameter $b$ is the bias term and $N(i_0, j_0)$ is a neighbourhood of the pixel $(i_0, j_0)$. Therefore, if we have an image with $n$ channels, and we convolute a $k\times k$ kernel on it, then we specifically require $(k^2n + 1)$ parameters including a bias term.</p>
<h2 id="why-do-we-need-convolution">Why do we need Convolution?</h2>
<p>To understand convolution better, consider two vectors $\textbf{x} = [x_1, x_2, \dots x_n]$ and $\textbf{y} = [y_1, y_2, \dots y_n]$, then the dot product between them is defined as;</p>
<p>$$\textbf{x}\cdot\textbf{y} = \sum_{k=1}^{n} x_ky_k$$</p>
<p>That&rsquo;s high school algebra. However, we also know that, dot product measures the similarity between the vectors $\textbf{x}$ and $\textbf{y}$, i.e. it is maximum when $\textbf{x},\textbf{y}$ are collinear, and is minimum when these are orthogonal to each other. Note that, the above formula of convolution exactly looks like the formula of a dot product, hence it measures the similarity between the patch of the image and the kernel that we have.</p>
<p>Now suppose, we have a kernel that looks like as follows:</p>
<p>$$K = \begin{bmatrix}
-1 &amp; 1 &amp; -1\\ -1 &amp; 1 &amp; -1\\ -1 &amp; 1 &amp; -1
\end{bmatrix}$$</p>
<p>Then, if we convolute the image with this kernel, then it attains maximum value when we have a horizontal line, and it will attain minimum value when we have a vertical line. Hence, the convolution will tell us the presence of horizontal and vertical edges in the images, hence will provide combined information or featurs about the images to next level. Similar lower level features can again be convoluted to give rise to higher level features.</p>
<p>It should also be ntoeworthy that each such result of convolution will tells about existence of one particular feature in the image. Hence, to effectively use it, we shall need to learn many such features, which in the literature of <strong>Image Processing</strong> is described as filters.</p>
<p>Coming back to the Design of Generator of Texture Network, we need several blocks. There are mainly two types of blocks.</p>
<ol>
<li>
<p><strong>Convolutional Block:</strong> It takes the input tensor (may be image or may be features processed in lower level of the network), and performs some Circular Convolution to process the tensor further and obtain some higher level features.</p>
</li>
<li>
<p><strong>Join Block:</strong> It takes the lower resolution tensor and a high resolution processed noise tensor as input, then it upsamples the lower resolution tensor to match the shape of the high resolution noise and finally merges them together. This noise actually allows the image to have very delicate and intricate variation in the image, as well as create the effect of increment of resolution.</p>
</li>
</ol>
<h2 id="designing-convolutional-block">Designing Convolutional Block</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">conv_block</span>(input_size, in_filters, out_filters):
    <span style="color:#e6db74">&#34;&#34;&#34;Implements the convolutional block with 3x3, 3x3, 1x1 filters, with proper batch normalization and activation&#34;&#34;&#34;</span>
    inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((input_size, input_size, in_filters, ))   <span style="color:#75715e"># in_filters many channels of input image</span>
    <span style="color:#75715e"># first 3x3 conv</span>
    conv1_pad <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Lambda(<span style="color:#66d9ef">lambda</span> x: CircularPadding(x))(inputs)
    conv1_out <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(out_filters, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>), strides <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, 
                                       padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;valid&#39;</span>, name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;conv1&#39;</span>)(conv1_pad)
    hidden_1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>BatchNormalization()(conv1_out)
    conv1_out_final <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>LeakyReLU(name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;rel1&#39;</span>)(hidden_1)
    <span style="color:#75715e"># second 3x3 conv</span>
    conv2_pad <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Lambda(<span style="color:#66d9ef">lambda</span> x: CircularPadding(x))(conv1_out_final)
    conv2_out <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(out_filters, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>), strides <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, 
                                       padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;valid&#39;</span>, name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;conv2&#39;</span>)(conv2_pad)
    hidden_2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>BatchNormalization()(conv2_out)
    conv2_out_final <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>LeakyReLU(name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;rel2&#39;</span>)(hidden_2)
    <span style="color:#75715e"># final 1x1 conv</span>
    conv3_out <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(out_filters, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), strides <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, 
                                       padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;same&#39;</span>, name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;conv3&#39;</span>)(conv2_out_final)
    hidden_3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>BatchNormalization()(conv3_out)
    conv3_out_final <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>LeakyReLU(name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;rel3&#39;</span>)(hidden_3)
    <span style="color:#75715e"># final model</span>
    conv_block <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Model(inputs, conv3_out_final)
    <span style="color:#66d9ef">return</span> conv_block
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">8</span>)
model<span style="color:#f92672">.</span>summary()
</code></pre></div><pre><code>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 16, 16, 3)]       0         
_________________________________________________________________
lambda (Lambda)              (None, 18, 18, 3)         0         
_________________________________________________________________
conv1 (Conv2D)               (None, 16, 16, 8)         224       
_________________________________________________________________
batch_normalization (BatchNo (None, 16, 16, 8)         32        
_________________________________________________________________
rel1 (LeakyReLU)             (None, 16, 16, 8)         0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 18, 18, 8)         0         
_________________________________________________________________
conv2 (Conv2D)               (None, 16, 16, 8)         584       
_________________________________________________________________
batch_normalization_1 (Batch (None, 16, 16, 8)         32        
_________________________________________________________________
rel2 (LeakyReLU)             (None, 16, 16, 8)         0         
_________________________________________________________________
conv3 (Conv2D)               (None, 16, 16, 8)         72        
_________________________________________________________________
batch_normalization_2 (Batch (None, 16, 16, 8)         32        
_________________________________________________________________
rel3 (LeakyReLU)             (None, 16, 16, 8)         0         
=================================================================
Total params: 976
Trainable params: 928
Non-trainable params: 48
_________________________________________________________________
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>plot_model(model, show_shapes<span style="color:#f92672">=</span>True)
</code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='index_29_0.png'>
</img>
<p>In the function <code>conv_block</code>, we take the height and width of the input tensor and the number of channels of the input tensor, and the number of filters to finally output after processing. Note that, after each of the Circular Convolution, we perform a Batch Normalization and a Leaky ReLU layer.</p>
<p>Batch Normalization layer basically normalizes the outputs with respect to the batch axis (i.e. with respect to the ? or None axis indicated in the diagram, which means you are pass arbitrary number of images through the network) so that the mean values remain close to 0, and standard deviation remains close to 1. The following image from the <a href="https://arxiv.org/abs/1803.08494">2018 paper on Group Normalization</a> by Yuxin Wu, Kaiming He described the idea of normalization through the following interesting image.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='norm.png'>
</img>
<p>Finally, Leaky ReLU is a layer than performs an nonlinear activation to the input vector. Leaky ReLU is basically the following function;</p>
<p>$$f(x) = \begin{cases}
x &amp; \text{ if } x \geq 0 \\ \alpha x &amp; \text{ if } x &lt; 0
\end{cases}$$</p>
<p>where $\alpha &lt; 1$ is a non-trainable constant. It is usually fixed at a very low value like $0.05$ or $0.01$. Compared to that, ReLU function is similar to Leaky ReLU, but it outputs 0 if $x &lt; 0$. Hence, Leaky ReLU is essentially a leaky version of ReLU, as it leaks out some small value for negative arguments. Since, such nonlinearity does not require additional parameters, we see 0 in the above summary output.</p>
<h2 id="designing-join-block">Designing Join Block</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">join_block</span>(input_size, n_filter_low, n_filter_high):
    input1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((input_size, input_size, n_filter_low, ))  <span style="color:#75715e"># input to low resolution image</span>
    input2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>input_size, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>input_size, n_filter_high, ))  <span style="color:#75715e"># input to high resolution image</span>
    upsampled_input <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>UpSampling2D(size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>))(input1)
    hidden_1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>BatchNormalization()(upsampled_input)
    hidden_2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>BatchNormalization()(input2)
    outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Concatenate(axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)([hidden_1, hidden_2])
    <span style="color:#75715e"># final model</span>
    join_block <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Model([input1, input2], outputs)
    <span style="color:#66d9ef">return</span> join_block
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> join_block(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">8</span>)
model<span style="color:#f92672">.</span>summary()
</code></pre></div><pre><code>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 128, 128, 32 0                                            
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 256, 256, 32) 0           input_3[0][0]                    
__________________________________________________________________________________________________
input_4 (InputLayer)            [(None, 256, 256, 8) 0                                            
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 256, 256, 32) 128         up_sampling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 256, 256, 8)  32          input_4[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 256, 256, 40) 0           batch_normalization_3[0][0]      
                                                                 batch_normalization_4[0][0]      
==================================================================================================
Total params: 160
Trainable params: 80
Non-trainable params: 80
__________________________________________________________________________________________________
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>plot_model(model, show_shapes<span style="color:#f92672">=</span>True)
</code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='index_33_0.png'>
</img>
<p>The <code>join block</code> is extremely simple, it upsamples the low resolution processed features of the image. Then it normalizes both the higher resolution processed noise, and upsampled version of low resolution features, so that the effect of both branches remain comparable in the network. Finally, it combines the normalized versions.</p>
<h2 id="completing-the-generator">Completing the Generator</h2>
<p>According to the <a href="https://arxiv.org/abs/1603.03417">paper</a> describing Texture Networks, the generator should have a structure similar to the following figure.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='gen.png'>
</img>
<p>However, it was also mentioned that for style transfer, increasing the number of noise from 5 to 6, actually provides much better quality. So, we start the network from a noise of size 8x8x3, and keep increasing it till 256x256x3, which is of the same size of our original content image.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generator_network</span>():
    <span style="color:#75715e"># create input nodes for noise tensors</span>
    noise1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">3</span>, ), name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;noise_1&#39;</span>)
    noise2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">3</span>, ), name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;noise_2&#39;</span>)
    noise3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, ), name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;noise_3&#39;</span>)
    noise4 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, ), name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;noise_4&#39;</span>)
    noise5 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>, ), name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;noise_5&#39;</span>)
    noise6 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>, ), name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;noise_6&#39;</span>)
    content <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input((<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">3</span>, ), name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;content_input&#39;</span>)
    <span style="color:#75715e"># downsample the content image</span>
    content_image_8 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Lambda(<span style="color:#66d9ef">lambda</span> x: tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(x, tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span>])))(content)
    content_image_16 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Lambda(<span style="color:#66d9ef">lambda</span> x: tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(x, tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>])))(content)
    content_image_32 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Lambda(<span style="color:#66d9ef">lambda</span> x: tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(x, tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>])))(content)
    content_image_64 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Lambda(<span style="color:#66d9ef">lambda</span> x: tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(x, tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>])))(content)
    content_image_128 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Lambda(<span style="color:#66d9ef">lambda</span> x: tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(x, tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>])))(content)
    <span style="color:#75715e"># create concatenation of downsampled content image and input nodes</span>
    noise6_con <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Concatenate(axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)([noise6, content_image_8])
    noise5_con <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Concatenate(axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)([noise5, content_image_16])
    noise4_con <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Concatenate(axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)([noise4, content_image_32])
    noise3_con <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Concatenate(axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)([noise3, content_image_64])
    noise2_con <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Concatenate(axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)([noise2, content_image_128])
    noise1_con <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Concatenate(axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)([noise1, content])
    noise6_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>)(noise6_con)   <span style="color:#75715e"># that produces 8x8x8 tensor</span>
    noise5_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>)(noise5_con)   <span style="color:#75715e"># that produces 16x16x8 tensor</span>
    join5 <span style="color:#f92672">=</span> join_block(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span>)([noise6_conv, noise5_conv])   <span style="color:#75715e"># that produces 16x16x16 tensor</span>
    join5_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>)(join5)   <span style="color:#75715e"># produces 16x16x16 tensor</span>
    noise4_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>)(noise4_con)   <span style="color:#75715e"># that produces 32x32x8 tensor</span>
    join4 <span style="color:#f92672">=</span> join_block(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">8</span>)([join5_conv, noise4_conv])   <span style="color:#75715e"># produces 32x32x24 tensor</span>
    join4_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">24</span>, <span style="color:#ae81ff">24</span>)(join4)   <span style="color:#75715e"># produces 32x32x24 tensor</span>
    noise3_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>)(noise3_con)  <span style="color:#75715e"># that produces 64x64x8 tensor</span>
    join3 <span style="color:#f92672">=</span> join_block(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">24</span>, <span style="color:#ae81ff">8</span>)([join4_conv, noise3_conv])   <span style="color:#75715e"># produces 64x64x32 tensor</span>
    join3_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>)(join3)   <span style="color:#75715e"># produces 64x64x32 tensor</span>
    noise2_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>)(noise2_con)  <span style="color:#75715e"># that produces 128x128x8 tensor</span>
    join2 <span style="color:#f92672">=</span> join_block(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">8</span>)([join3_conv, noise2_conv])   <span style="color:#75715e"># produces 128x128x40 tensor</span>
    join2_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">40</span>, <span style="color:#ae81ff">40</span>)(join2)   <span style="color:#75715e"># produces 128x128x40 tensor</span>
    noise1_conv <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>)(noise1_con)  <span style="color:#75715e"># that produces 256x256x8 tensor</span>
    join1 <span style="color:#f92672">=</span> join_block(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">40</span>, <span style="color:#ae81ff">8</span>)([join2_conv, noise1_conv])   <span style="color:#75715e"># produces 256x256x48 tensor</span>
    output <span style="color:#f92672">=</span> conv_block(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">48</span>, <span style="color:#ae81ff">3</span>)(join1)   <span style="color:#75715e"># produces 256x256x3 tensor</span>
    model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Model([content, noise1, noise2, noise3, noise4, noise5, noise6], output, name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;generator&#39;</span>)
    <span style="color:#66d9ef">return</span> model
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">generator <span style="color:#f92672">=</span> generator_network()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">generator<span style="color:#f92672">.</span>summary()
</code></pre></div><pre><code>Model: &quot;generator&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
content_input (InputLayer)      [(None, 256, 256, 3) 0                                            
__________________________________________________________________________________________________
noise_6 (InputLayer)            [(None, 8, 8, 3)]    0                                            
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 8, 8, 3)      0           content_input[0][0]              
__________________________________________________________________________________________________
noise_5 (InputLayer)            [(None, 16, 16, 3)]  0                                            
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, 16, 16, 3)    0           content_input[0][0]              
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 6)      0           noise_6[0][0]                    
                                                                 lambda_2[0][0]                   
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 16, 16, 6)    0           noise_5[0][0]                    
                                                                 lambda_3[0][0]                   
__________________________________________________________________________________________________
model_2 (Model)                 (None, 8, 8, 8)      1192        concatenate_1[0][0]              
__________________________________________________________________________________________________
model_3 (Model)                 (None, 16, 16, 8)    1192        concatenate_2[0][0]              
__________________________________________________________________________________________________
noise_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
lambda_4 (Lambda)               (None, 32, 32, 3)    0           content_input[0][0]              
__________________________________________________________________________________________________
model_4 (Model)                 (None, 16, 16, 16)   64          model_2[1][0]                    
                                                                 model_3[1][0]                    
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 32, 32, 6)    0           noise_4[0][0]                    
                                                                 lambda_4[0][0]                   
__________________________________________________________________________________________________
model_5 (Model)                 (None, 16, 16, 16)   5104        model_4[1][0]                    
__________________________________________________________________________________________________
model_6 (Model)                 (None, 32, 32, 8)    1192        concatenate_3[0][0]              
__________________________________________________________________________________________________
noise_3 (InputLayer)            [(None, 64, 64, 3)]  0                                            
__________________________________________________________________________________________________
lambda_5 (Lambda)               (None, 64, 64, 3)    0           content_input[0][0]              
__________________________________________________________________________________________________
model_7 (Model)                 (None, 32, 32, 24)   96          model_5[1][0]                    
                                                                 model_6[1][0]                    
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 64, 64, 6)    0           noise_3[0][0]                    
                                                                 lambda_5[0][0]                   
__________________________________________________________________________________________________
model_8 (Model)                 (None, 32, 32, 24)   11304       model_7[1][0]                    
__________________________________________________________________________________________________
model_9 (Model)                 (None, 64, 64, 8)    1192        concatenate_4[0][0]              
__________________________________________________________________________________________________
noise_2 (InputLayer)            [(None, 128, 128, 3) 0                                            
__________________________________________________________________________________________________
lambda_6 (Lambda)               (None, 128, 128, 3)  0           content_input[0][0]              
__________________________________________________________________________________________________
model_10 (Model)                (None, 64, 64, 32)   128         model_8[1][0]                    
                                                                 model_9[1][0]                    
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 128, 128, 6)  0           noise_2[0][0]                    
                                                                 lambda_6[0][0]                   
__________________________________________________________________________________________________
model_11 (Model)                (None, 64, 64, 32)   19936       model_10[1][0]                   
__________________________________________________________________________________________________
model_12 (Model)                (None, 128, 128, 8)  1192        concatenate_5[0][0]              
__________________________________________________________________________________________________
noise_1 (InputLayer)            [(None, 256, 256, 3) 0                                            
__________________________________________________________________________________________________
model_13 (Model)                (None, 128, 128, 40) 160         model_11[1][0]                   
                                                                 model_12[1][0]                   
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 256, 256, 6)  0           noise_1[0][0]                    
                                                                 content_input[0][0]              
__________________________________________________________________________________________________
model_14 (Model)                (None, 128, 128, 40) 31000       model_13[1][0]                   
__________________________________________________________________________________________________
model_15 (Model)                (None, 256, 256, 8)  1192        concatenate_6[0][0]              
__________________________________________________________________________________________________
model_16 (Model)                (None, 256, 256, 48) 192         model_14[1][0]                   
                                                                 model_15[1][0]                   
__________________________________________________________________________________________________
model_17 (Model)                (None, 256, 256, 3)  1431        model_16[1][0]                   
==================================================================================================
Total params: 76,567
Trainable params: 75,269
Non-trainable params: 1,298
__________________________________________________________________________________________________
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>plot_model(generator, show_shapes <span style="color:#f92672">=</span> True)
</code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='index_38_0.png'>
</img>
<p>Finally, we have a generator model with about 75,000 parameters.</p>
<p>It should be clear that the low level features, i.e. the outputs of the lower level (closer to input layer that final layer in VGG19) layers are basically features such a strokes, edges which defines the artistic features of the image, whereas, high level features, i.e. the outputs of the higher level (closer to final layer than input layer in VGG19) are features that contains the summarization of the image, namely the abstract object that image contains, specifically the content of the image.</p>
<p>However, Gatys et al. in their <a href="https://arxiv.org/abs/1508.06576">paper</a> <strong>A Neural Algorithm for Artistic Style</strong> depicts that the style is basically represented by the correlation of the low level features of an image, rather than the particular output of those low level features. An intuitive explanation to this particular observation can be given as follows: Consider our style image to be an image of a floor covered with square tiles, like a chessboard. Now, think of applying this tiling style to a complicated image of a landscape. To do this, one may need to consider rotating the tiles to be in a shape like diamonds, which may be able to capture better details of some corners presented in the content image. In this case, if we have a vertical and horizontal line detecting kernels as well as criss-cross line detecting kernels in VGG19, then such a pattern would not give similar output for both style image and final stylized image, but would give similar amount of correlation between those features.</p>
<p>Based on this idea, Gatys et al. introduced <strong>Gram Matrix</strong> to measure the style of an image. Let us particularly concentration on one such low level layer first, for which we have a output tensor of shape (batch size, height of the image, width of the image, number of filters output). Then the corresponding gram matrix of this layer is defined as a matrix of shape (batch size, height of the image, width of the image), whose elements are given by;</p>
<p>$$G_{bij} = \dfrac{\sum_c \sum_d X_{bijc} X_{bijd}}{HW}$$</p>
<p>where $X$ is the output tensor of that layer, $H, W$ denoting the height and width of the images. This formula can be implemented by <code>tf.linalg.einsum</code> function available in <code>tensorflow</code>, which performs the sum operation (of the numerator here) based on a given equation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gram_matrix</span>(input_tensor):
    result <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#39;bijc,bijd-&gt;bcd&#39;</span>, input_tensor, input_tensor)   <span style="color:#75715e"># compute the sum in numerator</span>
    input_shape <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>shape(input_tensor)  <span style="color:#75715e"># get the shape</span>
    num_locations <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(input_shape[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">*</span>input_shape[<span style="color:#ae81ff">2</span>], tf<span style="color:#f92672">.</span>float32)  
    <span style="color:#66d9ef">return</span> result<span style="color:#f92672">/</span>(num_locations)
</code></pre></div><p>Now, I create a function called <code>vgg_layers</code> which returns our descriptor network, given the name of layers of VGG19 network which will be used to compute content and style of the image outputted by Generator Network.</p>
<p>Furthermore, a class called <code>TextureNetwork</code> is created, which is inherited from <code>tf.keras.models.Model</code>. <code>tf.keras.models.Model</code> is a basic class to build new type of neural network model in <code>keras</code>. Inheritence allows us to automatically defines several properties of the keras model, in order to perform optimization and necessary computation in the training stage. Any object of <code>TextureNetwork</code> class is a Texture Network, initialized by specified names of content layers and style layers. Also, the <code>call</code> method of the class has been overridden from call method of <code>tf.keras.models.Model</code> class, which allows to define the exact workflow of feed forward system of our Texture Network. It finally outputs the generated image, along with the content output and style outputs (i.e. the gram matrices for style layers), which we can use to compute the loss function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">vgg_layers</span>(layer_names):
    <span style="color:#e6db74">&#34;&#34;&#34; Creates a vgg model that returns a list of intermediate output values.&#34;&#34;&#34;</span>
    <span style="color:#75715e"># Load our model. Load pretrained VGG, trained on imagenet data</span>
    vgg <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>applications<span style="color:#f92672">.</span>VGG19(include_top<span style="color:#f92672">=</span>False, weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;imagenet&#39;</span>)  <span style="color:#75715e"># load the vgg model</span>
    vgg<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False    <span style="color:#75715e"># do not train over vgg model parameters</span>
    outputs <span style="color:#f92672">=</span> [vgg<span style="color:#f92672">.</span>get_layer(name)<span style="color:#f92672">.</span>output <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> layer_names]    <span style="color:#75715e"># the output of the layers that we want</span>
    model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model([vgg<span style="color:#f92672">.</span>input], outputs)   <span style="color:#75715e"># create a keras model</span>
    <span style="color:#66d9ef">return</span> model
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TextureNetwork</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Model):
    <span style="color:#66d9ef">def</span> __init__(self, style_layers, content_layers):
        super(TextureNetwork, self)<span style="color:#f92672">.</span>__init__()   <span style="color:#75715e"># initialize the superClass</span>
        self<span style="color:#f92672">.</span>vgg <span style="color:#f92672">=</span>  vgg_layers(style_layers <span style="color:#f92672">+</span> content_layers)    <span style="color:#75715e"># obtain a VGG19 model with outputs being the style and content layers</span>
        self<span style="color:#f92672">.</span>style_layers <span style="color:#f92672">=</span> style_layers
        self<span style="color:#f92672">.</span>content_layers <span style="color:#f92672">=</span> content_layers
        self<span style="color:#f92672">.</span>num_style_layers <span style="color:#f92672">=</span> len(style_layers)
        self<span style="color:#f92672">.</span>vgg<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False  <span style="color:#75715e"># we are not going to train vgg network</span>
        self<span style="color:#f92672">.</span>gen <span style="color:#f92672">=</span> generator_network()   <span style="color:#75715e"># create a generator network as part of it</span>
        self<span style="color:#f92672">.</span>gen<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> True   <span style="color:#75715e"># we are going to train this generator</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call</span>(self, content, batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>):
        <span style="color:#75715e"># generates noise required for the network</span>
        noise1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform((batch_size, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">3</span>))
        noise2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform((batch_size, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">3</span>))
        noise3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform((batch_size, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>))
        noise4 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform((batch_size, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>))
        noise5 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform((batch_size, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>))
        noise6 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform((batch_size, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>))
        gen_image <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gen([content, noise1, noise2, noise3, noise4, noise5, noise6])   <span style="color:#75715e"># pass through the generator to obtain generated image</span>
        preprocessed_input <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>applications<span style="color:#f92672">.</span>vgg19<span style="color:#f92672">.</span>preprocess_input(gen_image)  <span style="color:#75715e"># preprocess the image</span>
        outputs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>vgg(preprocessed_input)  <span style="color:#75715e"># get the output from only the required layers</span>
        style_outputs, content_outputs <span style="color:#f92672">=</span> (outputs[:self<span style="color:#f92672">.</span>num_style_layers], 
                                      outputs[self<span style="color:#f92672">.</span>num_style_layers:])
        style_outputs <span style="color:#f92672">=</span> [gram_matrix(style_output)
                         <span style="color:#66d9ef">for</span> style_output <span style="color:#f92672">in</span> style_outputs]  <span style="color:#75715e"># create style type output to compare</span>
        style_dict <span style="color:#f92672">=</span> {style_name:value
                      <span style="color:#66d9ef">for</span> style_name, value
                      <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>style_layers, style_outputs)}
        content_dict <span style="color:#f92672">=</span> {content_name:value 
                    <span style="color:#66d9ef">for</span> content_name, value 
                    <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>content_layers, content_outputs)}
        <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#39;gen&#39;</span>:gen_image, <span style="color:#e6db74">&#39;content&#39;</span>:content_dict, <span style="color:#e6db74">&#39;style&#39;</span>:style_dict}
</code></pre></div><p>Now that we have defined our Texture Network class, we need to specify exactly which layers we are going to use for style features. According to the <a href="https://arxiv.org/abs/1603.03417">paper</a> on Texture Network, the style layers should be <em>rel1_1, rel2_1, rel3_1, rel4_1</em>, while the content layer should be <em>rel4_2</em>. To compare it with the naming convention of VGG19, replace <em>rel</em> with <em>block</em> and add the convolution layer specified. However, I decided to add layer <em>block5_conv2</em>, since according to Gatys et al. this layer has good features related to the content, and for the style image, we can get the tessellation effect accurately by using this particular layer. However, you are welcome to try different layers, which may result in interesting stylized images.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">style_layers <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;block1_conv1&#39;</span>,
                <span style="color:#e6db74">&#39;block2_conv1&#39;</span>,
                <span style="color:#e6db74">&#39;block3_conv1&#39;</span>,
                <span style="color:#e6db74">&#39;block4_conv1&#39;</span>,
                <span style="color:#e6db74">&#39;block5_conv2&#39;</span>]
content_layers <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;block4_conv2&#39;</span>]
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span>)  <span style="color:#75715e"># use an Adam optimizer</span>
tex_net <span style="color:#f92672">=</span> TextureNetwork(style_layers, content_layers)   <span style="color:#75715e"># create the texture network</span>
</code></pre></div><p>Note that, since the initial weights are set to 0, or close to 0, the illiterate network produces a black image. So, let us teach the network to learn its trainable parameters and weights, so that it can produces meaningful outputs.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">output <span style="color:#f92672">=</span> tex_net(content_image, <span style="color:#ae81ff">1</span>)
tensor_to_image(output[<span style="color:#e6db74">&#39;gen&#39;</span>])
</code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='index_47_0.png'>
</img>
<h1 id="training-the-network">Training the Network</h1>
<p>Before training, we require the target variables, based on which our network compares its performance and computes the loss function. In the <a href="https://arxiv.org/abs/1603.03417">paper</a> on Texture Network, the author trains this network on MS COCO and ImageNet dataset. However, we shall use only the content and style image that we have repeatedly, since training on those large datasets would take particularly large amount of time and much computational power, which I lack because of scarcity of funds. So, I shall feed in the same content image and style image to the network repeatedly, and hope that the random noise inputs to the network is going to prevent overfitting of the network.</p>
<p>So, I define a function called <code>extract targets</code> which will extract the style and content from the style and content images, and then those values can be used to compute the loss function. To explicity write the loss function, we shall use the formula;</p>
<p>$$\text{Loss} = w_c \times L_c + w_s \times \sum_{k} L_{s_k}$$</p>
<p>where $L_c$ is the content loss, computed as the mean squared error between the content tensor of content image and the content tensor of generated image. $L_{s_k}$ is similar mean squared error between the gram matix of style image and gram matrix of generated image at $k$-th style layer, and $w_c$ and $w_s$ denotes the weights corresponding to content and style loss.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_targets</span>(inputs):
    inputs <span style="color:#f92672">=</span> inputs<span style="color:#f92672">*</span><span style="color:#ae81ff">255.0</span>
    preprocessed_input <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>applications<span style="color:#f92672">.</span>vgg19<span style="color:#f92672">.</span>preprocess_input(inputs)  <span style="color:#75715e"># preprocess the input image</span>
    outputs <span style="color:#f92672">=</span> vgg_layers(style_layers <span style="color:#f92672">+</span> content_layers)(preprocessed_input)  <span style="color:#75715e"># get the output from only the required layers</span>
    style_outputs, content_outputs <span style="color:#f92672">=</span> (outputs[:len(style_layers)], 
                                       outputs[len(style_layers):])
    style_outputs <span style="color:#f92672">=</span> [gram_matrix(style_output)
                         <span style="color:#66d9ef">for</span> style_output <span style="color:#f92672">in</span> style_outputs]  <span style="color:#75715e"># create style type output to compare</span>
    style_dict <span style="color:#f92672">=</span> {style_name:value
                      <span style="color:#66d9ef">for</span> style_name, value
                      <span style="color:#f92672">in</span> zip(style_layers, style_outputs)}
    content_dict <span style="color:#f92672">=</span> {content_name:value 
                    <span style="color:#66d9ef">for</span> content_name, value 
                    <span style="color:#f92672">in</span> zip(content_layers, content_outputs)}
    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#39;content&#39;</span>:content_dict, <span style="color:#e6db74">&#39;style&#39;</span>:style_dict}
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">style_targets <span style="color:#f92672">=</span> extract_targets(style_image)[<span style="color:#e6db74">&#39;style&#39;</span>]
content_targets <span style="color:#f92672">=</span> extract_targets(content_image)[<span style="color:#e6db74">&#39;content&#39;</span>]
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">style_weight <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-5</span>
content_weight <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</code></pre></div><p>This particular choice to style and content weights seems to work for me pretty well. However, you are encouraged to try out different style and content weight combination to dig up more interesting findings.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">custom_loss</span>(outputs, batch_size):
    gen_outputs <span style="color:#f92672">=</span> outputs[<span style="color:#e6db74">&#39;gen&#39;</span>]
    style_outputs <span style="color:#f92672">=</span> outputs[<span style="color:#e6db74">&#39;style&#39;</span>]   <span style="color:#75715e"># for generated image, get the style</span>
    content_outputs <span style="color:#f92672">=</span> outputs[<span style="color:#e6db74">&#39;content&#39;</span>]  <span style="color:#75715e"># get content</span>
    batch_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(batch_size):
        style_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>add_n([tf<span style="color:#f92672">.</span>reduce_mean((style_outputs[name][i]<span style="color:#f92672">-</span>style_targets[name])<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>) 
                           <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> style_outputs<span style="color:#f92672">.</span>keys()])
        style_loss <span style="color:#f92672">*=</span> style_weight <span style="color:#f92672">/</span> len(style_layers)
        content_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>add_n([tf<span style="color:#f92672">.</span>reduce_mean((content_outputs[name][i]<span style="color:#f92672">-</span>content_targets[name])<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>) 
                                 <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> content_outputs<span style="color:#f92672">.</span>keys()])
        content_loss <span style="color:#f92672">*=</span> content_weight <span style="color:#f92672">/</span> len(content_layers)
        loss <span style="color:#f92672">=</span> style_loss <span style="color:#f92672">+</span> content_loss
        batch_loss <span style="color:#f92672">+=</span> loss
    batch_loss <span style="color:#f92672">/=</span> batch_size
    <span style="color:#66d9ef">return</span> batch_loss
</code></pre></div><p>Finally, we define our <code>train_step</code> method, which performs one step of training of the Texture Network. The <code>tf.function</code> decorator used for the function actually converts this function and underlying object to tensorflow graph, based on which we can perform the feed forward pass as well as compute the gradients based on back propagation. Defining this decorator is essential for the training. More details on this is available at <a href="https://www.tensorflow.org/tutorials/customization/performance">Tensorflow website</a>.</p>
<p>In the <code>train_step</code> function, we use <code>tf.GradientTape</code> to record the feed forward passes through the network. It works like computing a recording of the feed forward pass through the model, and finally playing it backwards, in order to perform back propagation. This gradient tape records all the gradients happening through the graph, and finally enables us to apply the training rule to update the current value of the parameters using these gradients by <code>apply_gradients</code> function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a6e22e">@tf.function</span>()
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_step</span>(content_image, batch_size):
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
        outputs <span style="color:#f92672">=</span> tex_net(content_image, batch_size)
        loss <span style="color:#f92672">=</span> custom_loss(outputs, batch_size)
    gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, tex_net<span style="color:#f92672">.</span>trainable_variables)  <span style="color:#75715e"># obtain the gradients recorded by the tape</span>
    optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, tex_net<span style="color:#f92672">.</span>trainable_variables))   <span style="color:#75715e"># apply the training rule using the gradients to modify the current value of prameters</span>
    <span style="color:#66d9ef">return</span> output, loss
</code></pre></div><p>Finally, I used 2500 iterations of update to train the network. I also show the images generated by the network after each 250 iterations, to see how the network improves over training.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>
my_content <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>concat([content_image <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(batch_size)], axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
<p>n_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
n_iter <span style="color:#f92672">=</span> <span style="color:#ae81ff">250</span>
iter_to_show_output <span style="color:#f92672">=</span> <span style="color:#ae81ff">25</span></p>
<p>loss_array <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(n_epoch):
msg <span style="color:#f92672">=</span> <span style="color:#e6db74">'Epoch: '</span> <span style="color:#f92672">+</span> str(epoch)
<span style="color:#66d9ef">print</span>(msg)
os<span style="color:#f92672">.</span>system(<span style="color:#e6db74">'echo '</span> <span style="color:#f92672">+</span> msg)
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(n_iter):
outputs, loss <span style="color:#f92672">=</span> train_step(my_content, batch_size)
<span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> iter_to_show_output <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
os<span style="color:#f92672">.</span>system(<span style="color:#e6db74">'echo loss: '</span> <span style="color:#f92672">+</span> str(float(loss)))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">'Loss: '</span>, loss)
loss_array<span style="color:#f92672">.</span>append(loss)
display<span style="color:#f92672">.</span>display(tensor_to_image(tex_net(content_image, <span style="color:#ae81ff">1</span>)[<span style="color:#e6db74">'gen'</span>]))
</code></pre></div><pre><code>Epoch: 0
Loss:  tf.Tensor(27138224.0, shape=(), dtype=float32)
Loss:  tf.Tensor(26169034.0, shape=(), dtype=float32)
Loss:  tf.Tensor(26064738.0, shape=(), dtype=float32)
Loss:  tf.Tensor(26034120.0, shape=(), dtype=float32)
Loss:  tf.Tensor(26004590.0, shape=(), dtype=float32)
Loss:  tf.Tensor(24625778.0, shape=(), dtype=float32)
Loss:  tf.Tensor(22932934.0, shape=(), dtype=float32)
Loss:  tf.Tensor(22210894.0, shape=(), dtype=float32)
Loss:  tf.Tensor(21038836.0, shape=(), dtype=float32)
Loss:  tf.Tensor(19649052.0, shape=(), dtype=float32)
</code></pre>
<img 
class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
src='index_57_1.png'>
</img></p>
<pre><code>Epoch: 1
Loss:  tf.Tensor(16565710.0, shape=(), dtype=float32)
Loss:  tf.Tensor(13611194.0, shape=(), dtype=float32)
Loss:  tf.Tensor(12152441.0, shape=(), dtype=float32)
Loss:  tf.Tensor(11187487.0, shape=(), dtype=float32)
Loss:  tf.Tensor(10576777.0, shape=(), dtype=float32)
Loss:  tf.Tensor(9360093.0, shape=(), dtype=float32)
Loss:  tf.Tensor(8675145.0, shape=(), dtype=float32)
Loss:  tf.Tensor(8301968.0, shape=(), dtype=float32)
Loss:  tf.Tensor(7836261.0, shape=(), dtype=float32)
Loss:  tf.Tensor(7389797.5, shape=(), dtype=float32)

...

Epoch: 9
Loss:  tf.Tensor(2416303.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2413389.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2430066.5, shape=(), dtype=float32)
Loss:  tf.Tensor(3215110.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2591146.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2484897.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2440708.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2416797.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2400430.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2389489.8, shape=(), dtype=float32)
</code></pre>
<p><img 
class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
src='index_57_19.png'>
</img></p>
<p>Note how the generated image becomes better and better with more iteration, by mimicking the style of tessellation on our image of AI generated celebrity. But the image does not change much after epoch 5, except the colour gets more violet-ish rather than blue-ish. However, you can manipulate the style weights and content weights properly, in order to have a good balance between the content and style. Also, if we take a look at the loss function, we see that the loss was decreasing rapidly at the beginning, and finally it has more or less stabilized at a point where it cannot be lowered further by much. Hence, by looking at the loss function, it seems the generator is trained enough to meet its capacities.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>plot(loss_array)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Loss Function over time&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='index_59_0.png'>
</img>
<p>From the generation the final stylized image, it seems as if the image of the celebrity is properly tessellated, as we desired. However, it is mostly blue, and there are some funny artifacts and the colour of the skin appearing at the forehead area. This can possibly be resolved by carefully trying out different style and content layer repesentations, as well as adding a variational loss to the custom loss function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tensor_to_image(tex_net(content_image, <span style="color:#ae81ff">1</span>)[<span style="color:#e6db74">&#39;gen&#39;</span>])
</code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='index_61_0.png'>
</img>
<h2 id="references">References</h2>
<ol>
<li>Texture Networks: Feed-forward Synthesis of Textures and Stylized Images - Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky. <a href="https://arxiv.org/abs/1603.03417">https://arxiv.org/abs/1603.03417</a>.</li>
<li><a href="https://www.tensorflow.org/tutorials/generative/style_transfer">https://www.tensorflow.org/tutorials/generative/style_transfer</a></li>
<li>A Neural Algorithm of Artistic Style - Leon A. Gatys, Alexander S. Ecker, Matthias Bethge. <a href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a>.</li>
<li><a href="https://github.com/DmitryUlyanov/texture_nets">https://github.com/DmitryUlyanov/texture_nets</a></li>
<li>Perceptual Losses for Real-Time Style Transfer and Super-Resolution - Justin Johnson, Alexandre Alahi, Li Fei-Fei. <a href="https://arxiv.org/abs/1603.08155">https://arxiv.org/abs/1603.08155</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/Circular_convolution">https://en.wikipedia.org/wiki/Circular_convolution</a>.</li>
<li><a href="https://github.com/DmitryUlyanov/texture_nets">https://github.com/DmitryUlyanov/texture_nets</a></li>
<li>Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky. Texture Networks: Feed-forward Synthesis of Textures and Stylized Images. ArXiv <a href="https://arxiv.org/abs/1603.03417">https://arxiv.org/abs/1603.03417</a>.</li>
</ol>

                </div>
            </div>
        </div>
    </div>
    <div class="flex flex-row justify-center items-center mb-3">
        <h2 class="text-lg text-blue-600">
            With heartfelt appreciation, thank you for being a valued reader, until next time!
        </h2>
    </div>
</div>



<div class="my-6 max-w-6xl md:mx-auto mx-4 text-center shadow-md rounded-md p-4">
    <p class="italic text-lg font-semibold my-4">Explore more posts like this</p>
    <div class="flex flex-row justify-center gap-4">
        
            <a href="/posts/covid19-esir-model/" 
                class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                    transition-all ease-in-out rounded-md shadow-sm">
                Previous Post
            </a>
        
        
            <a href="/posts/k-arm-bandit/" class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900
             focus:bg-neutral-900 transition-all ease-in-out rounded-md shadow-sm">
                Next Post
            </a>
        
    </div>
    <div class="flex flex-row justify-center gap-4 my-4">
        <a href="/posts/" 
            class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                transition-all ease-in-out rounded-md shadow-sm">
            See all posts
        </a>
    </div>
</div>


<div class="my-6 max-w-6xl md:mx-auto mx-4">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statwizard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>



        </div>

        
<section
  id="contact-me"
  class="py-8 mt-16 w-full bg-gradient-to-r from-neutral-900 to-black"
>
  <div class="max-w-6xl mx-auto my-8 text-white text-center">
    <p class="font-normal text-2xl py-4">Reach out to me via</p>
    
    <div class="hidden">
      <div class="hover:bg-red-500 focus:bg-red-500"></div>
      <div class="hover:bg-blue-500 focus:bg-blue-500"></div>
      <div class="hover:bg-gray-500 focus:bg-gray-500"></div>
      <div class="hover:bg-indigo-800 focus:bg-indigo-800"></div>
      <div class="hover:bg-pink-800 focus:bg-pink-800"></div>
    </div>
    
    <div class="flex flex-row flex-wrap justify-center items-center mx-auto gap-4">
      
      <a
        href="mailto:subhrajyotyroy@gmail.com"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-red-500 focus:bg-red-500 hover:text-white focus:text-white"
      >
        <i class="fas fa-envelope fa-2x"></i>
      </a>
      
      <a
        href="https://www.facebook.com/subroy13/"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-blue-500 focus:bg-blue-500 hover:text-white focus:text-white"
      >
        <i class="fab fa-facebook fa-2x"></i>
      </a>
      
      <a
        href="https://github.com/subroy13"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-gray-500 focus:bg-gray-500 hover:text-white focus:text-white"
      >
        <i class="fab fa-github fa-2x"></i>
      </a>
      
      <a
        href="https://www.linkedin.com/in/subroy13"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-indigo-800 focus:bg-indigo-800 hover:text-white focus:text-white"
      >
        <i class="fab fa-linkedin-in fa-2x"></i>
      </a>
      
      <a
        href="#"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-pink-800 focus:bg-pink-800 hover:text-white focus:text-white"
      >
        <i class="fab fa-instagram fa-2x"></i>
      </a>
      
    </div>
  </div>
</section>


<footer class="bg-neutral-900 text-center text-white">
    
    <div class="text-center px-0 py-4 w-full" style="background-color: rgba(0, 0, 0, 0.2)">
        © 2023 Copyright:
        <a class="text-white" href="/">StatWizard.in</a>
    </div>
</footer>

    </div>

    <script>
    $(document).ready(() => {

        
        $('#mobile-menu-button').click(() => {
            $('#mobile-menu').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('block');
            $('#mobile-menu-open-icon').toggleClass('hidden');
            $('#mobile-menu-open-icon').toggleClass('block');
        });

    });
</script>

<script async src="https://kit.fontawesome.com/ca14d5004b.js" crossorigin="anonymous"></script>

    




    
    <script defer>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>



</body>
</html>