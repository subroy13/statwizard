<!DOCTYPE html>
<html lang="en">
<head>
    
    
    
    <link rel="preconnect" href="https://www.googletagmanager.com">
    <link rel="preconnect" href="https://www.google-analytics.com">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-97N9TLJ517"></script>
    <script async>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-97N9TLJ517');
    </script>
    

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="StatWizard: Access beginner-friendly blog posts covering various applications of statistics, data science, computer programming and finance. Also hosts the personal website of the author, Subhrajyoty Roy.">
    <link rel="icon" href="/svg/avatar.svg">
    <title>
        
Reinforcement Learning (Part 5) - Q Learning and Optimal Policy Finding

    </title>

    

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>


<style>
   
  @font-face {
    font-family: 'Roboto';
    font-style: italic;
    font-weight: 100;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOiCnqEu92Fr1Mu51QrEzAdLw.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: italic;
    font-weight: 500;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOjCnqEu92Fr1Mu51S7ACc6CsQ.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: italic;
    font-weight: 900;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOjCnqEu92Fr1Mu51TLBCc6CsQ.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: normal;
    font-weight: 100;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOkCnqEu92Fr1MmgVxIIzI.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: normal;
    font-weight: 500;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmEU9fBBc4.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }

  @font-face {
    font-family: 'Roboto';
    font-style: normal;
    font-weight: 900;
    font-display: swap;
    src: url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmYUtfBBc4.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
  }
</style>



<script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">




<link rel="stylesheet" type="text/css" href="/css/index.8d5958b9698e0d2c4cacf8691b4c5842.css" />






<script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>


<style>
  html {
      scroll-behavior: smooth;
  }
</style>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/>

     
     
</head>
<body>
    <div class="app-container min-h-[100vh] flex flex-col justify-between">
        <nav class="bg-white shadow-xl z-50">
  
  <div
    class="hidden mx-auto max-w-7xl py-2 px-4 md:px-6 lg:px-8 md:flex flex-row items-center"
  >
    <div class="w-[200px] flex items-center">
      <a
        class="uppercase font-extrabold font-mono text-2xl flex flex-row justify-center items-center gap-2"
        href="/"
      >
        <img
          src="/images/logo-wide-resized.png"
          height="50px"
          width="100px"
          class="inline-block"
          alt="StatWizard Logo"
        />
      </a>
    </div>
    <div class="w-full flex flex-row justify-center items-center gap-4">
      
      <a
        href="/"
        class="px-4 py-2 border rounded-lg hover:bg-gray-100"
      >
        Home
      </a>
      
      <a
        href="/posts"
        class="px-4 py-2 border rounded-lg hover:bg-gray-100"
      >
        Posts
      </a>
      
      <a
        href="/aboutme"
        class="px-4 py-2 border rounded-lg hover:bg-gray-100"
      >
        About Me
      </a>
      
      <a
        href="/#contact-me"
        class="px-4 py-2 border rounded-lg hover:bg-gray-100"
      >
        Contact Me
      </a>
      
    </div>
    <div class="w-[200px] flex flex-row justify-around items-center">
      
      <a
        href="mailto:subhrajyotyroy@gmail.com"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fas fa-envelope for link mailto:subhrajyotyroy@gmail.com"
        target="_blank"
      >
        <i class="fas fa-envelope fa-lg"></i>
      </a>
      
      <a
        href="https://www.facebook.com/subroy13/"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fab fa-facebook for link https://www.facebook.com/subroy13/"
        target="_blank"
      >
        <i class="fab fa-facebook fa-lg"></i>
      </a>
      
      <a
        href="https://github.com/subroy13"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fab fa-github for link https://github.com/subroy13"
        target="_blank"
      >
        <i class="fab fa-github fa-lg"></i>
      </a>
      
      <a
        href="https://www.linkedin.com/in/subroy13"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fab fa-linkedin-in for link https://www.linkedin.com/in/subroy13"
        target="_blank"
      >
        <i class="fab fa-linkedin-in fa-lg"></i>
      </a>
      
      <a
        href="#"
        class="hover:text-gray-600 transition-all ease-in-out"
        aria-label="fab fa-instagram for link #"
        target="_blank"
      >
        <i class="fab fa-instagram fa-lg"></i>
      </a>
      
    </div>
  </div>

  
  <div
    class="flex py-4 px-2 max-w-7xl flex-row justify-end items-center md:hidden"
  >
    <div class="w-full flex justify-center items-center">
      <p class="uppercase font-extrabold font-mono text-2xl">
        StatWizard
      </p>
    </div>
    <div class="w-[30px] mr-4 relative">
      <button
        type="button"
        id="mobile-menu-button"
        class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:bg-gray-700 hover:text-white focus:outline-none"
        aria-controls="mobile-menu"
        aria-expanded="false"
      >
        <span class="sr-only">Open main menu</span>
        
        <svg
          id="mobile-menu-close-icon"
          class="bg-white text-neutral-800 outline-none block h-6 w-6"
          fill="none"
          viewBox="0 0 24 24"
          stroke-width="1.5"
          stroke="currentColor"
          aria-hidden="true"
        >
          <path
            stroke-linecap="round"
            stroke-linejoin="round"
            d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"
          />
        </svg>
        
        <svg
          id="mobile-menu-open-icon"
          class="bg-white text-neutral-800 outline-none hidden h-6 w-6"
          fill="none"
          viewBox="0 0 24 24"
          stroke-width="1.5"
          stroke="currentColor"
          aria-hidden="true"
        >
          <path
            stroke-linecap="round"
            stroke-linejoin="round"
            d="M6 18L18 6M6 6l12 12"
          />
        </svg>
      </button>

      <div
        id="mobile-menu"
        class="absolute end-0 mt-4 -mr-4 bg-white w-[90vw] hidden"
      >
        <ul class="flex flex-col">
          
          <li
            class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100"
          >
            <a href="/" class=""> Home </a>
          </li>
          
          <li
            class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100"
          >
            <a href="/posts" class=""> Posts </a>
          </li>
          
          <li
            class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100"
          >
            <a href="/aboutme" class=""> About Me </a>
          </li>
          
          <li
            class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100"
          >
            <a href="/#contact-me" class=""> Contact Me </a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</nav>


        <div class="content-container">
            



    <div class="relative h-[50vh] max-h-[300px] w-full bg-cover bg-center bg-no-repeat bg-fixed rounded-b-md" 
        style = "background-image: url('/posts/2023/q-learning/featured.webp')">
        
            <div class = "absolute top-0 left-0 bg-neutral-800/100 w-fit text-white p-1 ml-1 font-xs rounded-md">
                <p>Cover image taken from </p>
            </div>
        
    </div>



<div class="mt-4 flex flex-col gap-4 mx-2 px-2 md:mx-4 md:px-8">
    <h1 class="text-3xl text-center text-neutral-600 font-bold md:px-8 md:pt-6">
        Reinforcement Learning (Part 5) - Q Learning and Optimal Policy Finding
    </h1>
    <div class="m-2 p-4 rounded-lg shadow-lg border-2">
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="flex flex-col justify-center items-center gap-2">   
                <div class="mt-1 flex flex-row text-neutral-800 gap-4">
                    <p class="font-bold">Date:</p>
                    <p class="font-normal">07 September, 2023</p>
                </div> 
                <p class="mt-1 text-neutral-600">
                    <span class="font-bold">11</span> minutes read
                </p>
                <div class="my-1">
                    
                        <a href="/tags/reinforcement-learning/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Reinforcement Learning
                        </a>
                    
                </div>    
            </div>
            <div class="flex flex-col justify-start gap-2">
                <h1 class="text-lg font-bold">Prerequisites</h1>
                <ul class="space-y-2 text-left">
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Python Programming - </span>
                        
                            <span class="text-yellow-600 font-semibold">Intermediate</span>
                        
                    </li>
                    
                </ul>
            </div>    
        </div>
        <p class="mt-4 p-4 text-sm">
            <span class="font-semibold">Summary: </span> This is the 5th part of the Reinforcement Learning Series: Here I discuss about techniques to finding optimal policies, and then demonstrate the popular Q-learning algorithm. I also show how it can be implemented to solve actual problems
        </p>
    </div>
    <div class="grid grid-cols-1 md:grid-cols-4 gap-2">
        <div class="relative col-span-1">
            <div class="sticky top-0 left-0 w-full pl-4 pt-4">
                <h1 class="text-bold text-xl">Table of Contents</h1>
                <div class="prose">
                    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#optimal-policy-finding">Optimal Policy Finding</a></li>
    <li><a href="#q-learning">Q-Learning</a></li>
    <li><a href="#q-learning-in-action">Q-Learning in action</a></li>
    <li><a href="#q-learning-for-moon-landing">Q-Learning for Moon Landing?</a></li>
  </ul>
</nav>
                </div>
            </div>
        </div>
        <div class="col-span-3">
            <div class="text-justify break-words w-full">
                <div class="prose" style="max-width: none;">
                    <h2 id="introduction">Introduction</h2>
<p>In my previous post (check <a href="https://www.statwizard.in/posts/td-algorithms/">here</a> if you haven&rsquo;t already), we learned how we can use different algorithms to estimate the value function of a markov decision process. If you are not familiar with these concepts / terminologies, I recommend to check out my other posts on the same topic. In this post, we shall finally take a look at how we can use the value estimates to know which actions to take, and how we can find out optimal policies in a reinforcement learning system.</p>
<p>So far, we have seen that given a policy $\pi$, it is possible to use different algorithms to get an estimate of the value function of the policy $\pi$, which will tell us how valuable each state is. We learned about the methods as:</p>
<ol>
<li>Dynamic Programming with Bellman equation.</li>
<li>Monte Carlo algorithm.</li>
<li>Temporal Different learning algorithms.</li>
</ol>
<p>We applied all of these methods for our maze game, and estimated the value function under the policy of random movement (i.e., all four directions are picked with equal probability). Also, I have indicated before that at every state, if we just keep taking some action that improves the value of the state, we can reach the end goal for this scenario. However, in general, this procedure can get stuck in a local optimum. Imagine that you have estimates on how much traffic will be in different roads while commuting to your work, so you accordingly choose a particular road. But may be, there is some construction work going on that road, due to which the best possible road that you have in mind is actually sub-optimal.</p>
<h2 id="optimal-policy-finding">Optimal Policy Finding</h2>
<p>However, to find an optimal policy, we can follow this principle and try to estimate the Q-function (quality function) instead of the value function for every state-action pair. That means,</p>
<ol>
<li>
<p>For the dynamic programming, you can use the Bellman equation of expressing $Q(s, a)$ as a sum over its next state-action Q-values.</p>
</li>
<li>
<p>For Monte Carlo algorithms, we keep track of rewards and gains grouped by the state-action pairs instead of grouping by states only.</p>
</li>
<li>
<p>For temporal difference algorithms, we update $Q(s, a)$ as 
$$
Q^\pi_{new}(S_t, A_t) = Q^\pi_{old}(S_t, A_t) + \alpha \left( R_{t+1}(A_t) + \gamma \sum_{a \in A} \pi(a \mid S_{t+1}) Q^\pi_{old}(S_{t+1}, A_{t+1} = a) - Q^\pi_{old}(S_t, A_t) \right)
$$
which is very similar to the TD iteration rule that we have seen before.</p>
</li>
</ol>
<p>So, lets say for a policy $\pi$, we have an estimate of Q-value function $Q^\pi(s, a)$. Then, based on the above principle, we can choose the action $a$ which has the highest Q-value given state $s$.</p>
<p>In particular,</p>
<p>$$
\pi_{new}(a \mid s) = \begin{cases}
1 &amp; \text{ if } a = \arg\max_{a \in A} Q^\pi(s, a)\\
0 &amp; \text{ otherwise}
\end{cases}
$$</p>
<p>Naturally, this new policy $\pi_{new}$ is called a greedy with respect to the Q-function. Since $\pi_{new}$ is a new policy, the Q-function for this new policy will also change. Using the learning techniques, we can obtain its Q-function estimate, and then again from that Q-function, we can take the greedy approach. We can repeat these two steps <strong>Policy Evaluation</strong> (Q-function estimation) and <strong>Policy Improvement</strong> (Improving the policy by taking greedy action with respect to current Q-function estimate), until we reach a halt. At that time, we have found a policy which is doing its best according to its own value function, and hence it cannot be improved further. Thus, it becomes an optimal policy.</p>
<div class="w-full flex justify-center items-center mermaid">
    graph LR
    E1[$\pi_0$] --> |PE| E2["$Q_{\pi_0}(s, a)$"] --> |PI| E3[$\pi_1$] --> |PE| E4["$Q_{\pi_1}(s, a)$"] --> |PI| E5[$\pi_2$] --> |PE| E6["$\dots$"]
</div>
<p>This approach to finding the optimal policy is called <strong>Policy iteration</strong>.</p>
<h2 id="q-learning">Q-Learning</h2>
<p>Q-Learning is a fundamental reinforcement learning algorithm that plays a pivotal role in finding optimal policies for Markov decision processes (MDPs). It&rsquo;s a powerful and widely used technique because it combines the benefits of both model-free and off-policy learning. Now, before delving further, let us understand these two key terms: <em>model-free</em> and <em>off-policy learning</em>.</p>
<ul>
<li>
<p>Model Free: It is basically some kind of learning algorithm which can accumulate experience and use that to estimate the value, as opposed to the Dynamic programming which requires the model of the environment in terms of state-transition probabilties. Examples would be the Monte-Carlo method.</p>
</li>
<li>
<p>Off-Policy Learning: In this kind of learning, you learn the value estimates of a different policy which can in turn tell you how to improve your current policy. That is, the policy evaluation step and the policy improvement steps run on different policies, but they still provides correct optimal policy.</p>
</li>
</ul>
<p>To understand it, notice that in the TD-learning, we do the iteration with a target value of
$$
R_{t+1}(A_t) + \gamma \sum_{a \in A} \pi(a \mid S_{t+1}) Q^\pi_{old}(S_{t+1}, A_{t+1} = a)
$$
which tells that all the actions coming from policy $\pi$ has an effect on the current action-value estimates. However, ideally you would want to only that the action that maximizes your Q-function. So, we can revise the target like
$$
R_{t+1}(A_t) + \gamma \max_{a \in A} Q^\pi_{old}(S_{t+1}, A_{t+1} = a)
$$</p>
<p>Notice now that we don&rsquo;t need to rely on the current policy $\pi$ anymore, hence this becomes an off-policy learning. This is the basis of Q-learning. Since the entire algorithm relies on only the quality function (or the action value function), the algorithm was named Q-learning. Now we summarize the broad steps for performing Q-learning.</p>
<ol>
<li>Maintain $Q(s, a)$ values for each state $s$ and each action $a$. Start with some random initial values.</li>
<li>For each step of the iteration $t = 0, 1, 2, \dots$ (it could be each action you take in multiple episodes), update $Q$ values using</li>
</ol>
<p>$$
Q_{new}(S_t, A_t) = Q_{old}(S_t, A_t) + \alpha \left( R_{t+1} + \gamma \max_{a \in A}Q_{old}(S_t, a) - Q_{old}(S_t, A_t) \right)
$$</p>
<ol start="3">
<li>Finally, output the optimal policy as $\pi^\ast$ as the policy which takes action $a = \arg\max_{a \in A}Q(S_t, a)$ at state $S_t$.</li>
</ol>
<h2 id="q-learning-in-action">Q-Learning in action</h2>
<p>Now, we shall try to implement this Q-learning algorithm in an actual reinforcement learning game. For that, we shall be using the <code>gymnasium</code> package, which is the evolved version of the old <code>gym</code> package developed by openai. <a href="https://gymnasium.farama.org/content/basic_usage/">Here</a> is the package documentation where you can find lots of different game environments which you can use to see if a reinforcement learning works properly or not.</p>
<p>We start by importing the package and creating the game environment we are going to use.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> gymnasium <span style="color:#66d9ef">as</span> gym
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#34;Acrobot-v1&#34;</span>, render_mode <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;rgb_array&#34;</span>)
</span></span></code></pre></div><p>Each environment describes a game. The setup of the game of <code>Acrobot-v1</code> is as follows: There are two chain links hanging from a point, tied together, and each of the link can freely move within the reasonable bound of physical system. The objective is to apply force to the joint of these two links to ensure that the link touches a line above a certain height from the chain.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='./acrobot/episode-1.gif'>
</img>
<p>The observation space or the state for this game consists of some properties like linear and angular velocity of the chains and stuffs, (very complicated physics for me!), so we are just going to assume that it is simply a vector. There are only 3 possible actions at every time, push the joint to the left or to the right or do nothing.</p>
<p>While we take some random actions, you can see that the chain links just moves erratically and it does not achieve the objective. So, let us apply Q-learning and see whether we can make the agent learn to solve this game.</p>
<p>Before we do that, since the state is continuous, it is difficult to apply Q-learning right away, since we need to maintain a (state, action) pair table to store the Q-values. So, we discretize the entire state space into $10^6$ boxes, namely $10$ grid boxes for each coordinate of the $6$ dimensional state vector. The following python function is doing that.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span>N_STEPS <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>  <span style="color:#75715e"># number of steps / grids on each coordinate</span>
</span></span><span style="display:flex;"><span>obs_space_high <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>high)
</span></span><span style="display:flex;"><span>obs_space_low <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>low)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># function to convert state vector to discrete state</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">convert_observation_to_state</span>(observation):
</span></span><span style="display:flex;"><span>    obs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(observation)
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>floor((obs <span style="color:#f92672">-</span> obs_space_low) <span style="color:#f92672">/</span> (obs_space_high <span style="color:#f92672">-</span> obs_space_low) <span style="color:#f92672">*</span> N_STEPS)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> state<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;int&#39;</span>)<span style="color:#f92672">.</span>clip(<span style="color:#ae81ff">0</span>, N_STEPS<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Now that we have a function to convert the continuous states to discretized state vector, we shall write a class which represents the Acrobot playing RL agent. It shall have 2 functions, one for taking an action given the state and another for updating its Q-values.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># So we are going to create a class which will represent the learning agent</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AcrobotAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, lr, gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.95</span>):
</span></span><span style="display:flex;"><span>        q_shape <span style="color:#f92672">=</span> (N_STEPS, ) <span style="color:#f92672">*</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> (env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n, )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q_vals <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(q_shape)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lr <span style="color:#f92672">=</span> lr
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma
</span></span></code></pre></div><p>The first function will be the function to get the action with the maximum Q-value. This is the Policy improvement step.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_action</span>(self, observation):
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> convert_observation_to_state(observation)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>q_vals[(<span style="color:#f92672">*</span>state, slice(<span style="color:#66d9ef">None</span>))]<span style="color:#f92672">.</span>argmax()
</span></span></code></pre></div><p>The second function will be the one which updates the Q-values according to the Q-learning equation described above.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update</span>(self, obs, action, reward, terminated, next_obs):
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> convert_observation_to_state(obs)
</span></span><span style="display:flex;"><span>    next_state <span style="color:#f92672">=</span> convert_observation_to_state(next_obs)
</span></span><span style="display:flex;"><span>    target <span style="color:#f92672">=</span> reward <span style="color:#f92672">+</span> (<span style="color:#f92672">not</span> terminated) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>q_vals[(<span style="color:#f92672">*</span>next_state, slice(<span style="color:#66d9ef">None</span>))]<span style="color:#f92672">.</span>max()
</span></span><span style="display:flex;"><span>    temporal_diff <span style="color:#f92672">=</span> (target <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>q_vals[(<span style="color:#f92672">*</span>state, action)])
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>q_vals[(<span style="color:#f92672">*</span>state, action)] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> temporal_diff
</span></span></code></pre></div><p>Now, we will try these Q-learning method on the Acrobot game for $5000$ episodes, with a learning rate of $0.01$.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>n_episodes <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> AcrobotAgent(learning_rate)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># now, perform many episodes</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> range(n_episodes):
</span></span><span style="display:flex;"><span>    obs, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()   <span style="color:#75715e"># start from at rest position</span>
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># play a single episode</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>get_action(obs)
</span></span><span style="display:flex;"><span>        next_obs, reward, terminated, truncated, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># update the agent&#39;s q-value estimates</span>
</span></span><span style="display:flex;"><span>        agent<span style="color:#f92672">.</span>update(obs, action, reward, terminated, next_obs)
</span></span><span style="display:flex;"><span>        done <span style="color:#f92672">=</span> terminated <span style="color:#f92672">or</span> truncated  <span style="color:#75715e"># check if episode reached a terminating state</span>
</span></span><span style="display:flex;"><span>        obs <span style="color:#f92672">=</span> next_obs
</span></span></code></pre></div><p>Once we perform these 2000 episodes, we try to save the agent&rsquo;s behaviour on a new episode and record that as a GIF image.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='./acrobot/episode-2001.gif'>
</img>
<p>It looks like the agent is not learning anything. Wonder why is that? The behaviour still looks very random.</p>
<p>The reason is that only a very specific set of actions will lead to the correct goal. So, most of the time, the agent is not able to get any reward. Hence from the initial time itself, the Q-values are not updating much and staying at their initial values only.</p>
<p>Instead we can do the decaying $\epsilon$-greedy type strategy that we explored in the first post (Check <a href="https://www.statwizard.in/posts/k-arm-bandit/">here</a> if you have not already). Because in the initial episodes, we will be spending much time exploring the consequences of different actions, it is more likely to hit the target a few times and the rewards would propagate through the Q-values appropriately. As the episodes increases, we shall try to decrease the $\epsilon$ to reduce exploration.</p>
<p>So, here&rsquo;s the few modifications we need to do. The new get action function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_action</span>(self, observation):
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>convert_observation_to_state(observation)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random() <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>eps:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>q_vals[(<span style="color:#f92672">*</span>state, slice(<span style="color:#66d9ef">None</span>))]<span style="color:#f92672">.</span>argmax()
</span></span></code></pre></div><p>The new decay episilon function which reduces the episilon after every episode.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">decay_epsilon</span>(self):
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> max(self<span style="color:#f92672">.</span>final_eps, self<span style="color:#f92672">.</span>eps <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>eps_decay)
</span></span></code></pre></div><p>and finally in the learning loop we would have</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>start_epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>epsilon_decay <span style="color:#f92672">=</span> start_epsilon <span style="color:#f92672">/</span> (n_episodes <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># reduce the exploration over time</span>
</span></span><span style="display:flex;"><span>final_epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> AcrobotAgent(learning_rate)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># now, perform many episodes</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> range(n_episodes):
</span></span><span style="display:flex;"><span>    obs, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()   <span style="color:#75715e"># start from at rest position</span>
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># play a single episode</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>get_action(obs)
</span></span><span style="display:flex;"><span>        next_obs, reward, terminated, truncated, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># update the agent&#39;s q-value estimates</span>
</span></span><span style="display:flex;"><span>        agent<span style="color:#f92672">.</span>update(obs, action, reward, terminated, next_obs)
</span></span><span style="display:flex;"><span>        done <span style="color:#f92672">=</span> terminated <span style="color:#f92672">or</span> truncated  <span style="color:#75715e"># check if episode reached a terminating state</span>
</span></span><span style="display:flex;"><span>        obs <span style="color:#f92672">=</span> next_obs
</span></span><span style="display:flex;"><span>    agent<span style="color:#f92672">.</span>decay_epsilon()
</span></span></code></pre></div><p>Now that we are exploring a bit at the beginning, we do $5000$ episodes, and after that the result becomes very promising.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='./acrobot/episode-5000.gif'>
</img>
<p>It is a bit hard to see that the chain link is touching the target line. Here&rsquo;s the final frame.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='./acrobot/acrobot-solution.png'>
</img>
<h2 id="q-learning-for-moon-landing">Q-Learning for Moon Landing?</h2>
<p>Next, we will try the same method for the lunar landing enviroment present in the <code>gymnasium</code> package. It contains several continuous state related information as before, and making 4 actions: fire the left engine, fire the right engine, fire the main engine or do nothing.</p>
<p>Here&rsquo;s how the environment looks in action.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='./lunarlander/episode-1.gif'>
</img>
<p>We try the same thing as we did for the acrobot game. We discretize the state space, and apply Q-learning algorithm to train an RL agent. After $10000$ episodes, this is the result.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='./lunarlander/episode-10000.gif'>
</img>
<p>Looks like it is failing pretty much. In between we have a few decent episodes though!</p>
<p><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='./lunarlander/episode-6001.gif'>
</img>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='./lunarlander/episode-8001.gif'>
</img></p>
<p>It seems that applying Q-Learning to the LunarLander environment presents some challenges compared to the Acrobot environment. Can you think of why? Please share your thoughts in the comments!</p>
<p>However, so far all the algorithms that we have explored uses a table to track down the Q-values for the states action pairs. This is why we need to discretize the continuous observation into a state vector, which makes it lose a few pieces of information. In the next post, we shall learn different algorithms that we can use to solve reinforcement learning with these continuous state space, and event continuous action spaces. Some of these algorithms include Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO) and Deep Q-Network (DQN).</p>

                </div>
            </div>
        </div>
    </div>
    <div class="flex flex-row justify-center items-center mb-3">
        <h2 class="text-lg text-blue-600">
            With heartfelt appreciation, thank you for being a valued reader, until next time!
        </h2>
    </div>
</div>



<div class="my-6 max-w-6xl md:mx-auto mx-4 text-center shadow-md rounded-md p-4">
    <p class="italic text-lg font-semibold my-4">Explore more posts like this</p>
    <div class="flex flex-row justify-center gap-4">
        
            <a href="/posts/2023/td-algorithms/" 
                class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                    transition-all ease-in-out rounded-md shadow-sm">
                Previous Post
            </a>
        
        
            <a href="/posts/2023/linear-learning/" class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900
             focus:bg-neutral-900 transition-all ease-in-out rounded-md shadow-sm">
                Next Post
            </a>
        
    </div>
    <div class="flex flex-row justify-center gap-4 my-4">
        <a href="/posts/" 
            class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                transition-all ease-in-out rounded-md shadow-sm">
            See all posts
        </a>
    </div>
</div>


<div class="my-6 max-w-6xl md:mx-auto mx-4">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statwizard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>



        </div>

        
<section
  id="contact-me"
  class="py-8 mt-16 w-full bg-gradient-to-r from-neutral-900 to-black"
>
  <div class="max-w-6xl mx-auto my-8 text-white text-center">
    <p class="font-normal text-2xl py-4">Reach out to me via</p>
    
    <div class="hidden">
      <div class="hover:bg-red-500 focus:bg-red-500"></div>
      <div class="hover:bg-blue-500 focus:bg-blue-500"></div>
      <div class="hover:bg-gray-500 focus:bg-gray-500"></div>
      <div class="hover:bg-indigo-800 focus:bg-indigo-800"></div>
      <div class="hover:bg-pink-800 focus:bg-pink-800"></div>
    </div>
    
    <div class="flex flex-row flex-wrap justify-center items-center mx-auto gap-4">
      
      <a
        href="mailto:subhrajyotyroy@gmail.com"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-red-500 focus:bg-red-500 hover:text-white focus:text-white"
      >
        <i class="fas fa-envelope fa-2x"></i>
      </a>
      
      <a
        href="https://www.facebook.com/subroy13/"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-blue-500 focus:bg-blue-500 hover:text-white focus:text-white"
      >
        <i class="fab fa-facebook fa-2x"></i>
      </a>
      
      <a
        href="https://github.com/subroy13"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-gray-500 focus:bg-gray-500 hover:text-white focus:text-white"
      >
        <i class="fab fa-github fa-2x"></i>
      </a>
      
      <a
        href="https://www.linkedin.com/in/subroy13"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-indigo-800 focus:bg-indigo-800 hover:text-white focus:text-white"
      >
        <i class="fab fa-linkedin-in fa-2x"></i>
      </a>
      
      <a
        href="#"
        target="_blank"
        class="m-2 p-4 h-[55px] w-[55px] flex justify-center items-center
                    rounded-full border-2 border-white border-solid text-white transition-all 
                    duration-300 ease-in-out
                    hover:bg-pink-800 focus:bg-pink-800 hover:text-white focus:text-white"
      >
        <i class="fab fa-instagram fa-2x"></i>
      </a>
      
    </div>
  </div>
</section>


<footer class="bg-neutral-900 text-center text-white">
    
    <div class="text-center px-0 py-4 w-full" style="background-color: rgba(0, 0, 0, 0.2)">
        © 2023 Copyright:
        <a class="text-white" href="/">StatWizard.in</a>
    </div>
</footer>

    </div>

    <script>
    $(document).ready(() => {

        
        $('#mobile-menu-button').click(() => {
            $('#mobile-menu').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('block');
            $('#mobile-menu-open-icon').toggleClass('hidden');
            $('#mobile-menu-open-icon').toggleClass('block');
        });

    });
</script>
<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="statwizard" data-description="Support me on Buy me a coffee!" data-message="Thank you for visiting! You can now support the StatWizard to build a Hogwarts for Statistics!" data-color="#5F7FFF" data-position="Right" data-x_margin="18" data-y_margin="18"></script>

<script async src="https://kit.fontawesome.com/ca14d5004b.js" crossorigin="anonymous"></script>

    


    <script type="module" defer>
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, securityLevel: "loose" });
    </script>



    
    <script defer>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>



</body>
</html>