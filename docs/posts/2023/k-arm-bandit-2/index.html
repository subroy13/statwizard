<!DOCTYPE html>
<html lang="en">
  <head>
    

    
    <link rel="preconnect" href="https://www.googletagmanager.com" />
    <link rel="preconnect" href="https://www.google-analytics.com" />

    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-97N9TLJ517"
    ></script>
    <script async>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-97N9TLJ517");
    </script>
    

    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="StatWizard: Access beginner-friendly blog posts covering various applications of statistics, data science, computer programming and finance. Also hosts the personal website of the author, Subhrajyoty Roy."
    />
    <link rel="icon" href="/images/logo.png" />
    <title>
Reinforcement Learning (Part 2) - K-arm Bandit Problem
</title>

    

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">


<script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">




<link rel="stylesheet" type="text/css" href="/css/index.d7a9aa5245029e12dff5521a4ac76703.css" />






<script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>


<style>
  html {
      scroll-behavior: smooth;
  }
  
  .roboto-thin {
    font-family: "Roboto", sans-serif;
    font-weight: 100;
    font-style: normal;
  }

  .roboto-light {
    font-family: "Roboto", sans-serif;
    font-weight: 300;
    font-style: normal;
  }

  .roboto-regular {
    font-family: "Roboto", sans-serif;
    font-weight: 400;
    font-style: normal;
  }

  .roboto-medium {
    font-family: "Roboto", sans-serif;
    font-weight: 500;
    font-style: normal;
  }

  .roboto-bold {
    font-family: "Roboto", sans-serif;
    font-weight: 700;
    font-style: normal;
  }

  .roboto-black {
    font-family: "Roboto", sans-serif;
    font-weight: 900;
    font-style: normal;
  }

  .roboto-thin-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 100;
    font-style: italic;
  }

  .roboto-light-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 300;
    font-style: italic;
  }

  .roboto-regular-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 400;
    font-style: italic;
  }

  .roboto-medium-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 500;
    font-style: italic;
  }

  .roboto-bold-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 700;
    font-style: italic;
  }

  .roboto-black-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 900;
    font-style: italic;
  }
</style>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/> 
      
     
  </head>
  <body>
    <div class="app-container min-h-[100vh] flex flex-col justify-between">
      <nav class="bg-white shadow-xl z-50">
  
  <div class="hidden md:flex w-full py-2 px-4 md:px-6 lg:px-8 md:flex flex-row items-center justify-between">
    
    <div class="flex flex-row justify-start items-center gap-4">
      
      <a
        href="/"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Home
      </a>
      
      <a
        href="/posts"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Blog
      </a>
      
      <a
        href="/research"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Research
      </a>
      
      <a
        href="/collection"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Collection
      </a>
      
      <a
        href="/cv"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        CV
      </a>
      
      <a
        href="/contact"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Contact
      </a>
      
    </div>


    
    <div class="w-[200px] flex flex-row justify-around items-center">
      
      <a
        href="mailto:subhrajyotyroy@gmail.com"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fas fa-envelope for link mailto:subhrajyotyroy@gmail.com"
        target="_blank"
      >
        <i class="fas fa-envelope fa-lg"></i>
      </a>
      
      <a
        href="https://github.com/subroy13"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-github for link https://github.com/subroy13"
        target="_blank"
      >
        <i class="fab fa-github fa-lg"></i>
      </a>
      
      <a
        href="https://www.linkedin.com/in/subroy13"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-linkedin-in for link https://www.linkedin.com/in/subroy13"
        target="_blank"
      >
        <i class="fab fa-linkedin-in fa-lg"></i>
      </a>
      
      <a
        href="https://scholar.google.com/citations?user=Gocm0lYAAAAJ&amp;hl=en&amp;authuser=1"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-google-scholar for link https://scholar.google.com/citations?user=Gocm0lYAAAAJ&amp;hl=en&amp;authuser=1"
        target="_blank"
      >
        <i class="fab fa-google-scholar fa-lg"></i>
      </a>
      
      <a
        href="#"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-instagram for link #"
        target="_blank"
      >
        <i class="fab fa-instagram fa-lg"></i>
      </a>
      
      <a
        href="https://www.facebook.com/subroy13/"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-facebook for link https://www.facebook.com/subroy13/"
        target="_blank"
      >
        <i class="fab fa-facebook fa-lg"></i>
      </a>
      
    </div>
  </div>
  

  
  <div class="w-full mx-0 flex flex-col gap-0 md:hidden">
    <div class="w-full mx-0 flex flx-row px-2 py-4 justify-between items-center">
      
      <div class="w-[200px] flex items-center">
        <a
          class="uppercase font-extrabold font-mono text-2xl flex flex-row justify-center items-center gap-2"
          href="/"
        >
          <img
            src="/images/logo-wide-2.png"
            height="75px"
            width="150px"
            class="inline-block"
            alt="StatWizard Logo"
          />
        </a>
      </div>

      
      <div class="w-[30px] mr-4">
        <button 
          type="button"
          id="mobile-menu-button"
          class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:text-white outline-none"
          aria-controls="mobile-menu"
          aria-expanded="false"
        >
          <span class="sr-only">Open main menu</span>
          
          <svg
            id="mobile-menu-close-icon"
            class="bg-white text-neutral-800 outline-none block h-6 w-6"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            aria-hidden="true"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"
            />
          </svg>
          
          <svg
            id="mobile-menu-open-icon"
            class="bg-white text-neutral-800 outline-none hidden h-6 w-6"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            aria-hidden="true"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M6 18L18 6M6 6l12 12"
            />
          </svg>
        </button>
      </div>
    </div>

    
    <div id="mobile-menu" class="bg-white w-full mx-0 hidden">
      <ul class="flex flex-col gap-2">
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/" class=""> Home </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/posts" class=""> Blog </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/research" class=""> Research </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/collection" class=""> Collection </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/cv" class=""> CV </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/contact" class=""> Contact </a>
        </li>
        
      </ul>
    </div>
  </div>
  
</nav>


      <div class="content-container mx-0 px-0">
        



    <div class="relative h-[50vh] max-h-[300px] w-full bg-cover bg-center bg-no-repeat bg-fixed rounded-b-md" 
        style = "background-image: url('/posts/2023/k-arm-bandit-2/featured.webp')">
        
            <div class = "absolute top-0 left-0 bg-neutral-800/100 w-fit text-white p-1 ml-1 font-xs rounded-md">
                <p>Cover image taken from </p>
            </div>
        
    </div>



<div class="mt-4 flex flex-col gap-4 mx-2 px-2 md:mx-4 md:px-8">
    <h1 class="text-3xl text-center text-neutral-600 font-bold md:px-8 md:pt-6">
        Reinforcement Learning (Part 2) - K-arm Bandit Problem
    </h1>
    <div class="m-2 p-4 rounded-lg shadow-lg border-2">
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="flex flex-col justify-center items-center gap-2">   
                <div class="mt-1 flex flex-row text-neutral-800 gap-4">
                    <p class="font-bold">Date:</p>
                    <p class="font-normal">07 August, 2023</p>
                </div> 
                <p class="mt-1 text-neutral-600">
                    <span class="font-bold">12</span> minutes read
                </p>
                <div class="my-1">
                    
                        <a href="/tags/reinforcement-learning/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Reinforcement Learning
                        </a>
                    
                </div>    
            </div>
            <div class="flex flex-col justify-start gap-2">
                <h1 class="text-lg font-bold">Prerequisites</h1>
                <ul class="space-y-2 text-left">
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Python Programming - </span>
                        
                            <span class="text-green-600 font-semibold">Beginner</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Probability and Bayes Rule - </span>
                        
                            <span class="text-yellow-600 font-semibold">Intermediate</span>
                        
                    </li>
                    
                </ul>
            </div>    
        </div>
        <p class="mt-4 p-4 text-sm">
            <span class="font-semibold">Summary: </span> This is the second part of the Reinforcement Learning series: Here I discuss about dynamically balancing between exploration and exploitation to reach optimal return in long run.
        </p>
    </div>
    <div class="grid grid-cols-1 md:grid-cols-4 gap-2">
        <div class="relative col-span-1">
            <div class="sticky top-0 left-0 w-full pl-4 pt-4">
                <h1 class="text-bold text-xl">Table of Contents</h1>
                <div class="prose">
                    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#a-bit-of-change-in-the-problem">A Bit of Change in the Problem</a></li>
      </ul>
    </li>
    <li><a href="#upper-confidence-band-algorithm">Upper Confidence Band Algorithm</a></li>
    <li><a href="#ucb-variant---klucb-algorithm">UCB Variant - KLUCB Algorithm</a></li>
    <li><a href="#thompson-sampling---a-bayesian-touch">Thompson Sampling - A Bayesian Touch</a>
      <ul>
        <li><a href="#the-mathematical-concepts">The Mathematical Concepts</a></li>
        <li><a href="#the-algorithm">The Algorithm</a></li>
      </ul>
    </li>
    <li><a href="#some-questions-to-think-about">Some Questions to think about</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
                </div>
            </div>
        </div>
        <div class="col-span-3">
            <div class="text-justify break-words w-full">
                <div class="prose" style="max-width: none;">
                    <h2 id="introduction">Introduction</h2>
<p>This is a continuation of my previous post where I explore $\epsilon$-greedy methods to solve the $k$-arm bandit problem. If you are not yet familiar with these terms, please check out my previous post <a href="https://www.statwizard.in/posts/k-arm-bandit/">here</a>.</p>
<p>In the last post, we saw how $\epsilon$-greedy method, which does a bit more exploration with exploitation can outperform the greedy approach in the long run. We also saw how changing this exploration parameter $\epsilon$ can be decreasing over time to obtain an even better result. But we ended up with a question: What kind of functions of the number of rounds we need to choose? Should it be $10/t$ or $0.1/t$ or may be even $(1000-t)$ or something else?</p>
<h3 id="a-bit-of-change-in-the-problem">A Bit of Change in the Problem</h3>
<p>In the <a href="https://www.statwizard.in/posts/k-arm-bandit/">previous post</a> we considered $10$ arms, each arm giving a random reward according to a normally distributed random variable. If you want to learn more about normal distribution, check out this <a href="https://www.3blue1brown.com/lessons/gaussian-convolution">awesome video by 3Blue1Brown</a>. One problem with this normal distribution is that, the generated random variable may even come as negative. This is a bit weird for the reward distribution, as negative rewards looks more like a punishment. Another problem is that the reward can potentially be extremely large and unbounded; however, almost all practical materialistic resources are limited, hence bounded.</p>
<p>For this post,</p>
<ul>
<li>We will have $5$ arms instead of $10$ arms.</li>
<li>Each arm $a$ has a coin inside it, which turns up head with probability $p_a$.</li>
<li>When you pull the arm, it tosses the coin. If it is head, it gives a reward of $1$, and if it is tail, there is no reward.</li>
<li>We will still have $1000$ such rounds.</li>
<li>The goal is still the same, it is to maximize the total reward.</li>
</ul>
<p>Here is a bit of python code that now implements this reward function.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.25</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.75</span>, <span style="color:#ae81ff">0.9</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">k_arm_bandit_reward</span>(arm_k):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> arm_k <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">or</span> arm_k <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">4</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">ValueError</span>(<span style="color:#e6db74">&#34;Invalid arm&#34;</span>)
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random() <span style="color:#f92672">&gt;</span> rewards[arm_k] <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span></code></pre></div><h2 id="upper-confidence-band-algorithm">Upper Confidence Band Algorithm</h2>
<p>Upper Confidence Band (UCB) algorithm is one of the solution to this problem, which lets us dynamically adjusting the exploration-exploitation balance to get to the best result. It was independently proposed by Agarwal<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> and Katehakis and Robbins<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, after the seminal work by Lai and Robbins<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> which proves some optimality property about an algorithm that maintains a score which can provide the best return in the long run. (<em>I won&rsquo;t go more mathematical than this, but if you want to know more, let me know in the comments, I&rsquo;ll make a post about it.</em>)</p>
<p>Assume that you are currently at round $t$. So, till this round, you have been maintaining your best estimate abour $p_a$ for each action, let us call that $\widehat{p}_a(t)$. Also, you have choosen arm $a$ for $N_a(t)$ number of times. Then, there are three things at play here.</p>
<ol>
<li>If $\widehat{p}_a(t)$ is high for an arm $a$, then that arm probably give higher reward. So, you would like to exploit it more.</li>
<li>If $N_a(t)$ is very low for an arm $a$, then there is not enough certainty about the estimate $\widehat{p}_a(t)$, since you have very little observations about the reward distribution of that arm. So, you would like to explore it more.</li>
<li>If the index of the current round $t$ is high, then it means you must be nearing the end of your chances. In such case, you should explore less and exploit more. As an extreme example, imagine there are $1000$ rounds and you are at the last round. It clearly does not make sense to explore in this round as you cannot use that new information you have gained by exploring in future.</li>
</ol>
<p>Combining this, we have an algorithm which maintains an &ldquo;optimistic&rdquo; score of the rewards for each arm, and chooses the arm that maximize this optimistic UCB score.</p>
<p>$$
UCB_t(a) = \widehat{p}_a(t) + \sqrt{\dfrac{2\log(t)}{N_a(t)}}
$$</p>
<p>Note that, if $\widehat{p}_a(t)$ is high, we are more likely to choose that arm. However, even when $N_a(t)$ is low, this score can become high leading to exploring that particular arm as it has less number of observations.</p>
<p>Here, we write a python code as before to simulate the performance of UCB algorithm for $2000$ such bandit problems.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ucb_rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">2000</span>, <span style="color:#ae81ff">1000</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2000</span>):
</span></span><span style="display:flex;"><span>   avg_rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>   action_counts <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>      action_k <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(avg_rewards <span style="color:#f92672">+</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> t) <span style="color:#f92672">/</span> (action_counts <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) )<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span>)  <span style="color:#75715e"># take maximum of UCB band</span>
</span></span><span style="display:flex;"><span>      reward <span style="color:#f92672">=</span> k_arm_bandit_reward(action_k)
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># update the average</span>
</span></span><span style="display:flex;"><span>      avg_rewards[action_k] <span style="color:#f92672">=</span> (avg_rewards[action_k] <span style="color:#f92672">*</span> action_counts[action_k] <span style="color:#f92672">+</span> reward) <span style="color:#f92672">/</span> (action_counts[action_k] <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>      action_counts[action_k] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># update ucb_rewards</span>
</span></span><span style="display:flex;"><span>      ucb_rewards[b, t] <span style="color:#f92672">=</span> reward
</span></span><span style="display:flex;"><span>ucb_rewards <span style="color:#f92672">=</span> ucb_rewards<span style="color:#f92672">.</span>mean(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>Again, we plot the average reward obtained as a function of the number of rounds, for both the cases with $10/t$-greedy algorithm as well as UCB algorithm. The UCB algorithm learns more slowly than the $10/t$-greedy algorithm, but on the long turn, it reaches the level of same performance (or may be slightly better).</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='ucb-fig1.png'>
</img>
<h2 id="ucb-variant---klucb-algorithm">UCB Variant - KLUCB Algorithm</h2>
<p>There are many variants of the UCB algorithm which have been proposed over last two decades.  Here, we shall describe one variant of the algorithm called KL-UCB algorithm, proposed by Garivier and Cappe<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>Here, the choice of upper confidence bound (UCB) score is given by</p>
<p>$$
\max{ q: KL(\widehat{p}_a(t), q) \leq \log(t)/N_a(t) }
$$</p>
<p>where $KL(p, q)$ denotes the Kullback Leibler divergence between the probabilities $p$ and $q$, which is
$$
KL(p, q) = p\log(p/q) + (1-p)\log((1-p)/(1-q))
$$</p>
<p>Note that, unlike the UCB algorithm, here the upper confidence bound is provided implicitly and must be obtained through some numerical procedure. Once this UCB score is obtained, again we choose the arm $a$ which have the maximum UCB score as before.</p>
<p>The following code implements this KL-UCB algorithm.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>klucb_rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">2000</span>, <span style="color:#ae81ff">1000</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2000</span>):
</span></span><span style="display:flex;"><span>   avg_rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>   action_counts <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># find q using simple search</span>
</span></span><span style="display:flex;"><span>      qa_vals <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>      rhs_vals <span style="color:#f92672">=</span> (np<span style="color:#f92672">.</span>log(t <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> c <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> t))) <span style="color:#f92672">/</span> (action_counts <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>          qs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0.01</span>, <span style="color:#ae81ff">0.99</span>, <span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>          lhs_vals <span style="color:#f92672">=</span> avg_rewards[a] <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(avg_rewards[a] <span style="color:#f92672">/</span> qs) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> avg_rewards[a]) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log((<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> avg_rewards[a]) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> qs))
</span></span><span style="display:flex;"><span>          tmp <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(lhs_vals <span style="color:#f92672">&lt;=</span> rhs_vals[a])[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">if</span> len(tmp) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>              qa_vals[a] <span style="color:#f92672">=</span> qs[tmp[<span style="color:#ae81ff">0</span>]]
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>              qa_vals[a] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>      action_k <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(qa_vals)  <span style="color:#75715e"># take maximum of UCB band</span>
</span></span><span style="display:flex;"><span>      reward <span style="color:#f92672">=</span> k_arm_bandit_reward(action_k)
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># update the average</span>
</span></span><span style="display:flex;"><span>      avg_rewards[action_k] <span style="color:#f92672">=</span> (avg_rewards[action_k] <span style="color:#f92672">*</span> action_counts[action_k] <span style="color:#f92672">+</span> reward) <span style="color:#f92672">/</span> (action_counts[action_k] <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>      action_counts[action_k] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># update ucb_rewards</span>
</span></span><span style="display:flex;"><span>      klucb_rewards[b, t] <span style="color:#f92672">=</span> reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>klucb_rewards <span style="color:#f92672">=</span> klucb_rewards<span style="color:#f92672">.</span>mean(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>Again we plot its performance over $1000$ rounds.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='ucb-fig2.png'>
</img>
<p>Clearly, it is much better than the simple UCB algorithm, it is quite close to the $10/t$-greedy approach during the initial rounds, which means this algorithm very quickly learns the optimal action.</p>
<h2 id="thompson-sampling---a-bayesian-touch">Thompson Sampling - A Bayesian Touch</h2>
<h3 id="the-mathematical-concepts">The Mathematical Concepts</h3>
<p>If you carefully look at what we have been doing so far, there are 2 main things.</p>
<ol>
<li>Based on whatever knowlegde we have, we take some action like pulling a particular arm.</li>
<li>That arm gives us a reward, we observe that, and then we update our knowledge (or estimates) to incorporate that new information about the reward.</li>
</ol>
<p>This is very similar to the concept of Bayesian paradigm in statistics, where one starts with a prior belief about the unknown values (called parameters), he (she) observe the data, and then he (she) updates his (her) belief about these values in the light of data, which is called posterior belief. This entire field is based on <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes theorem</a> which came from an essay by Thomas Bayes in 1763. Here&rsquo;s a quick <a href="https://www.youtube.com/watch?v=9wCnvr7Xw4E">video</a> by StatQuest which provides a decent introduction to Bayes&rsquo; theorem.</p>
<p>Thompson sampling is an algorithm based on this Bayes theorem, tuned to the particular case of reinforcement learning. Here, we start by assuming that every unknown $p_a$ (the probability of getting a reward when pulling arm $a$) follows a prior distribution with density
$$
\pi(p_a) = \dfrac{\Gamma(\alpha_a + \beta_a)}{\Gamma(\alpha_a)\Gamma(\beta_a)} p_a^{\alpha_a - 1} (1-p_a)^{\beta_a - 1}, \ p_a \in [0, 1]
$$</p>
<p>This is actually a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> with parameters $\alpha_a, \beta_a$. You can read more about it on <a href="https://en.wikipedia.org/wiki/Beta_distribution">Wikipedia</a>, but here&rsquo;s the fundamental thing that we will need.</p>
<ul>
<li>
<p>Higher value of $\alpha_a$ means it is more likely that $p_a$ is higher than $1/2$. Higher values of $\beta_a$ means $p_a$ will usually be lower than $1/2$.</p>
</li>
<li>
<p>The expectation (i.e., the mean) of this distribution is $\alpha_a/(\alpha_a + \beta_a)$.</p>
</li>
<li>
<p>As $\alpha_a$ or $\beta_a$ increases, the distribution becomes more and more concentrated near its mean (or expectation).</p>
</li>
</ul>
<p>Now for every arm $a$ that we pull, we observe a reward $r_a(t)$ at round $t$, then $r_a(t)$ actually has a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a> which has a probability mass function</p>
<p>$$
P(r_a(t)) = p_a^{r_a(t)} (1 - p_a)^{(1-r_a(t))}, \ r_a(t) = 0, 1
$$</p>
<p>To verify this, you can simply put the values $r_a(t) = 0$ or $r_a(t) = 1$ and work out that the probability that $r_a(t) = 1$ turns out to be exactly $p_a$ as expected.</p>
<h3 id="the-algorithm">The Algorithm</h3>
<p>Now we are ready with all the mathematical insights that we need to understand Thompson sampling. Let&rsquo;s say we are at $t$-th round. We pull an arm $a$.</p>
<ol>
<li>
<p>If the arm gave a reward of $1$, then we should increase our estimate $\widehat{p}_a(t)$ a bit. Since $\mathbb{E}(p_a) = \alpha_a / (\alpha_a + \beta_a)$ (i.e., the expectation of the prior distribution is $\alpha_a / (\alpha_a + \beta_a)$), it can be achieved by increasing $\alpha_a$, so we add 1 to the value of $\alpha_a$ for this particular arm.</p>
</li>
<li>
<p>If the arm gave a reward of $0$, then we need to decrease our estimate. Similar as before, we can increase $\beta_a$ by 1.</p>
</li>
<li>
<p>In both cases, either $\alpha_a$ or $\beta_a$ increases, so in a way, uncertainty decreases, as the beta distribution becomes a little bit more concentrated around $\alpha_a / (\alpha_a + \beta_a)$.</p>
</li>
<li>
<p>One can also show that this is the posterior distribution of $p_a$ given the observed reward $r_a(t)$, the reward at time $t$. Applying Bayes theorem, the posterior becomes</p>
</li>
</ol>
<p>$$
\pi(p_a \mid r_a(t)) \propto \pi(p_a) P(r_a(t) \mid p_a) \propto p_a^{r_a(t) + \alpha_a - 1} (1 - p_a)^{1- r_a(t) + \beta_a - 1}
$$</p>
<p>which is again a Beta distribution but with new parameters $r_a(t) + \alpha_a$ and $(1 - r_a(t)) + \beta_a$ as the alpha and beta parameters.</p>
<ol start="5">
<li>Finally, once we have the posterior distribution (i.e., the updated $\alpha_a$ and $\beta_a$), we can use this to simulate a future observation of the $p_a$ values. This basically means simulating one possible $k$-arm bandit game which have been consistent with the reward observations so far. Once we have simulated the $p_a$ values, clearly the best possible arm would to be take the one with the highest $p_a$ value (i.e., the highest probability of giving a reward).</li>
</ol>
<p>This entire algorithm is called <strong>Thompson Sampling</strong>, proposed by William R. Thompson in 1993<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Note that, in contrast to the greedy method where we find the best arm based on whatever knowledge we have so far, in Thompson sampling, we try to predict the multi-arm bandit game itself using the knowledge and use the best arm in the predicted game. You may want to check out this nice tutorial paper by Russo et al. on Thompson sampling<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</p>
<p>Here&rsquo;s a bit of python code that implements this Thompson sampling algorithm.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># perform the thompson sampling algorithm</span>
</span></span><span style="display:flex;"><span>thompson_rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">2000</span>, <span style="color:#ae81ff">1000</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2000</span>):
</span></span><span style="display:flex;"><span>   alphas <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">5</span>)   <span style="color:#75715e"># initially all alpha and beta are 1, hence uniform prior</span>
</span></span><span style="display:flex;"><span>   betas <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># generate the probs</span>
</span></span><span style="display:flex;"><span>      pa_vals <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>beta(a <span style="color:#f92672">=</span> alphas, b <span style="color:#f92672">=</span> betas)
</span></span><span style="display:flex;"><span>      action_k <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(pa_vals)  <span style="color:#75715e"># take maximum of simulated posterior probs</span>
</span></span><span style="display:flex;"><span>      reward <span style="color:#f92672">=</span> k_arm_bandit_reward(action_k)
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># calculate posterior and update parameters</span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">if</span> reward <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>          alphas[action_k] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>          betas[action_k] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># update thompson_rewards</span>
</span></span><span style="display:flex;"><span>      thompson_rewards[b, t] <span style="color:#f92672">=</span> reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>thompson_rewards <span style="color:#f92672">=</span> thompson_rewards<span style="color:#f92672">.</span>mean(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>And here&rsquo;s the plot for the experienced rewards by this strategy.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='ucb-fig3.png'>
</img>
<p>Wow! It looks like the Thompson sampling is a clear winner here. This is because instead of focusing on the short run and the future rewards, Thompson sampling focuses on the long run by guessing the multi-arm bandit game instead of the next possible rewards. Since the reward distributions of the arms don&rsquo;t change from time to time, Thompson sampling can leverage the this by sticking to the best arm of the most probable $k$-arm bandit game given the information collected so far.</p>
<h2 id="some-questions-to-think-about">Some Questions to think about</h2>
<p>Same as the previous post, here&rsquo;s some questions to explore.</p>
<ol>
<li>
<p>Thompson sampling gives good performance since the reward distributions do not change from time to time. What happens if the reward distributions change? For instance, if a slot machine gives you a jackpot reward at round $t$, it is very unlikely that it will also give you a jackpot reward at round $(t+1)$, since most of its rewards have been taken out by the previous jackpot.</p>
</li>
<li>
<p>How do we model this situation mathematically, i.e., when the reward itself is changing the attributes of the slot machine?</p>
</li>
</ol>
<p>I shall answer both of these questions in my next post of this series. Till then, feel free to explore (or exploit) the answers to these questions and let me know in the comments.</p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Agrawal, R. (1995). Sample Mean Based Index Policies with O(log n) Regret for the Multi-Armed Bandit Problem. Advances in Applied Probability, 27(4), 1054–1078. <a href="https://doi.org/10.2307/1427934">https://doi.org/10.2307/1427934</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Katehakis, M. N., &amp; Robbins, H. (1995). Sequential choice from several populations. Proceedings of the National Academy of Sciences of the United States of America, 92(19), 8584–8585. <a href="https://doi.org/10.1073/pnas.92.19.8584">https://doi.org/10.1073/pnas.92.19.8584</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Lai, T.L., &amp; Robbins, H. (1985). Asymptotically efficient adaptive allocation rules.  Advances in Applied Mathematics, 6(1), 4-22. <a href="https://doi.org/10.1016/0196-8858(85)90002-8">https://doi.org/10.1016/0196-8858(85)90002-8</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Garivier, A., &amp; Cappé, O. (2011, December). The KL-UCB algorithm for bounded stochastic bandits and beyond. In Proceedings of the 24th annual conference on learning theory (pp. 359-376). JMLR Workshop and Conference Proceedings.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Thompson, W. R. (1933). On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples. Biometrika, 25(3/4), 285–294. <a href="https://doi.org/10.2307/2332286">https://doi.org/10.2307/2332286</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., &amp; Wen, Z. (2018). A tutorial on thompson sampling. Foundations and Trends® in Machine Learning, 11(1), 1-96. <a href="https://arxiv.org/abs/1707.02038">https://arxiv.org/abs/1707.02038</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

                </div>
            </div>
        </div>
    </div>
    <div class="flex flex-row justify-center items-center mb-3">
        <h2 class="text-lg text-blue-600">
            With heartfelt appreciation, thank you for being a valued reader, until next time!
        </h2>
    </div>
</div>



<div class="my-6 max-w-6xl md:mx-auto mx-4 text-center shadow-md rounded-md p-4">
    <p class="italic text-lg font-semibold my-4">Explore more posts like this</p>
    <div class="flex flex-row justify-center gap-4">
        
            <a href="/posts/2023/random-matrix-theory-intro/" 
                class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                    transition-all ease-in-out rounded-md shadow-sm">
                Previous Post
            </a>
        
        
            <a href="/posts/2023/markov-decision-process/" class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900
             focus:bg-neutral-900 transition-all ease-in-out rounded-md shadow-sm">
                Next Post
            </a>
        
    </div>
    <div class="flex flex-row justify-center gap-4 my-4">
        <a href="/posts/" 
            class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                transition-all ease-in-out rounded-md shadow-sm">
            See all posts
        </a>
    </div>
</div>


<div class="my-6 max-w-6xl md:mx-auto mx-4">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statwizard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>



      </div>
    </div>

    <script>
    $(document).ready(() => {

        
        $('#mobile-menu-button').click(() => {
            $('#mobile-menu').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('block');
            $('#mobile-menu-open-icon').toggleClass('hidden');
            $('#mobile-menu-open-icon').toggleClass('block');
        });

    });
</script>
<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="statwizard" data-description="Support me on Buy me a coffee!" data-message="Thank you for visiting! You can now support the StatWizard to build a Hogwarts for Statistics!" data-color="#5F7FFF" data-position="Right" data-x_margin="18" data-y_margin="18"></script>

<script async src="https://kit.fontawesome.com/ca14d5004b.js" crossorigin="anonymous"></script>
 
    




    
    <script defer>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>



  </body>
</html>
