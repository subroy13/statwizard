<!DOCTYPE html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    

    
    <link rel="preconnect" href="https://www.googletagmanager.com" />
    <link rel="preconnect" href="https://www.google-analytics.com" />

    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-97N9TLJ517"
    ></script>
    <script async>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-97N9TLJ517");
    </script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8206710020783397"
    crossorigin="anonymous"></script>
    <meta name="google-adsense-account" content="ca-pub-8206710020783397">
    

    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="StatWizard: Access beginner-friendly blog posts covering various applications of statistics, data science, computer programming and finance. Also hosts the personal website of the author, Subhrajyoty Roy."
    />
    <link rel="icon" href="/images/logo.png" />
    <title>
Random Matrix Theory - Introduction
</title>

    

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">


<script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">




<link rel="stylesheet" type="text/css" href="//localhost:1313/css/index.be6765e4baa2004dd1fbd2c9a57dbc3f.css" />






<script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>


<style>
  html {
      scroll-behavior: smooth;
  }
  
  .roboto-thin {
    font-family: "Roboto", sans-serif;
    font-weight: 100;
    font-style: normal;
  }

  .roboto-light {
    font-family: "Roboto", sans-serif;
    font-weight: 300;
    font-style: normal;
  }

  .roboto-regular {
    font-family: "Roboto", sans-serif;
    font-weight: 400;
    font-style: normal;
  }

  .roboto-medium {
    font-family: "Roboto", sans-serif;
    font-weight: 500;
    font-style: normal;
  }

  .roboto-bold {
    font-family: "Roboto", sans-serif;
    font-weight: 700;
    font-style: normal;
  }

  .roboto-black {
    font-family: "Roboto", sans-serif;
    font-weight: 900;
    font-style: normal;
  }

  .roboto-thin-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 100;
    font-style: italic;
  }

  .roboto-light-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 300;
    font-style: italic;
  }

  .roboto-regular-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 400;
    font-style: italic;
  }

  .roboto-medium-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 500;
    font-style: italic;
  }

  .roboto-bold-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 700;
    font-style: italic;
  }

  .roboto-black-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 900;
    font-style: italic;
  }
</style>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/> 
      
     
  </head>
  <body>
    <div class="app-container min-h-[100vh] flex flex-col justify-between">
      <nav class="bg-white shadow-xl z-50">
  
  <div class="hidden md:flex w-full py-2 px-4 md:px-6 lg:px-8 md:flex flex-row items-center justify-between">
    
    <div class="flex flex-row justify-start items-center gap-4">
      
      <a
        href="/"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Home
      </a>
      
      <a
        href="/posts"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Blog
      </a>
      
      <a
        href="/research"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Research
      </a>
      
      <a
        href="/collection"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Collection
      </a>
      
      <a
        href="/cv"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        CV
      </a>
      
      <a
        href="/contact"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Contact
      </a>
      
    </div>


    
    <div class="w-[200px] flex flex-row justify-around items-center">
      
      <a
        href="mailto:subhrajyotyroy@gmail.com"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fas fa-envelope for link mailto:subhrajyotyroy@gmail.com"
        target="_blank"
      >
        <i class="fas fa-envelope fa-lg"></i>
      </a>
      
      <a
        href="https://github.com/subroy13"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-github for link https://github.com/subroy13"
        target="_blank"
      >
        <i class="fab fa-github fa-lg"></i>
      </a>
      
      <a
        href="https://www.linkedin.com/in/subroy13"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-linkedin-in for link https://www.linkedin.com/in/subroy13"
        target="_blank"
      >
        <i class="fab fa-linkedin-in fa-lg"></i>
      </a>
      
      <a
        href="https://scholar.google.com/citations?user=Gocm0lYAAAAJ&amp;hl=en&amp;authuser=1"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-google-scholar for link https://scholar.google.com/citations?user=Gocm0lYAAAAJ&amp;hl=en&amp;authuser=1"
        target="_blank"
      >
        <i class="fab fa-google-scholar fa-lg"></i>
      </a>
      
      <a
        href="#"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-instagram for link #"
        target="_blank"
      >
        <i class="fab fa-instagram fa-lg"></i>
      </a>
      
      <a
        href="https://www.facebook.com/subroy13/"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-facebook for link https://www.facebook.com/subroy13/"
        target="_blank"
      >
        <i class="fab fa-facebook fa-lg"></i>
      </a>
      
    </div>
  </div>
  

  
  <div class="w-full mx-0 flex flex-col gap-0 md:hidden">
    <div class="w-full mx-0 flex flx-row px-2 py-4 justify-between items-center">
      
      <div class="w-[200px] flex items-center">
        <a
          class="uppercase font-extrabold font-mono text-2xl flex flex-row justify-center items-center gap-2"
          href="/"
        >
          <img
            src="/images/logo-wide-2.png"
            height="75px"
            width="150px"
            class="inline-block"
            alt="StatWizard Logo"
          />
        </a>
      </div>

      
      <div class="w-[30px] mr-4">
        <button 
          type="button"
          id="mobile-menu-button"
          class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:text-white outline-none"
          aria-controls="mobile-menu"
          aria-expanded="false"
        >
          <span class="sr-only">Open main menu</span>
          
          <svg
            id="mobile-menu-close-icon"
            class="bg-white text-neutral-800 outline-none block h-6 w-6"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            aria-hidden="true"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"
            />
          </svg>
          
          <svg
            id="mobile-menu-open-icon"
            class="bg-white text-neutral-800 outline-none hidden h-6 w-6"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            aria-hidden="true"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M6 18L18 6M6 6l12 12"
            />
          </svg>
        </button>
      </div>
    </div>

    
    <div id="mobile-menu" class="bg-white w-full mx-0 hidden">
      <ul class="flex flex-col gap-2">
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/" class=""> Home </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/posts" class=""> Blog </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/research" class=""> Research </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/collection" class=""> Collection </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/cv" class=""> CV </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/contact" class=""> Contact </a>
        </li>
        
      </ul>
    </div>
  </div>
  
</nav>


      <div class="content-container mx-0 px-0">
        



    <div class="relative h-[50vh] max-h-[300px] w-full bg-cover bg-center bg-no-repeat bg-fixed rounded-b-md" 
        style = "background-image: url('//localhost:1313/posts/2023/random-matrix-theory-intro/featured.webp')">
        
            <div class = "absolute top-0 left-0 bg-neutral-800/100 w-fit text-white p-1 ml-1 font-xs rounded-md">
                <p>Cover image taken from </p>
            </div>
        
    </div>



<div class="mt-4 flex flex-col gap-4 mx-2 px-2 md:mx-4 md:px-8">
    <h1 class="text-3xl text-center text-neutral-600 font-bold md:px-8 md:pt-6">
        Random Matrix Theory - Introduction
    </h1>
    <div class="m-2 p-4 rounded-lg shadow-lg border-2">
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="flex flex-col justify-center items-center gap-2">   
                <div class="mt-1 flex flex-row text-neutral-800 gap-4">
                    <p class="font-bold">Date:</p>
                    <p class="font-normal">06 August, 2023</p>
                </div> 
                <p class="mt-1 text-neutral-600">
                    <span class="font-bold">12</span> minutes read
                </p>
                <div class="my-1">
                    
                        <a href="//localhost:1313/tags/random-matrix-theory/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Random Matrix Theory
                        </a>
                    
                </div>    
            </div>
            <div class="flex flex-col justify-start gap-2">
                <h1 class="text-lg font-bold">Prerequisites</h1>
                <ul class="space-y-2 text-left">
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Python Programming - </span>
                        
                            <span class="text-green-600 font-semibold">Beginner</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Probability (Distribution, Change of Variable, Convergence) - </span>
                        
                            <span class="text-yellow-600 font-semibold">Intermediate</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Linear Algebra - </span>
                        
                            <span class="text-yellow-600 font-semibold">Intermediate</span>
                        
                    </li>
                    
                </ul>
            </div>    
        </div>
        <p class="mt-4 p-4 text-sm">
            <span class="font-semibold">Summary: </span> This is the first introductory post on Random Matrix Theory series: Here I demonstrate some phenemenon about random matrices which are very non-intuitive to the classical thinking of asymptotic result
        </p>
    </div>
    <div class="grid grid-cols-1 md:grid-cols-4 gap-2">
        <div class="relative col-span-1">
            <div class="sticky top-0 left-0 w-full pl-4 pt-4">
                <h1 class="text-bold text-xl">Table of Contents</h1>
                <div class="prose">
                    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#law-of-large-numbers-lln">Law of Large Numbers (LLN)</a></li>
        <li><a href="#central-limit-theorem-clt">Central Limit Theorem (CLT)</a></li>
      </ul>
    </li>
    <li><a href="#exploration">Exploration</a>
      <ul>
        <li><a href="#distribution-of-frobenius-norm">Distribution of Frobenius Norm</a></li>
        <li><a href="#distribution-of-largest-eigenvalue">Distribution of Largest Eigenvalue</a></li>
        <li><a href="#distribution-of-eigenvalues">Distribution of Eigenvalues</a></li>
      </ul>
    </li>
    <li><a href="#the-distribution-of-the-spacings">The Distribution of the Spacings</a>
      <ul>
        <li><a href="#the-traditional-case">The Traditional Case</a></li>
        <li><a href="#the-2x2-gaussian-case">The 2X2 Gaussian Case</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
                </div>
            </div>
        </div>
        <div class="col-span-3">
            <div class="text-justify break-words w-full">
                <div class="prose" style="max-width: none;">
                    <h2 id="introduction">Introduction</h2>
<p>In asymptotic theory of probability, possibly the two most popular results are the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT), which has numerous applications in statistics, in particular, a very prominent one in establishing asymptotic theory of an estimator. Let us briefly recap what these results essentially states.</p>
<h3 id="law-of-large-numbers-lln">Law of Large Numbers (LLN)</h3>
<p>Let $X_1, X_2, \dots X_n$ be a sequence of random variables. Let us also assume that each of these $X_i$s have finite expectation, i.e., $\mathbb{E}(|X_i|) \leq \infty$. We also assume that these $X_i$s are independently distributed and $\mathbb{E}(X_i) = \mu$ for each $i = 1, 2, \dots n$. Then, the sample average of these random variables satisfy
$$
\dfrac{X_1 + \dots + X_n}{n} \rightarrow \mu
$$
as $n \rightarrow \infty$. The above convergence can be made in probability or in almost sure sense, in which cases the resulting theorem is called as Weak Law of Large Numbers (WLLN) and Strong Law of Large Numbers (SLLN) respectively. You can learn more about the different notions of probabilistic convergence here<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>The result can be generalized to the cases where $\mathbb{E}(X_i)$ is different for each $i$, and also to the case when the expectations either go to $+\infty$ or $-\infty$. However, for the time being, we shall refrain from delving into much depth here. Interested readers may want to check out any standard probabilty book<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<h3 id="central-limit-theorem-clt">Central Limit Theorem (CLT)</h3>
<p>We again start with a sequence of independent and identically distributed (i.i.d.) random variables $X_1, \dots X_n$ with finite second moments. Let, these have $0$ mean and variance $1$ (otherwise, we can also work with $Y_i = (X_i - \mathbb{E}(X_i))/\sqrt{var(X_i)}$). Then the Central Limit Theorem asserts that
$$
\sqrt{n}\left( \dfrac{X_1 + \dots + X_n}{n} \right) \rightarrow Z
$$
where $Z$ is a random variable following the standard normal distribution. The above convergence is the convergence in distribution, meaning that the probability distribution function of the left hand side converges to the probability distribution function of standard normal random variable pointwise (for most of the points).</p>
<p>In effect CLT tells us one fundamental thing about the sum of $n$ i.i.d. random variables with $0$ mean and unit variance, that they mostly lie within a boundary of $-3\sqrt{n}$ to $3\sqrt{n}$ (To see this, think about the $3\sigma$-limit of the standard normal random variable), and the sum very much concentrates around the mean $0$.</p>
<h2 id="exploration">Exploration</h2>
<p>So far we have been working with sums of i.i.d. random variables which is kind of a one-dimensional sequence. Now, let us try to explore how does a matrix with i.i.d. random entries behave. What all quantities or properties of this matrix will behave like LLN or CLT, and what properties do not behave like that, - these are the two fundamental questions we want to answer.</p>
<h3 id="distribution-of-frobenius-norm">Distribution of Frobenius Norm</h3>
<p>One of the basic property of a matrix is the norm of the matrix. Let&rsquo;s say we consider a $n \times n$ matrix with each entries randomly distributed as standard normal distribution. We call this $M$. However, as we draw the entries of $M$ randomly, it may not be symmetric. Having a symmetric matrix is usually a desirable property to begin with, so we symmetrize the matrix using the trick
$$
X = (M + M^{\top}) / 2
$$
which makes $X$ an $n \times n$ symmetric matrix.</p>
<p>Note that, the entries of $X$ still follows normal distribution, namely
$$
X_{ij} = (M_{ij} + M_{ji})/2 \sim
\begin{cases}
N(0, 1/2) &amp; \text{ if } i \neq j\\
N(0, 1) &amp; \text{ if } i = j
\end{cases}
$$</p>
<p>Now to see how the norm of the matrix $X$ is distributed, we can simulate many such matrices $M$, symmetrize it to get $X$, and record the norm of the matrix. Here, we take the Frobenius norm (i.e., the Euclidean $L_2$ norm of the matrix entries). Note that
$$
\Vert X\Vert_2 = \left( \sum_{i,j}X_{ij}^2 \right)^{1/2} = \left( \sum_{i &gt; j}(2X_{ij}^2) + \sum_{i=1}^n X_{ii}^2 \right)^{1/2}.
$$
Here, $2X_{ij}^2$ for $i &gt; j$ is independent and identically distributed as $X_{ii}^2$, hence the above sum of basically a sum of i.i.d. entries. We then expect the squared sum to range between $n-3\sqrt{n}$ to $n + 3\sqrt{n}$ using CLT, hence the square root of the sum is expected to range between $\sqrt{n - 3\sqrt{n}}, \sqrt{n + 3\sqrt{n}}$. Let us try to verify this by simulation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np   <span style="color:#75715e"># import necessary python packages</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span>
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>norms <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(B)   <span style="color:#75715e"># an array to hold the norms</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(B):
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(n, n)   <span style="color:#75715e"># each entry of the matrix is standard normal</span>
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> (X <span style="color:#f92672">+</span> X<span style="color:#f92672">.</span>T)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>   <span style="color:#75715e"># symmetrize it</span>
</span></span><span style="display:flex;"><span>  norms[b] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>norm(X)  <span style="color:#75715e"># look at matrix norm</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># finally, plot it</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(norms, bins <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Frobenius Norm of 100x100 Gaussian matrix&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>Running the above python code and plotting the histogram yields a distribution close to a normal distribution plot. Hence, it reaffirms our belief.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='fig1.png'>
</img>
<h3 id="distribution-of-largest-eigenvalue">Distribution of Largest Eigenvalue</h3>
<p>Now let&rsquo;s say instead of looking at the $L_2$-norm of the matrix entries, we consider the $L_2$-norm of vectors in the column space of the $X$ matrix. Specifically, we consider the quantity
$$
\sup_{\Vert v \Vert_2 = 1} \Vert Xv\Vert_2
$$
which can be proven to be equal to the largest eigenvalue of the matrix $X$. Note that, the above norm basically looks that how much change the linear transformation corresponding to $X$ does to a vector $v$, and tries to see the maximum change possible.</p>
<p>This time, instead of trying to theoretically bound this largest eigenvalue and see where it lies mostly, we will do the simulation. The theoretical tools require a bit of work, and I will try to explain them in subsequent posts of this series.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>eig_norms <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(B)   <span style="color:#75715e"># placeholder for storing the largest eigenvalue</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(B):
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(n, n)   <span style="color:#75715e"># each entry of the matrix is standard normal</span>
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> (X <span style="color:#f92672">+</span> X<span style="color:#f92672">.</span>T)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>   <span style="color:#75715e"># symmetrize it</span>
</span></span><span style="display:flex;"><span>  eig_norms[b] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>norm(X, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># look at the largest eigenvalue</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(eig_norms, bins <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Largest Eigenvalue of the 100x100 Gaussian matrix&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='fig2.png'>
</img>
<p>It turns out again we have some kind of Gaussian looking plot, most of the histogram is concentrated around its mean, however, it may be a bit positively skewed. Note that, even if the largest eigenvalue is not a very straightforward linear function as before, we still have this CLT like behaviour and concentration around the mean.</p>
<p>Let&rsquo;s do the same experiment now for the smallest eigenvalue.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>eig_norms <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(B)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(B):
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(n, n)   <span style="color:#75715e"># each entry of the matrix is standard normal</span>
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> (X <span style="color:#f92672">+</span> X<span style="color:#f92672">.</span>T)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>   <span style="color:#75715e"># symmetrize it</span>
</span></span><span style="display:flex;"><span>  eig_norms[b] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>eigvalsh(X)[<span style="color:#ae81ff">1</span>]  <span style="color:#75715e"># look at the smallest eigen value</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(eig_norms, bins <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Smallest Eigenvalue of the 100x100 Gaussian matrix&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='fig3.png'>
</img>
<p>Well, the concentration around the mean was expected, but now it is a bit negatively skewed. It is as if it is trying to balance out the slight positive skewness of the largest eigenvalue.</p>
<h3 id="distribution-of-eigenvalues">Distribution of Eigenvalues</h3>
<p>Well, so far we have only seen how each individual eigenvalues behave. Each individual eigenvalue looks approximately like a Gaussian distributed random variable (for large $n$ say, i.e., large number of rows and columns of the random matrix), but with a bit of skewness here and there. What if we try to combine all of these eigenvalues together now? Basically we would keep track of every eigenvalue of the random matrix $X$, and then plot all these eigenvalues together to form a histogram. This will give us some idea about how a random eigenvalue of a random matrix look like.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>eigs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((B, n))   <span style="color:#75715e"># placeholder for tracking the eigenvalues</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(B):
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(n, n)   <span style="color:#75715e"># each entry of the matrix is standard normal</span>
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> (X <span style="color:#f92672">+</span> X<span style="color:#f92672">.</span>T)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>   <span style="color:#75715e"># symmetrize it</span>
</span></span><span style="display:flex;"><span>  eval, evec <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>eig(X)  <span style="color:#75715e"># look at the eigenvalues</span>
</span></span><span style="display:flex;"><span>  eigs[b, :] <span style="color:#f92672">=</span> eval
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(eigs<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), bins <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Eigenvalues of </span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74">x</span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74"> Gaussian matrix&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='fig4.png'>
</img>
<p>Well, this is no way a Gaussian kind of a distribution anymore. There is, of course, no concentration about the mean, and this semicircular structure of the probability distribution is not very traditional in nature, like the CLT or LLN that we have know so far. Dealing with this kind of distribution thus requires new tools, so we have a separate theory on Random Matrices. There are basically two hindrances on extending the traditional tools.</p>
<ol>
<li>
<p>The eigenvalues are non-linear functions of the entries of the matrix $X$, which may not have a very explicit and nice expression. For instance, in generate the eigenvalues are defined as the solution to the characteristic function $det(X - \lambda I) = 0$. This will be a complicated $n$-degree polynomial of $\lambda$, the zeros of which will be difficult to obtain in terms of the coefficients.</p>
</li>
<li>
<p>Secondly, we usually work with independent random variables or conditionally independent random variables (in the case of Martingales<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>). However, the eigenvalues are the zeros of the same characteristic polynomial, which are very likely to be correlated.</p>
</li>
</ol>
<p>Same thing happens for the singular values. We get a quarter circle instead of half circle here.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sigs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((B, n))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(B):
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(n, n)   <span style="color:#75715e"># each entry of the matrix is standard normal</span>
</span></span><span style="display:flex;"><span>  X <span style="color:#f92672">=</span> (X <span style="color:#f92672">+</span> X<span style="color:#f92672">.</span>T)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>   <span style="color:#75715e"># symmetrize it</span>
</span></span><span style="display:flex;"><span>  sval <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svd(X, compute_uv <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>, full_matrices<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)  <span style="color:#75715e"># look at the singular values</span>
</span></span><span style="display:flex;"><span>  sigs[b, :] <span style="color:#f92672">=</span> sva
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(sigs<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), bins <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Singular values of </span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74">x</span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74"> Gaussian matrix&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='fig5.png'>
</img>
<h2 id="the-distribution-of-the-spacings">The Distribution of the Spacings</h2>
<p>Since we already know there exists some differences between the traditional asymptotic results and results of random matrices through simulation, here we try to do something concrete. Basically, we consider the distribution of spacings. In the traditional case, we can consider something like two i.i.d. standard normal random variables $x_1$ and $x_2$ and try to obtain the probability density function of $s = |x_1 - x_2|$. In the random matrix case, we will start with a $2\times 2$ random matrix with Gaussian entries, then try to see the probability distribution of the distance between the two eigenvalues.</p>
<p>If our intuition is correct, these two lead to wildly different result.</p>
<h3 id="the-traditional-case">The Traditional Case</h3>
<p>We have two i.i.d random variables $x_1, x_2 \sim N(0, 1)$ and $s = |x_1 - x_2|$. To determine the density of $s$, we first look the the cumulative probability distribution function of $s$. Clearly, when $s &lt; t$ (for some fixed constant $t$), then given $x_1$, the value of $x_2$ must be in the range from $(x_1 - t)$ to $(x_1 + t)$. Therefore,</p>
<p>$$
F(s) = \dfrac{1}{2\pi}\int_{-\infty}^{\infty} \int_{x_1-s}^{x_1 + s} e^{-(x_1^2 + x_2^2)/2} dx_2 dx_1
$$</p>
<p>We can now rewrite the inner integral in terms of a Gaussian integral, i.e.,</p>
<p>$$
F(s) = \dfrac{1}{2\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-x_1^2/2} \left( \int_{0}^{(x_1 + s)/\sqrt{2}} e^{-u^2}du - \int_{0}^{(x_1 - s)/\sqrt{2}} e^{-u^2}du \right) dx_1
$$</p>
<p>Now, we can integrate the above with respect to $s$ to obtain a probability density function. Here, we apply Leibnitz rule and the fundamental theorem of calculus to perform the differentiation.</p>
<p>$$
\begin{align*}
f(s) &amp; = \dfrac{\partial F(s)}{\partial s}\\
&amp; = \dfrac{1}{4\pi} \int_{-\infty}^{\infty} e^{-x_1^2/2} \left[ e^{-(x_1 + s)^2/2} + e^{-(x_1 - s)^2/2} \right] dx_1\\
&amp; = \dfrac{1}{4\pi} e^{-s^2/4}\left[ \int_{-\infty}^{\infty} \exp\left( -\dfrac{1}{2} (\sqrt{2}x_1 + s/\sqrt{2})^2 \right)dx_1 + \int_{-\infty}^{\infty} \exp\left( -\dfrac{1}{2} (\sqrt{2}x_1 - s/\sqrt{2})^2 \right)dx_1 \right]\\
&amp; = \dfrac{1}{\sqrt{\pi}} e^{-s^2/4}
\end{align*}
$$</p>
<p>Here&rsquo;s how this distribution looks like.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='fig6A.png'>
</img>
<h3 id="the-2x2-gaussian-case">The 2X2 Gaussian Case</h3>
<p>We have seen that the diagonal entries of symmetrized matrix has double the variance of the off-diagonal entries. Let us consider the $2\times 2$ case of a Gaussian random matrix, i.e.,
$$
X = \begin{bmatrix}
x_1 &amp; x_3\\
x_3 &amp; x_2\
\end{bmatrix}
$$
where $x_1, x_2 \sim N(0, 1)$ and $x_3 \sim N(0,1/2)$. In this case, the characteristic equation is simply
$$
det(X - \lambda I) = (x_1 - \lambda)(x_2 - \lambda) - x_3^2 = \lambda^2 - (x_1 + x_2)\lambda + (x_1x_2 - x_3^2) = 0
$$
This is a quadratic equation in $\lambda$, so we have use the well-known <a href="https://en.wikipedia.org/wiki/Sridhara">Sridharacharya&rsquo;s formula</a> to the rescue to find the eigenvalues of this $2\times 2$ random matrix.</p>
<p>$$
\lambda = ((x_1 + x_2) \pm \sqrt{(x_1 - x_2)^2 + 4x_3^2})/2
$$</p>
<p>Now instead of looking at the probability distribution of the eigenvalues, we will focus on the distribution of the spacings between these two eigenvalues. The spacing $s = \max{\lambda_1, \lambda_2} - \min{ \lambda_1, \lambda_2}$ can then be described in terms of the entries of the matrix $X$ as</p>
<p>$$
s = \sqrt{(x_1 - x_2)^2 + 4x_3^2}
$$</p>
<p>Now, we apply a change of variable technique to obtain the distribution of $s$. Namely, let us consider the transformation $(x_1 - x_2) = r\cos(\theta)$, $2x_3 = r\sin(\theta)$ and $x_1 + x_2 = t$. Then clearly, $s = r$ as $\sin^2(\theta) + \cos^2(\theta) = 1$. Also, $x_1 = (r\cos(\theta) + t)/2$, $x_2 = (t-r\cos(\theta))/2$ and $x_3 = r\sin(\theta)/2$. Therefore,</p>
<p>$$
p(r, \theta, t) = \dfrac{1}{2\pi\sqrt{\pi}} \exp\left[ -\dfrac{1}{2}\left( (\dfrac{r\cos(\theta) + t}{2})^2 + (\dfrac{t - r\cos(\theta)}{2})^2 + \dfrac{r^2\sin^2(\theta)}{2}  \right)  \right] \times (r/4)
$$</p>
<p>where the last part $r/4$ comes from the Jacobian of this transformation. Therefore,</p>
<p>$$
\begin{align*}
p(s) &amp; = p(r) = \dfrac{r}{8\pi\sqrt{\pi}}\int_{0}^{2\pi} \int_{-\infty}^{\infty} \exp\left[ -r^2/4 - t^2/4 \right] dt d\theta \\
&amp; = \dfrac{r}{4\pi} e^{-r^2/4} \int_{0}^{2\pi} d\theta\\
&amp; = \dfrac{s}{2} e^{-s^2/4}, \ s &gt; 0
\end{align*}
$$</p>
<p>Here&rsquo;s how this distribution looks like.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='fig6.png'>
</img>
<p>It clearly looks different than the one we obtained from spacings of two i.i.d. normal random variables. It shows that the eigenvalues are highly correlated. One eigenvalue feels the presence of the other, hence the spacing is not very likely to be near zero. This is known as eigenvalue repulsion. One application of this theory of repulsion is that it can be used to model the distribution of electrons as every electron repulses each other due to having the same charge.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> On the other hand, for the i.i.d. case, the spacings are very likely to be near $0$, since both of them are independently very likely to be close to their distributional expectation.</p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Billingsley, P. (2013). Convergence of probability measures. John Wiley &amp; Sons.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Feller, W. (1991). An introduction to probability theory and its applications, Volume 2 (Vol. 81). John Wiley &amp; Sons.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://en.wikipedia.org/wiki/Martingale_(probability_theory)">Martingale (Probability Theory) - Wikipedia</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Livan, G., Novaes, M., &amp; Vivo, P. (2018). Introduction to random matrices theory and practice. Monograph Award, 63, 54-57. <a href="https://arxiv.org/abs/1712.07903">https://arxiv.org/abs/1712.07903</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

                </div>
            </div>
        </div>
    </div>
    <div class="flex flex-col justify-center items-center my-4 p-4" style="background-color: #14101b;">
        <h2 class="text-lg text-white my-4">
            Thank you very much for being a valued reader! 🙏🏽 Subscribe below to get notified when new posts are out. 📢 Stay tuned until next time!
        </h2>
        <iframe src="https://statwizard.substack.com/embed" width="480" height="150" style="border:1px solid black; background:black;" frameborder="0" scrolling="no"></iframe>
    </div>
</div>



<div class="my-6 max-w-6xl md:mx-auto mx-4 text-center shadow-md rounded-md p-4">
    <p class="italic text-lg font-semibold my-4">Explore more posts like this</p>
    <div class="flex flex-row justify-center gap-4">
        
            <a href="//localhost:1313/posts/2023/k-arm-bandit/" 
                class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                    transition-all ease-in-out rounded-md shadow-sm">
                Previous Post
            </a>
        
        
            <a href="//localhost:1313/posts/2023/k-arm-bandit-2/" class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900
             focus:bg-neutral-900 transition-all ease-in-out rounded-md shadow-sm">
                Next Post
            </a>
        
    </div>
    <div class="flex flex-row justify-center gap-4 my-4">
        <a href="//localhost:1313/posts/" 
            class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                transition-all ease-in-out rounded-md shadow-sm">
            See all posts
        </a>
    </div>
</div>


<div class="my-6 max-w-6xl md:mx-auto mx-4">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statwizard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>



      </div>
    </div>

    <script>
    $(document).ready(() => {

        
        $('#mobile-menu-button').click(() => {
            $('#mobile-menu').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('block');
            $('#mobile-menu-open-icon').toggleClass('hidden');
            $('#mobile-menu-open-icon').toggleClass('block');
        });

    });
</script>


<script async src="https://kit.fontawesome.com/ca14d5004b.js" crossorigin="anonymous"></script>
 
    




    
    <script defer>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>



  </body>
</html>
