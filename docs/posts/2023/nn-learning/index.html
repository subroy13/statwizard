<!DOCTYPE html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    

    
    <link rel="preconnect" href="https://www.googletagmanager.com" />
    <link rel="preconnect" href="https://www.google-analytics.com" />

    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-97N9TLJ517"
    ></script>
    <script async>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-97N9TLJ517");
    </script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8206710020783397"
    crossorigin="anonymous"></script>
    <meta name="google-adsense-account" content="ca-pub-8206710020783397">
    

    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="StatWizard: Access beginner-friendly blog posts covering various applications of statistics, data science, computer programming and finance. Also hosts the personal website of the author, Subhrajyoty Roy."
    />
    <link rel="icon" href="/images/logo.png" />
    <title>
Reinforcement Learning (Part 7) - Solving Lunar Landing
</title>

    

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">


<script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">




<link rel="stylesheet" type="text/css" href="//localhost:1313/css/index.d7a9aa5245029e12dff5521a4ac76703.css" />






<script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>


<style>
  html {
      scroll-behavior: smooth;
  }
  
  .roboto-thin {
    font-family: "Roboto", sans-serif;
    font-weight: 100;
    font-style: normal;
  }

  .roboto-light {
    font-family: "Roboto", sans-serif;
    font-weight: 300;
    font-style: normal;
  }

  .roboto-regular {
    font-family: "Roboto", sans-serif;
    font-weight: 400;
    font-style: normal;
  }

  .roboto-medium {
    font-family: "Roboto", sans-serif;
    font-weight: 500;
    font-style: normal;
  }

  .roboto-bold {
    font-family: "Roboto", sans-serif;
    font-weight: 700;
    font-style: normal;
  }

  .roboto-black {
    font-family: "Roboto", sans-serif;
    font-weight: 900;
    font-style: normal;
  }

  .roboto-thin-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 100;
    font-style: italic;
  }

  .roboto-light-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 300;
    font-style: italic;
  }

  .roboto-regular-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 400;
    font-style: italic;
  }

  .roboto-medium-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 500;
    font-style: italic;
  }

  .roboto-bold-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 700;
    font-style: italic;
  }

  .roboto-black-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 900;
    font-style: italic;
  }
</style>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/> 
      
     
  </head>
  <body>
    <div class="app-container min-h-[100vh] flex flex-col justify-between">
      <nav class="bg-white shadow-xl z-50">
  
  <div class="hidden md:flex w-full py-2 px-4 md:px-6 lg:px-8 md:flex flex-row items-center justify-between">
    
    <div class="flex flex-row justify-start items-center gap-4">
      
      <a
        href="/"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Home
      </a>
      
      <a
        href="/posts"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Blog
      </a>
      
      <a
        href="/research"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Research
      </a>
      
      <a
        href="/collection"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Collection
      </a>
      
      <a
        href="/cv"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        CV
      </a>
      
      <a
        href="/contact"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Contact
      </a>
      
    </div>


    
    <div class="w-[200px] flex flex-row justify-around items-center">
      
      <a
        href="mailto:subhrajyotyroy@gmail.com"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fas fa-envelope for link mailto:subhrajyotyroy@gmail.com"
        target="_blank"
      >
        <i class="fas fa-envelope fa-lg"></i>
      </a>
      
      <a
        href="https://github.com/subroy13"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-github for link https://github.com/subroy13"
        target="_blank"
      >
        <i class="fab fa-github fa-lg"></i>
      </a>
      
      <a
        href="https://www.linkedin.com/in/subroy13"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-linkedin-in for link https://www.linkedin.com/in/subroy13"
        target="_blank"
      >
        <i class="fab fa-linkedin-in fa-lg"></i>
      </a>
      
      <a
        href="https://scholar.google.com/citations?user=Gocm0lYAAAAJ&amp;hl=en&amp;authuser=1"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-google-scholar for link https://scholar.google.com/citations?user=Gocm0lYAAAAJ&amp;hl=en&amp;authuser=1"
        target="_blank"
      >
        <i class="fab fa-google-scholar fa-lg"></i>
      </a>
      
      <a
        href="#"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-instagram for link #"
        target="_blank"
      >
        <i class="fab fa-instagram fa-lg"></i>
      </a>
      
      <a
        href="https://www.facebook.com/subroy13/"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-facebook for link https://www.facebook.com/subroy13/"
        target="_blank"
      >
        <i class="fab fa-facebook fa-lg"></i>
      </a>
      
    </div>
  </div>
  

  
  <div class="w-full mx-0 flex flex-col gap-0 md:hidden">
    <div class="w-full mx-0 flex flx-row px-2 py-4 justify-between items-center">
      
      <div class="w-[200px] flex items-center">
        <a
          class="uppercase font-extrabold font-mono text-2xl flex flex-row justify-center items-center gap-2"
          href="/"
        >
          <img
            src="/images/logo-wide-2.png"
            height="75px"
            width="150px"
            class="inline-block"
            alt="StatWizard Logo"
          />
        </a>
      </div>

      
      <div class="w-[30px] mr-4">
        <button 
          type="button"
          id="mobile-menu-button"
          class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:text-white outline-none"
          aria-controls="mobile-menu"
          aria-expanded="false"
        >
          <span class="sr-only">Open main menu</span>
          
          <svg
            id="mobile-menu-close-icon"
            class="bg-white text-neutral-800 outline-none block h-6 w-6"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            aria-hidden="true"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"
            />
          </svg>
          
          <svg
            id="mobile-menu-open-icon"
            class="bg-white text-neutral-800 outline-none hidden h-6 w-6"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            aria-hidden="true"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M6 18L18 6M6 6l12 12"
            />
          </svg>
        </button>
      </div>
    </div>

    
    <div id="mobile-menu" class="bg-white w-full mx-0 hidden">
      <ul class="flex flex-col gap-2">
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/" class=""> Home </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/posts" class=""> Blog </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/research" class=""> Research </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/collection" class=""> Collection </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/cv" class=""> CV </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/contact" class=""> Contact </a>
        </li>
        
      </ul>
    </div>
  </div>
  
</nav>


      <div class="content-container mx-0 px-0">
        



    <div class="relative h-[50vh] max-h-[300px] w-full bg-cover bg-center bg-no-repeat bg-fixed rounded-b-md" 
        style = "background-image: url('//localhost:1313/posts/2023/nn-learning/featured.webp')">
        
            <div class = "absolute top-0 left-0 bg-neutral-800/100 w-fit text-white p-1 ml-1 font-xs rounded-md">
                <p>Cover image taken from </p>
            </div>
        
    </div>



<div class="mt-4 flex flex-col gap-4 mx-2 px-2 md:mx-4 md:px-8">
    <h1 class="text-3xl text-center text-neutral-600 font-bold md:px-8 md:pt-6">
        Reinforcement Learning (Part 7) - Solving Lunar Landing
    </h1>
    <div class="m-2 p-4 rounded-lg shadow-lg border-2">
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="flex flex-col justify-center items-center gap-2">   
                <div class="mt-1 flex flex-row text-neutral-800 gap-4">
                    <p class="font-bold">Date:</p>
                    <p class="font-normal">20 October, 2023</p>
                </div> 
                <p class="mt-1 text-neutral-600">
                    <span class="font-bold">17</span> minutes read
                </p>
                <div class="my-1">
                    
                        <a href="//localhost:1313/tags/reinforcement-learning/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Reinforcement Learning
                        </a>
                    
                        <a href="//localhost:1313/tags/statistical-learning/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Statistical Learning
                        </a>
                    
                </div>    
            </div>
            <div class="flex flex-col justify-start gap-2">
                <h1 class="text-lg font-bold">Prerequisites</h1>
                <ul class="space-y-2 text-left">
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Python Programming - </span>
                        
                            <span class="text-red-600 font-semibold">Advanced</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Differential Calculus - </span>
                        
                            <span class="text-yellow-600 font-semibold">Intermediate</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Neural Networks - </span>
                        
                            <span class="text-green-600 font-semibold">Beginner</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Tensorflow and Keras - </span>
                        
                            <span class="text-green-600 font-semibold">Beginner</span>
                        
                    </li>
                    
                </ul>
            </div>    
        </div>
        <p class="mt-4 p-4 text-sm">
            <span class="font-semibold">Summary: </span> This is the 7th and final part of the Reinforcement Learning Series: Here I give a brief introduction to deep Q-learning and some techniques to increase RL agent&rsquo;s efficiency
        </p>
    </div>
    <div class="grid grid-cols-1 md:grid-cols-4 gap-2">
        <div class="relative col-span-1">
            <div class="sticky top-0 left-0 w-full pl-4 pt-4">
                <h1 class="text-bold text-xl">Table of Contents</h1>
                <div class="prose">
                    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#kernel-transform-and-linear-approximation">Kernel Transform and Linear Approximation</a></li>
    <li><a href="#more-power-lets-replicate-the-brain">More Power: Let&rsquo;s Replicate the Brain</a>
      <ul>
        <li><a href="#implementing-the-neural-network">Implementing the Neural Network</a></li>
        <li><a href="#putting-it-all-together">Putting it all together</a></li>
      </ul>
    </li>
    <li><a href="#neural-network-the-data-hungry-monster">Neural Network: The Data Hungry Monster</a>
      <ul>
        <li><a href="#experience-replay">Experience Replay</a></li>
        <li><a href="#batch-update-and-double-q-learning">Batch Update and Double Q-learning</a></li>
        <li><a href="#moment-of-truth">Moment of Truth</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
                </div>
            </div>
        </div>
        <div class="col-span-3">
            <div class="text-justify break-words w-full">
                <div class="prose" style="max-width: none;">
                    <h2 id="introduction">Introduction</h2>
<p>Hey everyone! In my last post about Reinforcement Learning (check <a href="https://www.statwizard.in/posts/linear-learning/">here</a>) we learned about techniques to deal with state spaces of infinite sizes, using the method of linear value approximation. We also solved the Mountain Car problem, and it turned out that adding a bit of nonlinearity like kernel based features helped a lot to train a better RL agent.</p>
<p>Remember that we learned about the interesting Lunar Lander environment<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> before (see my <a href="https://www.statwizard.in/posts/q-learning/">post</a> on Q-learning), where we were trying to train an agent to successfully land a moon rover on the surface of the moon. Here&rsquo;s a picture to refresh your memory.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='episode-6001.gif'>
</img>
<p>The goal of this is to train an agent to successfully land on the surface of the moon. It gets to observe state values as an 8-dimensional vector, constituting of its linear positions, linear velocities, angular velocities and many more. It simply has 4 actions to do as follows:</p>
<ol>
<li>Do nothing.</li>
<li>Fire the left engine.</li>
<li>Fire the main engine.</li>
<li>Fire the right engine.</li>
</ol>
<p>Before, we tried to train an agent to solve this with Q-learning technique, but that did not got us very far. We used sparse encoding to represent the 8-dimensional observation vector to a tabular grid. However, since we now know about the techniques to handle infinite state space, we might be able to do better.</p>
<h2 id="kernel-transform-and-linear-approximation">Kernel Transform and Linear Approximation</h2>
<p>We first try the technique exactly as we discussed before in this <a href="https://www.statwizard.in/posts/linear-learning/">post</a>. We consider $400$ different kernel based feature transformations of those $8$ variables that the state vector spits out, and then we try to have an weight vector associated with each of these actions. We take the inner product of the respective weight vector with these kernel features, and that becomes our Q-value for that particular action.</p>
<p>We solved the <a href="https://gymnasium.farama.org/environments/classic_control/mountain_car/">Mountain Car problem</a> with this technique, in about $200$ episodes. Since this is a bit difficult problem compared to that, we allow the agent to train for about $2000$ episodes. Well, turns out it learned something useful, but still not decent enough.</p>
<p>Here&rsquo;s a picture of the agent after $2000$ episodes (It takes about 20-25 minutes to completely run on a Google Colab notebook).</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='figure-2.gif'>
</img>
<p>We also tracked the rewards over these episodes. If we look at the plot of the history of these rewards, it looks like this.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='figure-2a.png'>
</img>
<p>Turns out it is not learning after about $1000$ episodes, and it not getting better. Looks like we need something more powerful.</p>
<h2 id="more-power-lets-replicate-the-brain">More Power: Let&rsquo;s Replicate the Brain</h2>
<p>Well, if we look at the kernel transformed features from before, it was serving two purposes:</p>
<ul>
<li>It was adding a bit of nonlinearities to capture more complicated functional forms.</li>
<li>Having $400$ different kernel transformed features is like having $400$ different transformations of the features to work with, that gave us lots of powers.</li>
</ul>
<p>Mathematically, if we have the state vector $s = (x_1, x_2, \dots x_8)$, then the action value we were deriving as</p>
<p>$$
q(a_i) = w_{1,1}\phi_{1,1}(x_1) + \dots + w_{100,1}\phi_{100,1}(x_1) + w_{1,2}\phi_{1,2}(x_2) + \dots
$$</p>
<p>So, what if instead of directly combining these transformed values into features, we try to combine these transformed values into another set of transformed values, and then may be another set of transformed values, and then another, and so on (you get the idea!) until you are satisfied, and then these whole bunch of these variables can come together to form the final opinion about the Q-value. Mathematically it will be like this.</p>
<p>$$
\begin{align*}
h_{1} &amp; = \phi_1(w_{1,1} x_{1} + w_{1,2}x_{2} + \dots + w_{1,8}x_{8}) \\
h_{2} &amp; = \phi_1(w_{2,1} x_{1} + w_{2,2}x_{2} + \dots + w_{2,8}x_{8}) \\
\dots &amp; \dots \\
h_{64} &amp; = \phi_1(w_{64,1} x_{1} + w_{64,2}x_{2} + \dots + w_{64,8}x_{8}) \\
\end{align*}
$$</p>
<p>and then,</p>
<p>$$
\begin{align*}
g_{1} &amp; = \phi_2(v_{1,1} h_{1} + v_{1,2}h_{2} + \dots + v_{1,64}h_{64}) \\
g_{2} &amp; = \phi_2(v_{2,1} h_{1} + v_{2,2}h_{2} + \dots + v_{2,64}h_{64}) \\
\dots &amp; \dots \\
g_{64} &amp; = \phi_2(v_{64,1} h_{1} + v_{64,2}h_{2} + \dots + v_{64,64}h_{64})\\
\end{align*}
$$</p>
<p>and so on, and finally,</p>
<p>$$
\begin{align*}
q_{1} &amp; = u_{1,1} g_{1} + u_{1,2}g_{2} + \dots + u_{1,64}g_{64}\\
\dots &amp; \dots \\
q_{4} &amp; = u_{4,1} g_{1} + u_{64,2}g_{2} + \dots + u_{8,64}g_{64}\\
\end{align*}
$$</p>
<p>where all these $w, v$ and $u$s are weight parameters which will be changed as the agent learns more and more about the environment. The final output will be the Q-values of all the 4 actions. The functions $\phi_1$ and $\phi_2$ are adding the nonlinearities like the kernel. If you draw out the input output pairs of the above equations as connections, then it will look like as follows, a fully connected Network. (This gives you one part of the story, why <em>Neural Network</em> is a <em>Network</em>).</p>
<div class="w-full flex justify-center items-center mermaid">
  graph LR
    x1 & x2 ---> h1 & h2 & h3 ---> g1 & g2 ---> q
    subgraph State
      x1 & x2
    end
    subgraph Layer1
      h1 & h2 & h3
    end
    subgraph Layer2
      g1 & g2
    end
    subgraph Qvalue
      q
    end
</div>
<p>Another way to think about this network is analogus to human brain. Human brain is comprised of multiple layers of neurons, each neuron is like a connection you see above. Basically, it takes some inputs and performs a chemical reaction (read it as linear combination) and the reaction either activates the neuron and sends input to next layer, or the reaction may die down. McChulloch and Pitts were the first to introduce this idea of representing the neural activity by mathematical terms in 1943<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, and then Frank Rosenblatt implemented the same in 1958, and he named it <strong>Perceptron</strong>, a machine that does the perception. You may want to read more about it in its Wikipedia page<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. This should now explain the other half of the name, why <em>Neural Network</em> is <em>Neural</em>.</p>
<h3 id="implementing-the-neural-network">Implementing the Neural Network</h3>
<p>Here, we will be using the <code>tensorflow</code> package (developed by <a href="https://www.tensorflow.org/">Google</a>) to implement this network of neurons.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">simple_nn_model</span>():
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape <span style="color:#f92672">=</span> (<span style="color:#ae81ff">8</span>, ))     <span style="color:#75715e"># these will be a layer where we pass the 8-dimension state</span>
</span></span><span style="display:flex;"><span>    h1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;relu&#39;</span>)(x)  <span style="color:#75715e"># their linear combination now creates 64 new features, and adds a nonlinearity by using RELU function</span>
</span></span><span style="display:flex;"><span>    h2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;relu&#39;</span>)(h1)   <span style="color:#75715e"># now the previous layer&#39;s output gets linearly combined</span>
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">4</span>)(h2)   <span style="color:#75715e"># finally, we combine these hidden variables into 4 Q-values for 4 actions</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs <span style="color:#f92672">=</span> x, outputs <span style="color:#f92672">=</span> out)  <span style="color:#75715e"># finally we join them together and make a neural network model</span>
</span></span></code></pre></div><p>While our network is pretty simple, sometimes it can be very complicated. Tensorflow models has a nice <code>summary</code> function that we can use to extract relevant information from a model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> simple_nn_model()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>summary()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>Model: &#34;model&#34;
</span></span><span style="display:flex;"><span>_________________________________________________________________
</span></span><span style="display:flex;"><span> Layer (type)                Output Shape              Param #   
</span></span><span style="display:flex;"><span>=================================================================
</span></span><span style="display:flex;"><span> input_1 (InputLayer)        [(None, 8)]               0         
</span></span><span style="display:flex;"><span> dense_1 (Dense)             (None, 64)                576       
</span></span><span style="display:flex;"><span> dense_2 (Dense)             (None, 64)                4160      
</span></span><span style="display:flex;"><span> dense_3 (Dense)             (None, 4)                 260       
</span></span><span style="display:flex;"><span>=================================================================
</span></span><span style="display:flex;"><span>Total params: 4996 (19.52 KB)
</span></span><span style="display:flex;"><span>Trainable params: 4996 (19.52 KB)
</span></span><span style="display:flex;"><span>Non-trainable params: 0 (0.00 Byte)
</span></span></code></pre></div><p>The weights of the above network is randomly initialized, usually from a standard normal distribution. Let&rsquo;s now see how we can use this network to obtain the Q-values for an arbitrary state from the Lunar Lander environment.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>sample()   <span style="color:#75715e"># we sample one observation</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> model(tf<span style="color:#f92672">.</span>convert_to_tensor([state]))  <span style="color:#75715e"># run the model on it,</span>
</span></span><span style="display:flex;"><span>print(x)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>tf.Tensor([[-0.6147699 -0.8249271  5.23029    0.7746165]], shape=(1, 4), dtype=float32)
</span></span></code></pre></div><p>One particular thing to notice here is that we are passing <code>[state]</code> to the model instead of simply <code>state</code> variable. This is because the neural networks provided in tensorflow are usually designed to work on a batch of inputs, instead of a single input, so it expects the first dimension of any variable to always represent the batch. Here, making a list out of <code>state</code> variable ensures we pass a 2-dimensional tensor to the model with batch dimension as $1$.</p>
<p>Next, in order to train the RL agent, we need to perform gradient descent to minimize the squared loss between the current value and the target value (which is either TD target, or Q-learning target or SARSA target). For linear value approximation, computing the gradient was easy. However, in case of a neural network, you need a bit more use of Chain rule of derivatives to calculate the gradient. Fortunately, we don&rsquo;t have to do that manually, <code>tensorflow</code> is nice enough to provide an automatic gradient calculator, called <code>tf.GradientTape()</code>. It is a python execution context which tracks down all operations done within the context, and then provides a gradient value through <code>tf.GradientTape().gradient()</code> function.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>observation, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> t:
</span></span><span style="display:flex;"><span>    action_probs <span style="color:#f92672">=</span> model(tf<span style="color:#f92672">.</span>convert_to_tensor([observation]))
</span></span><span style="display:flex;"><span>    new_observation, reward, terminated, truncated, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)  <span style="color:#75715e"># Do the action in the environment</span>
</span></span><span style="display:flex;"><span>    new_action_probs <span style="color:#f92672">=</span> model(tf<span style="color:#f92672">.</span>convert_to_tensor([new_observation]))
</span></span><span style="display:flex;"><span>    target <span style="color:#f92672">=</span> reward <span style="color:#f92672">+</span> DISCOUNT_FACTOR <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_max(new_action_probs)  <span style="color:#75715e"># create the target</span>
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> (target <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_max(action_probs))<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>  <span style="color:#75715e"># this is the squared error loss</span>
</span></span><span style="display:flex;"><span>    grad <span style="color:#f92672">=</span> t<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>trainable_variables)    <span style="color:#75715e"># Calculate gradients with respect to every trainable variable</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># finally, we print the gradients</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> var, g <span style="color:#f92672">in</span> zip(model<span style="color:#f92672">.</span>trainable_variables, grad):
</span></span><span style="display:flex;"><span>  print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>var<span style="color:#f92672">.</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">, shape: </span><span style="color:#e6db74">{</span>g<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>dense_6/kernel:0, shape: (8, 64)
</span></span><span style="display:flex;"><span>dense_6/bias:0, shape: (64,)
</span></span><span style="display:flex;"><span>dense_7/kernel:0, shape: (64, 64)
</span></span><span style="display:flex;"><span>dense_7/bias:0, shape: (64,)
</span></span><span style="display:flex;"><span>dense_8/kernel:0, shape: (64, 4)
</span></span><span style="display:flex;"><span>dense_8/bias:0, shape: (4,)
</span></span></code></pre></div><p>Now we need to modify the weights of the neural network and reduce them by the learning rate times the appropriate gradient. To do that, we can set up an <code>optimizer</code> in <code>tensorflow</code>, and apply a single step of the optimizer, instead of directly modifying the model variable and its weights.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>)   <span style="color:#75715e"># Instantiate an optimizer</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run one step of gradient descent by updating the value of the variables to minimize the loss.</span>
</span></span><span style="display:flex;"><span>optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grad, model<span style="color:#f92672">.</span>trainable_weights))
</span></span></code></pre></div><h3 id="putting-it-all-together">Putting it all together</h3>
<p>So far, we know how to create a neural network model and how to update the model parameters to reduce the loss function. Therefore, training the RL agent simply requires us to specify the target in each step of each episode, and then taking one step towards making the model better by applying the gradient descent rule.</p>
<p>Here&rsquo;s how the entire code would have looked like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>N_EPISODE <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>
</span></span><span style="display:flex;"><span>EPSILON <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>   <span style="color:#75715e"># 10% of the time it would explore</span>
</span></span><span style="display:flex;"><span>DISCOUNT_FACTOR <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>LR <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span>  <span style="color:#75715e"># learning rate</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>SGD(learning_rate<span style="color:#f92672">=</span>LR)   <span style="color:#75715e"># Instantiate an optimizer</span>
</span></span><span style="display:flex;"><span>episode_rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(N_EPISODE)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> ep <span style="color:#f92672">in</span> tqdm(range(N_EPISODE)):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># loop through the environment</span>
</span></span><span style="display:flex;"><span>    observation, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> t:
</span></span><span style="display:flex;"><span>            action_probs <span style="color:#f92672">=</span> model(tf<span style="color:#f92672">.</span>convert_to_tensor([observation]))
</span></span><span style="display:flex;"><span>            action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(action_probs<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random() <span style="color:#f92672">&lt;</span> EPSILON:
</span></span><span style="display:flex;"><span>              action <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()   <span style="color:#75715e"># randomly do an action for exploration</span>
</span></span><span style="display:flex;"><span>            new_observation, reward, terminated, truncated, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)  <span style="color:#75715e"># Do the action in the environment</span>
</span></span><span style="display:flex;"><span>            new_action_probs <span style="color:#f92672">=</span> model(tf<span style="color:#f92672">.</span>convert_to_tensor([new_observation]))
</span></span><span style="display:flex;"><span>            target <span style="color:#f92672">=</span> reward <span style="color:#f92672">+</span> DISCOUNT_FACTOR <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_max(new_action_probs)  <span style="color:#75715e"># create the target</span>
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> (target <span style="color:#f92672">-</span> action_probs<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>][action])<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>  <span style="color:#75715e"># this is the squared error loss</span>
</span></span><span style="display:flex;"><span>            grad <span style="color:#f92672">=</span> t<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>trainable_variables)        <span style="color:#75715e"># Calculate gradients with respect to every trainable variable</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Run one step of gradient descent by updating the value of the variables to minimize the loss.</span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>apply_gradients(zip(grad, model<span style="color:#f92672">.</span>trainable_weights))
</span></span><span style="display:flex;"><span>        episode_rewards[ep] <span style="color:#f92672">+=</span> reward   <span style="color:#75715e"># update the reward</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> terminated <span style="color:#f92672">or</span> truncated:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        observation <span style="color:#f92672">=</span> new_observation   <span style="color:#75715e"># update the observation</span>
</span></span></code></pre></div><p>We train for 200 episodes first to see what&rsquo;s happenning. It takes about 10 minutes in the same Google Colab notebook.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='figure-3.gif'>
</img>
<p>Well, turns out it is always taking the action of doing nothing. The history of rewards tells us that the rewards obtained in the episodes are wildly varying, but on average, it has learned almost nothing.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='figure-3a.png'>
</img>
<p>We had our hopes up with this better model, but let&rsquo;s try to investigate what&rsquo;s causing this issue.</p>
<h2 id="neural-network-the-data-hungry-monster">Neural Network: The Data Hungry Monster</h2>
<p>There are basically two things that went wrong while training the neural network based RL agent.</p>
<ol>
<li>
<p>Turns out since neural network is a bit complicated compared to the linear value approximation, so it requires much more amount of data to effectively train.</p>
</li>
<li>
<p>In linear value approximation, we could selectively update weights associated with the Q-value estimation for a specific action, hence all the weights were not being updated.</p>
</li>
</ol>
<ul>
<li>
<p>We can have separately $4$ different neural networks, one for each action. Then we can selectively update a single neural network for that single action. However, we would want some hidden layer features to be common across all these networks (some features which are important for knowing may be whether to fire some engine or not, and is shared between multiple actions). This is not possible if we use multiple neural network.</p>
</li>
<li>
<p>Another problem with this approach is that it does not scale. For instance, if you have $100$ possible actions, you would need $100$ different networks.</p>
</li>
</ul>
<h3 id="experience-replay">Experience Replay</h3>
<p><strong>Experience Replay</strong> is one way to address the first concern. Basically it tells to store some state, action, reward tuples in a buffer, and when the model needs to update its parameter, it can consume a batch of these tuples from the buffer and train on it. The buffer has a fixed size. When it overflows the buffer, we simply remove the oldest entries from the buffer to keep its size in check.</p>
<p>This method is called Experience Replay because it allows the RL agent to replay some of its stored experiences and keep learning from it over and over. This means, even when the agent is not interacting with the actual environment, it can still keep learning from its past experiences.</p>
<p>Here is how we can implement an experience replay buffer using <code>python</code> classes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ReplayBuffer</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, size <span style="color:#f92672">=</span> <span style="color:#ae81ff">10_000</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>size <span style="color:#f92672">=</span> size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transitions <span style="color:#f92672">=</span> []  <span style="color:#75715e"># each transition is tuple of (old_state, action, reward, new_state, done)</span>
</span></span></code></pre></div><p>We need two methods in this class. One for adding a new transition to the buffer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add</span>(self, transition):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transitions<span style="color:#f92672">.</span>append(transition)   <span style="color:#75715e"># append the transition to the last</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(self<span style="color:#f92672">.</span>transitions) <span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>size:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>transitions <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transitions[:self<span style="color:#f92672">.</span>size]  <span style="color:#75715e"># buffer is overflowing, so slice it</span>
</span></span></code></pre></div><p>Another is to get a batch of transactions for the RL agent to train on.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_batch</span>(self, batch_size):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> random<span style="color:#f92672">.</span>choices(self<span style="color:#f92672">.</span>transitions, k <span style="color:#f92672">=</span> batch_size)
</span></span></code></pre></div><h3 id="batch-update-and-double-q-learning">Batch Update and Double Q-learning</h3>
<p>To solve the second concern, instead of having one neural network for each action, we will consider only two copies of the same neural network. One of them will be evaluating the Q-values by keeping its weights fixed for multiple iterations, and another will be updating the weights to find the optimal policy.</p>
<p>Mathematically, it is like having two sets of weights $w_1$ and $w_2$. Let us denote the neural network by $Q$, so</p>
<p>$$
\Delta w_1 = \alpha \nabla (R_t + \gamma Q(w_2; S_{t+1}) - Q(w_1; S_{t}) )^2
$$</p>
<p>where $\alpha$ and $\gamma$ are the learning rate and discount factor respectively. Here, we keep updating the weights $w_1$ without updating the weights $w_2$ for several steps. After a fixed number of steps, we finally update $w_2$ by the current value of the weights $w_1$. This means the target $R_t + \gamma Q(w_2; S_{t+1})$ remains static for several steps, creating a more stable target for the neural network for achieve. This diminishes the chance of having wildly varying rewards function as the number of episodes progresses. This $Q(w_2; )$ is often called as the evaluation network or the target model.</p>
<p>We first make a function that produces the tensorflow neural network model. In addition to that, we also specify a custom loss function that takes the true Q-values (i.e., as obtained by the TD-target) and the predicted Q-values produced by the current neural network, and produces the sqaured difference of their maximums. This is exactly what you would get as an error for the Q-learning method.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># the custom loss function to use</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">custom_loss_fn</span>(q_true, q_pred):
</span></span><span style="display:flex;"><span>    q_true_action <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_max(q_true, axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    q_pred_action <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_max(q_pred, axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>reduce_sum((q_true_action <span style="color:#f92672">-</span> q_pred_action)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># the neural network model</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">simple_nn_model</span>(LR):
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape <span style="color:#f92672">=</span> (<span style="color:#ae81ff">8</span>, ))
</span></span><span style="display:flex;"><span>    h1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    h2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;relu&#39;</span>)(h1)
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">4</span>, activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;linear&#39;</span>)(h2)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs <span style="color:#f92672">=</span> x, outputs <span style="color:#f92672">=</span> out)
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>LR)
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span>optimizer, loss<span style="color:#f92672">=</span>custom_loss_fn)   <span style="color:#75715e"># makes easier to train</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span></code></pre></div><p>The last two lines is a bit new, they are preparing the network network model to reduce that loss function. This compilation step has no outside impact, but the model training (i.e., the gradient descent) becomes much more efficient and faster to do.</p>
<p>Now we get an evaluation model, which is a copy of the current neural network, we can use the following function, which creates a blank model using the above function and then copies the weights from the current model to the blank model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">copy_model</span>(model, LR):
</span></span><span style="display:flex;"><span>  model2 <span style="color:#f92672">=</span> simple_nn_model_with_regularizer(LR)   <span style="color:#75715e"># used only for evaluation, so LR does not matter</span>
</span></span><span style="display:flex;"><span>  model2<span style="color:#f92672">.</span>set_weights(model<span style="color:#f92672">.</span>get_weights())
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> model2
</span></span></code></pre></div><p>One particular thing you might be thinking is that why we did not simply do</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model2 <span style="color:#f92672">=</span> model
</span></span></code></pre></div><p>The reason is that in such a case, <code>model2</code> simply refers to a pointer to the original variable <code>model</code> that holds the neural network, it does not create an independent copy. Hence, if we perform gradient descent on the first neural network <code>model</code>, the weights of <code>model2</code> also will keep changing, but we do not want that.</p>
<h3 id="moment-of-truth">Moment of Truth</h3>
<p>So, we combine all of these ideas and put them together in action. Before that, we define as few utility functions which will be reused.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># get the q-values for the current state, based on the model</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_q_values</span>(model, state):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> model<span style="color:#f92672">.</span>predict([state])[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># selects the best action but with epsilon-greedy nature</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">select_action_epsilon_greedy</span>(q_values, epsilon):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random() <span style="color:#f92672">&lt;</span> epsilon:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>argmax(q_values)
</span></span></code></pre></div><p>We also provide another function that uses the evaluation network and a batch of state transitions from the replay buffer to compute the target values for that batch.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculate_target_values</span>(target_model, state_transitions, discount_factor):
</span></span><span style="display:flex;"><span>  targets <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> state_transition <span style="color:#f92672">in</span> state_transitions:
</span></span><span style="display:flex;"><span>    new_state <span style="color:#f92672">=</span> state_transition[<span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>    best_action_next_state_q_value <span style="color:#f92672">=</span> (get_q_values(target_model, new_state))<span style="color:#f92672">.</span>max()  <span style="color:#75715e"># this is like the Q-learning, gets the best Q-value in the next state</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> state_transition[<span style="color:#ae81ff">4</span>]:
</span></span><span style="display:flex;"><span>      target_value <span style="color:#f92672">=</span> state_transition[<span style="color:#ae81ff">2</span>]   <span style="color:#75715e"># if it is terminal state, no next reward</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>      target_value <span style="color:#f92672">=</span> state_transition[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">+</span> discount_factor <span style="color:#f92672">*</span> best_action_next_state_q_value
</span></span><span style="display:flex;"><span>    target_vector <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]  <span style="color:#75715e"># creates a vector to hold the Q-values of 4 actions</span>
</span></span><span style="display:flex;"><span>    target_vector[state_transition[<span style="color:#ae81ff">1</span>]] <span style="color:#f92672">=</span> target_value  <span style="color:#75715e"># here we only update the target for the current action</span>
</span></span><span style="display:flex;"><span>    targets<span style="color:#f92672">.</span>append(target_vector)
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>array(targets)
</span></span></code></pre></div><p>And here we specify the hyperparameters, initialize a bunch of stuffs like our policy and evaluation networks, and also the replay buffer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>BUFFER_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">10_000</span>
</span></span><span style="display:flex;"><span>EPSILON <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.05</span>  <span style="color:#75715e"># always explore 5% of time</span>
</span></span><span style="display:flex;"><span>DISCOUNT <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>LR <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span>   <span style="color:#75715e"># learning rate</span>
</span></span><span style="display:flex;"><span>N_EPISODE <span style="color:#f92672">=</span> <span style="color:#ae81ff">2_000</span>  <span style="color:#75715e"># number of episodes to go through    </span>
</span></span><span style="display:flex;"><span>TARGET_NETWORK_REPLACE_STEP <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>   <span style="color:#75715e"># every 100 steps, we will update our evaluation network as the current model</span>
</span></span><span style="display:flex;"><span>TRAIN_EVERY <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>   <span style="color:#75715e"># every 4 steps, halt and train using batch from replay buffer</span>
</span></span><span style="display:flex;"><span>TRAIN_BATCH <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>   <span style="color:#75715e"># use a batch size of 128 </span>
</span></span><span style="display:flex;"><span>replay_buffer <span style="color:#f92672">=</span> ReplayBuffer(BUFFER_SIZE)  <span style="color:#75715e"># initialize the replay buffer</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> simple_nn_model(LR)   <span style="color:#75715e"># create the policy model</span>
</span></span><span style="display:flex;"><span>target_model <span style="color:#f92672">=</span> copy_model(model, LR)    <span style="color:#75715e"># create the evaluation network</span>
</span></span><span style="display:flex;"><span>episode_rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(N_EPISODE)     <span style="color:#75715e"># an array to store historical rewards of the episodes</span>
</span></span><span style="display:flex;"><span>stepcount <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>     <span style="color:#75715e"># counts the number of steps</span>
</span></span></code></pre></div><p>Finally, we combine all of these into a training loop to train the RL agent.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> ep <span style="color:#f92672">in</span> tqdm(range(N_EPISODE)):
</span></span><span style="display:flex;"><span>  observation, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>    stepcount <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    q_values <span style="color:#f92672">=</span> get_q_values(model, observation)
</span></span><span style="display:flex;"><span>    action <span style="color:#f92672">=</span> select_action_epsilon_greedy(q_values, EPSILON)
</span></span><span style="display:flex;"><span>    new_observation, reward, terminated, truncated, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)  <span style="color:#75715e"># Do the action in the environment</span>
</span></span><span style="display:flex;"><span>    episode_rewards[ep] <span style="color:#f92672">+=</span> reward
</span></span><span style="display:flex;"><span>    replay_buffer<span style="color:#f92672">.</span>add((observation, action, reward, new_observation, terminated <span style="color:#f92672">or</span> truncated))   <span style="color:#75715e"># add it to the replay buffer</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> stepcount <span style="color:#f92672">%</span> TARGET_NETWORK_REPLACE_STEP <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>      target_model <span style="color:#f92672">=</span> copy_model(model, LR)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> stepcount <span style="color:#f92672">%</span> TRAIN_EVERY <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># train the model, take a batch from replay buffer</span>
</span></span><span style="display:flex;"><span>      batch <span style="color:#f92672">=</span> replay_buffer<span style="color:#f92672">.</span>get_batch(batch_size<span style="color:#f92672">=</span>TRAIN_BATCH)
</span></span><span style="display:flex;"><span>      targets <span style="color:#f92672">=</span> calculate_target_values(target_model, batch, DISCOUNT)
</span></span><span style="display:flex;"><span>      states <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([state_transition[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> state_transition <span style="color:#f92672">in</span> batch])
</span></span><span style="display:flex;"><span>      model<span style="color:#f92672">.</span>fit(states, targets, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, batch_size<span style="color:#f92672">=</span>len(targets), verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)   <span style="color:#75715e"># this does the gradient descent, but over the batch of inputs</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> terminated <span style="color:#f92672">or</span> truncated:
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>    observation <span style="color:#f92672">=</span> new_observation
</span></span></code></pre></div><p>Here, we could use the <code>model.fit</code> method to perform the gradient descent without using <code>tf.GradientTape()</code> as before, this is because we compiled the model before.</p>
<p>It took about 25 minutes to complete the training, although I did a little cheating! Used a free GPU at Google colab as it was available. Here&rsquo;s how the result looks after 2000 episodes.</p>
<img 
    class='max-w-md mx-4 md:mx-auto rounded-lg h-auto shadow-md' 
    src='figure-4.gif'>
</img>
<p>Great! Now our lunar lander is successfully prepared to land on the surface of the moon, and this means, you have now added a bit of rocket science to your awesome store of knowledge.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Starting from an almost zero knowledge of Reinforcement Learning, we have come a long way. Congratulations for making it this far! Hope you have liked the series of posts.</p>
<p>This was a beginner-friendly introductory series for all the RL enthusiasts out there. We tried to learn the basic concepts, implement a few hands-on examples, and we have built a whooping Lunar Lander! If you wish for more posts like this, on advanced Reinforcement Learning techniques, let me know in the comments!</p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Gymnasium Lunar Lander: <a href="https://gymnasium.farama.org/environments/box2d/lunar_lander/">https://gymnasium.farama.org/environments/box2d/lunar_lander/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Rosenblatt, Frank (1958). &ldquo;The Perceptron—a perceiving and recognizing automaton&rdquo;. Report 85-460-1. Cornell Aeronautical Laboratory.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Wikipedia - Perceptron. <a href="https://en.wikipedia.org/wiki/Perceptron">https://en.wikipedia.org/wiki/Perceptron</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

                </div>
            </div>
        </div>
    </div>
    <div class="flex flex-row justify-center items-center mb-3">
        <h2 class="text-lg text-blue-600">
            With heartfelt appreciation, thank you for being a valued reader, until next time!
        </h2>
    </div>
</div>



<div class="my-6 max-w-6xl md:mx-auto mx-4 text-center shadow-md rounded-md p-4">
    <p class="italic text-lg font-semibold my-4">Explore more posts like this</p>
    <div class="flex flex-row justify-center gap-4">
        
            <a href="//localhost:1313/posts/2023/linear-learning/" 
                class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                    transition-all ease-in-out rounded-md shadow-sm">
                Previous Post
            </a>
        
        
            <a href="//localhost:1313/posts/2023/storage-analysis/" class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900
             focus:bg-neutral-900 transition-all ease-in-out rounded-md shadow-sm">
                Next Post
            </a>
        
    </div>
    <div class="flex flex-row justify-center gap-4 my-4">
        <a href="//localhost:1313/posts/" 
            class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                transition-all ease-in-out rounded-md shadow-sm">
            See all posts
        </a>
    </div>
</div>


<div class="my-6 max-w-6xl md:mx-auto mx-4">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statwizard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>



      </div>
    </div>

    <script>
    $(document).ready(() => {

        
        $('#mobile-menu-button').click(() => {
            $('#mobile-menu').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('block');
            $('#mobile-menu-open-icon').toggleClass('hidden');
            $('#mobile-menu-open-icon').toggleClass('block');
        });

    });
</script>
<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="statwizard" data-description="Support me on Buy me a coffee!" data-message="Thank you for visiting! You can now support the StatWizard to build a Hogwarts for Statistics!" data-color="#5F7FFF" data-position="Right" data-x_margin="18" data-y_margin="18"></script>

<script async src="https://kit.fontawesome.com/ca14d5004b.js" crossorigin="anonymous"></script>
 
    


    <script type="module" defer>
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, securityLevel: "loose" });
    </script>



    
    <script defer>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>



  </body>
</html>
