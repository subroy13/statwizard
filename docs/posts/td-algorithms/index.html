<!DOCTYPE html>
<html lang="en">
<head>
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-97N9TLJ517"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-97N9TLJ517');
    </script>
    

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="/svg/avatar.svg">
    <title>
        
Reinforcement Learning (Part 4) - Temporal Difference Algorithms

    </title>

    

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,500;0,900;1,100;1,500;1,900&display=swap" rel="stylesheet">


<script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>


<script src="https://kit.fontawesome.com/ca14d5004b.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">




<link rel="stylesheet" type="text/css" href="/css/index.d5a5c99c471763e7038d35dc4e9d20e7.css" />





<link rel="stylesheet" href="/css/paginatorStyle.css">


<script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>


<style>
  html {
      scroll-behavior: smooth;
  }
</style>



<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/>

     
     
</head>
<body>
    <div class="app-container min-h-[100vh] flex flex-col justify-between">
        <nav class="bg-white shadow-xl z-50">
    
    <div class="hidden mx-auto max-w-7xl py-2 px-4 md:px-6 lg:px-8 md:flex flex-row items-center">
        <div class="w-[200px] flex items-center">
            <a class="uppercase font-extrabold font-mono text-2xl flex flex-row justify-center items-center gap-2"> 
                <img src="/images/logo-wide.png" class="h-[75px] inline-block" />    
            </a>
        </div>
        <div class="w-full flex flex-row justify-center items-center gap-4">
            
                <a href="/" class="px-4 py-2 border rounded-lg hover:bg-gray-100">
                    Home
                </a>
            
                <a href="/posts" class="px-4 py-2 border rounded-lg hover:bg-gray-100">
                    Posts
                </a>
            
                <a href="/aboutme" class="px-4 py-2 border rounded-lg hover:bg-gray-100">
                    About Me
                </a>
            
                <a href="/#contact-me" class="px-4 py-2 border rounded-lg hover:bg-gray-100">
                    Contact Me
                </a>
            
        </div>
        <div class="w-[200px] flex flex-row justify-around items-center">
            
                <a href="mailto:subhrajyotyroy@gmail.com" class="hover:text-gray-600 transition-all ease-in-out">
                    <i class = "fas fa-envelope fa-lg"></i>
                </a>
            
                <a href="https://www.facebook.com/subroy13/" class="hover:text-gray-600 transition-all ease-in-out">
                    <i class = "fab fa-facebook fa-lg"></i>
                </a>
            
                <a href="https://github.com/subroy13" class="hover:text-gray-600 transition-all ease-in-out">
                    <i class = "fab fa-github fa-lg"></i>
                </a>
            
                <a href="https://www.linkedin.com/in/subroy13" class="hover:text-gray-600 transition-all ease-in-out">
                    <i class = "fab fa-linkedin-in fa-lg"></i>
                </a>
            
                <a href="#" class="hover:text-gray-600 transition-all ease-in-out">
                    <i class = "fab fa-instagram fa-lg"></i>
                </a>
            
        </div>
    </div>

    
    <div class="flex py-4 px-2 max-w-7xl flex-row justify-end items-center md:hidden">
        <div class="w-full flex justify-center items-center">
            <p class="uppercase font-extrabold font-mono text-2xl">
                StatWizard
            </p>
        </div>
        <div class="w-[30px] mr-4 relative">
            <button type="button"
                id="mobile-menu-button"
                class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:bg-gray-700 hover:text-white focus:outline-none"
                aria-controls="mobile-menu" aria-expanded="false">
                <span class="sr-only">Open main menu</span>
                    
                    <svg id="mobile-menu-close-icon" class="bg-white text-neutral-800 outline-none block h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
                        aria-hidden="true">
                        <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" />
                    </svg>
                
                <svg id="mobile-menu-open-icon" class="bg-white text-neutral-800 outline-none hidden h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
                    aria-hidden="true">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" />
                </svg>
            </button>

            <div id="mobile-menu" class="absolute end-0 mt-4 -mr-4 bg-white w-[90vw] hidden">
                <ul class="flex flex-col">
                    
                        <li class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100">
                            <a href="/" class="">
                                Home
                            </a>
                        </li>
                    
                        <li class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100">
                            <a href="/posts" class="">
                                Posts
                            </a>
                        </li>
                    
                        <li class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100">
                            <a href="/aboutme" class="">
                                About Me
                            </a>
                        </li>
                    
                        <li class="px-4 sm:px-8 py-2 border first:rounded-t-lg last:rounded-b-lg hover:bg-gray-100">
                            <a href="/#contact-me" class="">
                                Contact Me
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </div>
</nav>


        <div class="content-container">
            



    <div class="relative h-[33vh] max-h-[300px] w-full bg-cover bg-center bg-no-repeat bg-fixed rounded-b-md" 
        style = "background-image: url('/posts/td-algorithms/featured.png')">
        
            <div class = "absolute top-0 left-0 bg-neutral-800/100 w-fit text-white p-1 ml-1 font-xs rounded-md">
                <p>Cover image taken from </p>
            </div>
        
    </div>



<div class="mt-4 flex flex-col gap-4 mx-2 px-2 md:mx-4 md:px-8">
    <h1 class="text-3xl text-center text-neutral-600 font-bold md:px-8 md:pt-6">
        Reinforcement Learning (Part 4) - Temporal Difference Algorithms
    </h1>
    <div class="m-2 p-4 rounded-lg shadow-lg border-2">
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="flex flex-col justify-center items-center gap-2">   
                <div class="mt-1 flex flex-row text-neutral-800 gap-4">
                    <p class="font-bold">Date:</p>
                    <p class="font-normal">24 August, 2023</p>
                </div> 
                <p class="mt-1 text-neutral-600">
                    <span class="font-bold">11</span> minutes read
                </p>
                <div class="my-1">
                    
                        <a href="/tags/reinforcement-learning/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Reinforcement Learning
                        </a>
                    
                </div>    
            </div>
            <div class="flex flex-col justify-start gap-2">
                <h1 class="text-lg font-bold">Prerequisites</h1>
                <ul class="space-y-2 text-left">
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Python Programming - </span>
                        
                            <span class="text-yellow-600 font-semibold">Intermediate</span>
                        
                    </li>
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Probability (Conditional Probability, Expection) - </span>
                        
                            <span class="text-yellow-600 font-semibold">Intermediate</span>
                        
                    </li>
                    
                </ul>
            </div>    
        </div>
        <p class="mt-4 p-4 text-sm">
            <span class="font-semibold">Summary: </span> This is the 4th part of the Reinforcement Learning Series: Here I discuss about how the idea of Bellman equations and Monte Carlo methods can be combined into a single algorithm called Temporal Difference. We will also learn about its variants and how we can use it to estimate the value functions.
        </p>
    </div>
    <div class="grid grid-cols-1 md:grid-cols-4 gap-2">
        <div class="relative col-span-1">
            <div class="sticky top-0 left-0 w-full pl-4 pt-4">
                <h1 class="text-bold text-xl">Table of Contents</h1>
                <div class="prose">
                    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#temporal-difference-learning">Temporal Difference Learning</a>
      <ul>
        <li><a href="#update-rule">Update Rule</a></li>
        <li><a href="#td-algorithm-for-maze-game">TD Algorithm for Maze Game</a></li>
      </ul>
    </li>
    <li><a href="#n-step-td-variant">n-step TD Variant</a></li>
    <li><a href="#tdlambda-variant2">TD($\lambda$) Variant</a></li>
    <li><a href="#next-in-queue">Next in Queue</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
                </div>
            </div>
        </div>
        <div class="col-span-3">
            <div class="text-justify break-words w-full">
                <div class="prose" style="max-width: none;">
                    <h2 id="introduction">Introduction</h2>
<p>In my previous post (check <a href="https://www.statwizard.in/posts/markov-decision-process/">here</a> if you haven&rsquo;t already), we learned two contrasting method of value function estimation. First, the Monte Carlo method which accumulates experiences. Second, the dynamic programming (sometimes also called bootstrapping) method which uses the Bellman equation. Both of these algorithms has its good and bad effects.</p>
<p>Monte Carlo method accumulates experiences, so it does not rely on your knowledge of how the environment behaves. This is particularly useful when you are playing a two-person board game where your opponent is a human, and you don&rsquo;t exactly know which move it is going to do. However, Monte Carlo requires you to reach the end of the game unless you can update your estimates, in order words, as you play the game, you are gaining knowledge about how the game works, but you are not putting them into use but storing them to be used later. This means you have a large memory requirements for the games which runs very long, and sometimes there is no end or terminal state (think of games like temple run or subway surfer which has no end unless you lose), so the Monte Carlo method never updates the estimates.</p>
<p>On the other end of the spectrum, we have Dynamic programming method which uses its current estimates along with the Bellman equation to refine and adjust its own estimates, thus enabling one to work with these infinitely long games. However, the problem is it is not model-free, i.e., it requires the knowledge of how environment works, and often that is a big problem by itself.</p>
<h2 id="temporal-difference-learning">Temporal Difference Learning</h2>
<p>So in 1988, Richard Sutton<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> came up with an algorithm that combines the benefits of these two algorithms. He named the method <strong>Temporal Difference</strong>, we shall why such a name is appropriate in a moment. He started by rewriting the Bellman equation as a two-step expectation as follows:</p>
<p>$$
v^\pi(S_{t}) = \sum_{A_t} \sum_{S_{t+1}} \pi(A_t \mid S_t) p(S_{t+1} \mid S_t, A_t) \left( R_{t+1} + \gamma v^\pi(S_{t+1}) \right)
$$</p>
<p>Here the quantity $\pi(A_t \mid S_t) p(S_{t+1} \mid S_t, A_t)$ denotes the probability of a two-step process: from the current state $S_t$, we generate an action $A_t$ according to policy $\pi$ and then environment gives back a state $S_{t+1}$. In a model-free design, we don&rsquo;t have the access to the probability $p(S_{t+1} \mid S_t, A_t)$. So Monte Carlo method avoids that by simulating the action and then receiving the state to accumulate the experience. Hence, in the above equation, we can similarly remove the expectation step and replace that by a simulation step, hence we believe that the following relation should hold approximately.</p>
<p>$$
v^\pi(S_{t}) \approx R_{t+1} + \gamma v^\pi(S_{t+1})
$$</p>
<h3 id="update-rule">Update Rule</h3>
<p>Now we look at the equation which incrementally updates the value estimates for Monte Carlo, let the current state value being updated is $s$ and it has been seen $n$ times below. So,</p>
<p>$$
v^\pi_{new}(s) = \dfrac{n v^\pi_{old}(s) + G_t}{n+1} = v^\pi_{old}(s) + \dfrac{1}{n+1}\left( G_t - v^\pi_{old}(s) \right)
$$</p>
<p>It means that the new estimate is old estimate plus a multiple of the error $G_t - v^\pi_{old}(s)$ in the current estimate. It is as if $G_t$ is the target that we are trying to estimate using the value function. However, we have established that $v^\pi(S_{t}) \approx R_{t+1} + \gamma v^\pi(S_{t+1})$, so we can replace $G_t$ by $R_{t+1} + \gamma v^\pi(S_{t+1})$. But we do not know $v^\pi(S_{t+1})$ yet, so we again approximate that by our current estimate of the value function. Hence we can consider an update equation as</p>
<p>$$
v^\pi_{new}(S_t) = v^\pi_{old}(S_t) + \alpha\left( R_{t+1} + \gamma v^\pi_{old}(S_{t+1}) - v^\pi_{old}(S_t) \right)
$$</p>
<p>This method is called <strong>Temporal Difference (TD)</strong><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, as the update equation now consists of the incremental changes in value estimates between two successive timepoints ($\gamma v^\pi_{old}(S_{t+1}) - v^\pi_{old}(S_t)$). The quantity $\alpha$ is called the learning rate which is typically choosen to be a small value.</p>
<p>In essence, the TD algorithm proceeds with the following steps:</p>
<ol>
<li>Start with an initial estimate of the value function, and a starting state $S_0$.</li>
<li>For $t = 1, 2, \dots $:
<ul>
<li>Simulate one step of your policy, $A_t$ and receive $R_{t+1}, S_{t+1}$, starting from $S_t$.</li>
<li>Use the update equation: $v^\pi(S_{t}) \rightarrow v^\pi(S_{t}) + \alpha(R_{t+1} + \gamma v^\pi(S_{t+1}) - v^\pi(S_{t}))$.</li>
<li>Keep doing this until convergence, i.e., the value estimates are not changing much.</li>
</ul>
</li>
</ol>
<h3 id="td-algorithm-for-maze-game">TD Algorithm for Maze Game</h3>
<p>Now we will try to apply the TD algorithm on the same maze game from the <a href="https://www.statwizard.in/posts/markov-decision-process/">last post</a>. Just to recap, the maze game has a maze shown as follow in the following figure, starting at the red corner to reach the blue corner, avoiding the obstacles shown in gray. The reward for reaching the end goal is $100$ but hitting any obstacle is $-1$.</p>
<p><img src="fig1.png" alt=""></p>
<p>Here is a code that performs the TD algorithm on the maze game.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># temporal difference learning</span>
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>   <span style="color:#75715e"># number of episodes</span>
</span></span><span style="display:flex;"><span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span>
</span></span><span style="display:flex;"><span>val_ests <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>val_ests[<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>  <span style="color:#75715e"># the value for the terminating state</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>  <span style="color:#75715e"># learning rate</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(B):
</span></span><span style="display:flex;"><span>  niter <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>  <span style="color:#75715e"># number of iterations</span>
</span></span><span style="display:flex;"><span>  cur_state <span style="color:#f92672">=</span> (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>)  <span style="color:#75715e"># the starting state</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># take a random move from current state</span>
</span></span><span style="display:flex;"><span>    action <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;up&#34;</span>, <span style="color:#e6db74">&#34;down&#34;</span>, <span style="color:#e6db74">&#34;left&#34;</span>, <span style="color:#e6db74">&#34;right&#34;</span>][np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">4</span>)]
</span></span><span style="display:flex;"><span>    new_state, reward <span style="color:#f92672">=</span> gridworld_maze(cur_state, action)
</span></span><span style="display:flex;"><span>    val_ests[cur_state[<span style="color:#ae81ff">0</span>], cur_state[<span style="color:#ae81ff">1</span>]] <span style="color:#f92672">+=</span> lr <span style="color:#f92672">*</span> (reward <span style="color:#f92672">+</span> gamma <span style="color:#f92672">*</span> val_ests[new_state[<span style="color:#ae81ff">0</span>], new_state[<span style="color:#ae81ff">1</span>]] <span style="color:#f92672">-</span> val_ests[cur_state[<span style="color:#ae81ff">0</span>], cur_state[<span style="color:#ae81ff">1</span>]])  <span style="color:#75715e"># the TD(0) step</span>
</span></span><span style="display:flex;"><span>    niter <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> niter <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">or</span> new_state <span style="color:#f92672">==</span> (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">break</span>  <span style="color:#75715e"># reached the end, reset the game</span>
</span></span><span style="display:flex;"><span>    cur_state <span style="color:#f92672">=</span> new_state <span style="color:#75715e"># update the state and continue the game</span>
</span></span></code></pre></div><p>The resulting value estimates turn out to be as follows.</p>
<div class="flex gap-4 justify-center items-center flex-wrap">
    <img src="./fig2a.png">
    <img src="./fig2b.png">
</div>
<p>Although the estimates differ from the Monte Carlo estimates and the Dynamic Programming estimated we calculated in the <a href="https://www.statwizard.in/posts/markov-decision-process/">previous post</a>, the heatmap shows that the relative ordering of the value estimates are maintained. This is the main principle of having a value estimate, so that it enables you to compare between two states of the game, like a chess grandmaster often knows which of the two chess board positions is more advantageous with just a glance at them.</p>
<h2 id="n-step-td-variant">n-step TD Variant</h2>
<p>The Temporal Difference algorithm that we discussed so far is the 1-step version of it, this is because we are simulating the response for the environment for only 1 step ahead, and then using the Bellman backup equation to approximate the gain to update the estimate. We can generalize the same principle to create an $n$-step version of it, for any $n$. As you have guessed, it will simulate the game for $n$-steps ahead of the current state, and then approximate the gain by discounted sum of rewards from all these $n$ steps.</p>
<p>The update equation becomes
$$
v^\pi_{new}(S_t) = v^\pi_{old}(S_t) + \alpha\left( R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{n} v^\pi_{old}(S_{t+n}) - v^\pi_{old}(S_t) \right)
$$</p>
<p>This now gives you control over how much information you want to propagate to back. (Doesn&rsquo;t it look like a hybrid version of deep neural network with a flexibility to control how much you will propagate the gradients back for parameter estimation!).</p>
<p>Another example for a better understanding: Think of the $1$-step TD is the strategy that a novice chess player like me would employ. I often only think of only 1 step ahead, just seeing a fork or an x-ray kind of move or 1 move immediate checkmate. Though I might only see only $0$ step ahead if you play too well 😞. $n$-step TD is just extending that capability of that RL agent to play like a grandmaster, who can see several, may be $15-20$ (read $n$) moves ahead. And you want to create a chess world champion like <a href="https://en.wikipedia.org/wiki/AlphaZero">AlphaZero</a>, you might try your hand at 100-step TD! 😎</p>
<p>Let&rsquo;s see how $5$-step TD algorithm does in estimating value for the maze game.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># temporal difference learning - 5-step TD</span>
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>   <span style="color:#75715e"># number of episodes</span>
</span></span><span style="display:flex;"><span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span>
</span></span><span style="display:flex;"><span>td_step <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>val_ests <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>val_ests[<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>  <span style="color:#75715e"># the value for the terminating state</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>  <span style="color:#75715e"># learning rate</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> range(B):
</span></span><span style="display:flex;"><span>  niter <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>  <span style="color:#75715e"># number of iterations</span>
</span></span><span style="display:flex;"><span>  cur_state <span style="color:#f92672">=</span> (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>)  <span style="color:#75715e"># the starting state</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>    prev_visits <span style="color:#f92672">=</span> []   <span style="color:#75715e"># this holds the (state, reward) pairs</span>
</span></span><span style="display:flex;"><span>    runner_state <span style="color:#f92672">=</span> cur_state <span style="color:#66d9ef">if</span> len(prev_visits) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> prev_visits[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>]   <span style="color:#75715e"># an indexing state which runs through the n-step look ahead</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> len(prev_visits) <span style="color:#f92672">&lt;</span> td_step:
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># take a random move from current state</span>
</span></span><span style="display:flex;"><span>      action <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;up&#34;</span>, <span style="color:#e6db74">&#34;down&#34;</span>, <span style="color:#e6db74">&#34;left&#34;</span>, <span style="color:#e6db74">&#34;right&#34;</span>][np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">4</span>)]
</span></span><span style="display:flex;"><span>      runner_state, reward <span style="color:#f92672">=</span> gridworld_maze(runner_state, action)
</span></span><span style="display:flex;"><span>      prev_visits<span style="color:#f92672">.</span>append((runner_state, reward))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># once it has enough previous visits, we can now apply TD update step</span>
</span></span><span style="display:flex;"><span>    target1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum((gamma <span style="color:#f92672">**</span> np<span style="color:#f92672">.</span>arange(td_step)) <span style="color:#f92672">*</span>  np<span style="color:#f92672">.</span>array([r <span style="color:#66d9ef">for</span> (_, r) <span style="color:#f92672">in</span> prev_visits]))  <span style="color:#75715e"># this is the sum of discounted rewards of 5 steps</span>
</span></span><span style="display:flex;"><span>    target2 <span style="color:#f92672">=</span> (gamma <span style="color:#f92672">**</span> td_step) <span style="color:#f92672">*</span> val_ests[runner_state[<span style="color:#ae81ff">0</span>], runner_state[<span style="color:#ae81ff">1</span>]]  <span style="color:#75715e"># the estimated value at the last state</span>
</span></span><span style="display:flex;"><span>    val_ests[cur_state[<span style="color:#ae81ff">0</span>], cur_state[<span style="color:#ae81ff">1</span>]] <span style="color:#f92672">+=</span> lr <span style="color:#f92672">*</span> (target1 <span style="color:#f92672">+</span> target2 <span style="color:#f92672">-</span> val_ests[cur_state[<span style="color:#ae81ff">0</span>], cur_state[<span style="color:#ae81ff">1</span>]])  <span style="color:#75715e"># the final TD step</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    niter <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> niter <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">or</span> (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>) <span style="color:#f92672">in</span> [s <span style="color:#66d9ef">for</span> (s, _) <span style="color:#f92672">in</span> prev_visits]:
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">break</span> <span style="color:#75715e"># reached the end, reset the game</span>
</span></span><span style="display:flex;"><span>    cur_state <span style="color:#f92672">=</span> prev_visits[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]  <span style="color:#75715e"># the new starting state</span>
</span></span><span style="display:flex;"><span>    prev_visits <span style="color:#f92672">=</span> prev_visits[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span></code></pre></div><p>The resulting value estimates turn out to be as follows.</p>
<div class="flex gap-4 justify-center items-center flex-wrap">
    <img src="./fig2c.png">
    <img src="./fig2d.png">
</div>
<p>One of the interesting thing to note here is that there is no negative values in the estimates for the 5-step temporal difference method. This is probably because as soon as the end goal is only 5 steps away from a state, the state becomes valuable. With the value of the discount factor $\gamma = 0.9$, the ultimate reward of $100$ (i.e., the reward of reaching end goal) becomes $100\gamma^5 \approx 59$, which is clearly stil a larger reward compared to the punishment of $-1$ by hitting an obstacle.</p>
<h2 id="tdlambda-variant2">TD($\lambda$) Variant<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></h2>
<p>Although Temporal Difference algorithms has its strengths there are still a few ideas we can use to improve it.</p>
<ol>
<li>
<p>If we do an $n$-step TD method, it uses the reward upto next $n$ future steps. However, since we are waiting till $n$ steps, we have the information to perform $(n-1)$-step TD method also, $(n-2)$-step TD also, and so on. (You get the pattern!) But we are just relying on the $n$-step look ahead thinking that the middle step look ahead patterns are not meaningful.</p>
</li>
<li>
<p>In dynamic programming, the Bellman equation consisted of a theoretical expectation of the gain, i.e., it was</p>
</li>
</ol>
<p>$$
v^\pi(S_{t}) = \sum_{A_t} \sum_{S_{t+1}} \pi(A_t \mid S_t) p(S_{t+1} \mid S_t, A_t) G_{t:(t+1)}
$$</p>
<p>Here $G_{t:(t+1)}$ is the gain we obtain by stepping from state $S_t$ to $S_{t+1}$. In Monte Carlo, we approximate this theoretical expectation by simulating a single path. However, this is like approximating the probability of a coin turning up head by tossing the coin once. As you have guessed, it certainly won&rsquo;t be accurate. We need to toss the coin hundreds of times to get a reasonable approximation. This idea can also be theoretically shown using a result called the <strong>Law of Large Numbers</strong><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Therefore, the principle is to take aggregate multiple estimates together to reduce the uncertainty in estimation.</p>
<p>So, Richard Sutton<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> came up with the idea that instead of using the $n$-step gain
$$
G_{t:(t+n)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n v^\pi(S_{t+n})
$$
we can use the average of $1$-step gain, $2$-step gain, and so on until $n$-step gain, i.e.,
$$
\dfrac{1}{n} \left( G_{t:(t+1)} + G_{t:(t+2)} + \dots + G_{t:(t+n)} \right)
$$
as the target in the TD update equation. In general, we can take any weighted average of these temporal difference gains $G_{t:(t_1)}, G_{t:(t+2)}, \dots G_{t:(t+n)}$ and that can be regarded as a valid estimate for the discounted sum of future rewards.</p>
<p>Relating back to the chess example: Using the average of the $n$-step gains is to consult multiple chess players, ranging from novices who see only $1$-step to a grandmaster seeing $20$-steps ahead, and accumulate their suggestions. This is certainly better than relying on only one chess player, who might miss some of the obvious moves.</p>
<p>However, since again there is a long-term look ahead vs short-term look ahead, we can use a discounting process to take average. Thus, our new target becomes
$$
G^\lambda_{t:(t+n)} = \dfrac{G_{t:(t+1)} + \lambda G_{t:(t+2)} + \lambda^2G_{t:(t+3)} + \dots + \lambda^{n-1}G_{t:(t+n)} }{1 + \lambda + \lambda^2 + \dots + \lambda^{n-1}}
$$
for some $0 &lt; \lambda &lt; 1$. And we have the corresponding update equation
$$
v^\pi_{new}(S_t) = v^\pi_{old}(S_t) + \alpha\left( G^\lambda_{t:(t+n)} - v^\pi_{old}(S_t) \right)
$$
This is called the <strong>$n$-step TD($\lambda$) algorithm</strong> for value estimation. In principle, this is just reducing the uncertainty by averaging estimates from multiple $n$-step TD algorithms.</p>
<p>I won&rsquo;t demonstrate the code for $n$-step TD($\lambda$) algorithm here, but I encourage the enthusiatic readers to try it out for the maze game and comment below the solution. Unfortunately, there is no prize for getting it, since I&rsquo;m also on a tight budget here 😅!</p>
<h2 id="next-in-queue">Next in Queue</h2>
<p>In my next post in the RL blog series, we will dive deep into how we can use these value estimates to find an optimal strategy for an RL agent. And we will use python to code the algorithm, and then use the code to successfully land a space shuttle on the lunar surface inside a simulator. This should give a sneak peek at the amazing achievements of ISRO scientists who have successfully landed Chadrayaan-3 on the surface of the moon yesterday, on 23rd of August, 2023.</p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Sutton, R.S. Learning to predict by the methods of temporal differences. Mach Learn 3, 9–44 (1988). <a href="https://doi.org/10.1007/BF00115009">https://doi.org/10.1007/BF00115009</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://amreis.github.io/ml/reinf-learn/2017/07/08/reinforcement-learning-policy-evaluation-through-temporal-difference.html">Reinforcement Learning: Policy Evaluation through Temporal Difference</a> - Alister Reis&rsquo;s Blog.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://amreis.github.io/ml/reinf-learn/2017/11/02/reinforcement-learning-eligibility-traces.html">Reinforcement Learning: Eligibility Traces and TD(lambda)</a> - Alister Reis&rsquo;s Blog.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">https://en.wikipedia.org/wiki/Law_of_large_numbers</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Sutton, R. S., Barto, A. G. (2018). <a href="https://www.google.co.in/books/edition/Reinforcement_Learning_second_edition/sWV0DwAAQBAJ?hl=en">Reinforcement Learning: An Introduction.</a> United Kingdom: MIT Press.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

                </div>
            </div>
        </div>
    </div>
    <div class="flex flex-row justify-center items-center mb-3">
        <h2 class="text-lg text-blue-600">
            With heartfelt appreciation, thank you for being a valued reader, until next time!
        </h2>
    </div>
</div>



<div class="my-6 max-w-6xl md:mx-auto mx-4 text-center shadow-md rounded-md p-4">
    <p class="italic text-lg font-semibold my-4">Explore more posts like this</p>
    <div class="flex flex-row justify-center gap-4">
        
            <a href="/posts/markov-decision-process/" 
                class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                    transition-all ease-in-out rounded-md shadow-sm">
                Previous Post
            </a>
        
        
    </div>
    <div class="flex flex-row justify-center gap-4 my-4">
        <a href="/posts/" 
            class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                transition-all ease-in-out rounded-md shadow-sm">
            See all posts
        </a>
    </div>
</div>


<div class="my-6 max-w-6xl md:mx-auto mx-4">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statwizard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>



        </div>

        
<footer class="bg-neutral-900 text-center text-white">
    
    <div class="text-center px-0 py-4 w-full" style="background-color: rgba(0, 0, 0, 0.2)">
        © 2023 Copyright:
        <a class="text-white" href="/">StatWizard.in</a>
    </div>
</footer>

    </div>

    <script>
    $(document).ready(() => {

        
        $('#mobile-menu-button').click(() => {
            $('#mobile-menu').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('block');
            $('#mobile-menu-open-icon').toggleClass('hidden');
            $('#mobile-menu-open-icon').toggleClass('block');
        });

    });
</script>
<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true, securityLevel: "loose" });
</script>

     
</body>
</html>