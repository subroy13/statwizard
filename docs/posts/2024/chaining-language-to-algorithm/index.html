<!DOCTYPE html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    

    
    <link rel="preconnect" href="https://www.googletagmanager.com" />
    <link rel="preconnect" href="https://www.google-analytics.com" />

    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-97N9TLJ517"
    ></script>
    <script async>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-97N9TLJ517");
    </script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8206710020783397"
    crossorigin="anonymous"></script>
    <meta name="google-adsense-account" content="ca-pub-8206710020783397">
    

    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="StatWizard: Access beginner-friendly blog posts covering various applications of statistics, data science, computer programming and finance. Also hosts the personal website of the author, Subhrajyoty Roy."
    />
    <link rel="icon" href="/images/logo.png" />
    <title>
Chaining Language Model to Algorithm: Generative AI Series Part 5
</title>

    

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">


<script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">




<link rel="stylesheet" type="text/css" href="//localhost:1313/css/index.be6765e4baa2004dd1fbd2c9a57dbc3f.css" />






<script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>


<style>
  html {
      scroll-behavior: smooth;
  }
  
  .roboto-thin {
    font-family: "Roboto", sans-serif;
    font-weight: 100;
    font-style: normal;
  }

  .roboto-light {
    font-family: "Roboto", sans-serif;
    font-weight: 300;
    font-style: normal;
  }

  .roboto-regular {
    font-family: "Roboto", sans-serif;
    font-weight: 400;
    font-style: normal;
  }

  .roboto-medium {
    font-family: "Roboto", sans-serif;
    font-weight: 500;
    font-style: normal;
  }

  .roboto-bold {
    font-family: "Roboto", sans-serif;
    font-weight: 700;
    font-style: normal;
  }

  .roboto-black {
    font-family: "Roboto", sans-serif;
    font-weight: 900;
    font-style: normal;
  }

  .roboto-thin-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 100;
    font-style: italic;
  }

  .roboto-light-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 300;
    font-style: italic;
  }

  .roboto-regular-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 400;
    font-style: italic;
  }

  .roboto-medium-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 500;
    font-style: italic;
  }

  .roboto-bold-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 700;
    font-style: italic;
  }

  .roboto-black-italic {
    font-family: "Roboto", sans-serif;
    font-weight: 900;
    font-style: italic;
  }
</style>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/> 
      
     
  </head>
  <body>
    <div class="app-container min-h-[100vh] flex flex-col justify-between">
      <nav class="bg-white shadow-xl z-50">
  
  <div class="hidden md:flex w-full py-2 px-4 md:px-6 lg:px-8 md:flex flex-row items-center justify-between">
    
    <div class="flex flex-row justify-start items-center gap-4">
      
      <a
        href="/"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Home
      </a>
      
      <a
        href="/posts"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Blog
      </a>
      
      <a
        href="/research"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Research
      </a>
      
      <a
        href="/collection"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Collection
      </a>
      
      <a
        href="/cv"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        CV
      </a>
      
      <a
        href="/contact"
        class="px-4 py-2 box-border rounded-t-lg hover:bg-gray-100 hover:border-b-2 hover:border-blue-600 hover:text-blue-600 roboto-medium"
      >
        Contact
      </a>
      
    </div>


    
    <div class="w-[200px] flex flex-row justify-around items-center">
      
      <a
        href="mailto:subhrajyotyroy@gmail.com"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fas fa-envelope for link mailto:subhrajyotyroy@gmail.com"
        target="_blank"
      >
        <i class="fas fa-envelope fa-lg"></i>
      </a>
      
      <a
        href="https://github.com/subroy13"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-github for link https://github.com/subroy13"
        target="_blank"
      >
        <i class="fab fa-github fa-lg"></i>
      </a>
      
      <a
        href="https://www.linkedin.com/in/subroy13"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-linkedin-in for link https://www.linkedin.com/in/subroy13"
        target="_blank"
      >
        <i class="fab fa-linkedin-in fa-lg"></i>
      </a>
      
      <a
        href="https://scholar.google.com/citations?user=Gocm0lYAAAAJ&amp;hl=en&amp;authuser=1"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-google-scholar for link https://scholar.google.com/citations?user=Gocm0lYAAAAJ&amp;hl=en&amp;authuser=1"
        target="_blank"
      >
        <i class="fab fa-google-scholar fa-lg"></i>
      </a>
      
      <a
        href="#"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-instagram for link #"
        target="_blank"
      >
        <i class="fab fa-instagram fa-lg"></i>
      </a>
      
      <a
        href="https://www.facebook.com/subroy13/"
        class="hover:text-blue-600 transition-all ease-in-out"
        aria-label="fab fa-facebook for link https://www.facebook.com/subroy13/"
        target="_blank"
      >
        <i class="fab fa-facebook fa-lg"></i>
      </a>
      
    </div>
  </div>
  

  
  <div class="w-full mx-0 flex flex-col gap-0 md:hidden">
    <div class="w-full mx-0 flex flx-row px-2 py-4 justify-between items-center">
      
      <div class="w-[200px] flex items-center">
        <a
          class="uppercase font-extrabold font-mono text-2xl flex flex-row justify-center items-center gap-2"
          href="/"
        >
          <img
            src="/images/logo-wide-2.png"
            height="75px"
            width="150px"
            class="inline-block"
            alt="StatWizard Logo"
          />
        </a>
      </div>

      
      <div class="w-[30px] mr-4">
        <button 
          type="button"
          id="mobile-menu-button"
          class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:text-white outline-none"
          aria-controls="mobile-menu"
          aria-expanded="false"
        >
          <span class="sr-only">Open main menu</span>
          
          <svg
            id="mobile-menu-close-icon"
            class="bg-white text-neutral-800 outline-none block h-6 w-6"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            aria-hidden="true"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"
            />
          </svg>
          
          <svg
            id="mobile-menu-open-icon"
            class="bg-white text-neutral-800 outline-none hidden h-6 w-6"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            aria-hidden="true"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M6 18L18 6M6 6l12 12"
            />
          </svg>
        </button>
      </div>
    </div>

    
    <div id="mobile-menu" class="bg-white w-full mx-0 hidden">
      <ul class="flex flex-col gap-2">
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/" class=""> Home </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/posts" class=""> Blog </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/research" class=""> Research </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/collection" class=""> Collection </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/cv" class=""> CV </a>
        </li>
        
        <li
          class="px-4 py-2 box-border hover:bg-gray-100 hover:text-blue-600 hover:border-b-2 hover:border-blue-600"
        >
          <a href="/contact" class=""> Contact </a>
        </li>
        
      </ul>
    </div>
  </div>
  
</nav>


      <div class="content-container mx-0 px-0">
        



    <div class="relative h-[50vh] max-h-[300px] w-full bg-cover bg-center bg-no-repeat bg-fixed rounded-b-md" 
        style = "background-image: url('//localhost:1313/posts/2024/chaining-language-to-algorithm/featured.webp')">
        
            <div class = "absolute top-0 left-0 bg-neutral-800/100 w-fit text-white p-1 ml-1 font-xs rounded-md">
                <p>Cover image taken from <a href="https://www.unstability.ai/">Unstable Diffusion</a></p>
            </div>
        
    </div>



<div class="mt-4 flex flex-col gap-4 mx-2 px-2 md:mx-4 md:px-8">
    <h1 class="text-3xl text-center text-neutral-600 font-bold md:px-8 md:pt-6">
        Chaining Language Model to Algorithm: Generative AI Series Part 5
    </h1>
    <div class="m-2 p-4 rounded-lg shadow-lg border-2">
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="flex flex-col justify-center items-center gap-2">   
                <div class="mt-1 flex flex-row text-neutral-800 gap-4">
                    <p class="font-bold">Date:</p>
                    <p class="font-normal">14 June, 2024</p>
                </div> 
                <p class="mt-1 text-neutral-600">
                    <span class="font-bold">12</span> minutes read
                </p>
                <div class="my-1">
                    
                        <a href="//localhost:1313/tags/artificial-intelligence/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Artificial Intelligence
                        </a>
                    
                        <a href="//localhost:1313/tags/deep-learning/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Deep Learning
                        </a>
                    
                        <a href="//localhost:1313/tags/generative-ai/" 
                            class = "inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-base text-gray-700 mr-2 mb-2
                            hover:underline hover:bg-gray-300 transition ease-in-out">
                            Generative AI
                        </a>
                    
                </div>    
            </div>
            <div class="flex flex-col justify-start gap-2">
                <h1 class="text-lg font-bold">Prerequisites</h1>
                <ul class="space-y-2 text-left">
                    
                    <li class="flex items-center space-x-3">
                        <svg class="flex-shrink-0 w-3 h-3 text-green-500" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 12">
                            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5.917 5.724 10.5 15 1.5"/>
                        </svg>
                        <span>Probability - </span>
                        
                            <span class="text-green-600 font-semibold">Beginner</span>
                        
                    </li>
                    
                </ul>
            </div>    
        </div>
        <p class="mt-4 p-4 text-sm">
            <span class="font-semibold">Summary: </span> This is the 5th part of the blog post series on Generative AI and Prompting Techniques. In this post, we dive into the prompting technique known as function calling and tool use and see them in action.
        </p>
    </div>
    <div class="grid grid-cols-1 md:grid-cols-4 gap-2">
        <div class="relative col-span-1">
            <div class="sticky top-0 left-0 w-full pl-4 pt-4">
                <h1 class="text-bold text-xl">Table of Contents</h1>
                <div class="prose">
                    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#solving-hallunication-introducing-tools">Solving Hallunication: Introducing Tools</a>
      <ul>
        <li><a href="#designing-prompt-for-function-calling">Designing Prompt for Function Calling</a></li>
        <li><a href="#parsing-function-calling-response">Parsing Function Calling Response</a></li>
        <li><a href="#building-the-calculator-function">Building the Calculator Function</a></li>
        <li><a href="#final-step-passing-the-result-back-to-llm">Final Step: Passing the result back to LLM</a></li>
      </ul>
    </li>
    <li><a href="#the-good">The Good</a>
      <ul>
        <li><a href="#and-the-bad">And the bad!</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
                </div>
            </div>
        </div>
        <div class="col-span-3">
            <div class="text-justify break-words w-full">
                <div class="prose" style="max-width: none;">
                    <h2 id="introduction">Introduction</h2>
<p>In my previous <a href="http://statwizard.in/posts/2024/making-most-from-prompts/">post</a> on the Generative AI series, one problem with the language model, namely “the knowledge cutoff” issue, and we learnt about a technique called RAG that helps to solve the problem. Continuing further, here we look at another problem of the language models: <strong>Hallucination</strong>.</p>
<p>Hallucination is the problem that makes large language models arrogant 😒 about their answers, where they insist on validating their answer even if the argument is logically flawed. For instance, if you prompt an LLM to find out the answer to a multiplication of two three digits numbers, say 12346 x 98754 and show the output step by step, it will show you some mathematical steps that look correct but actually are not. It is unlikely that it will even output the correct answer 1219216884 in this case. I prompted the same to the Claude LLM, it responded back with</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-md" data-lang="md"><span style="display:flex;"><span>Assistant: Sure, let&#39;s multiply 12346 by 98754 step-by-step:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 1: Write the numbers one under the other, aligning the ones digits.
</span></span><span style="display:flex;"><span>    98754
</span></span><span style="display:flex;"><span>    12346
</span></span><span style="display:flex;"><span>-----------
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 2: Multiply the bottom number (12346) by each digit of the top number (98754) from right to left.
</span></span><span style="display:flex;"><span>12346 x 4 = 49,384 (Write this partial product on the next line)
</span></span><span style="display:flex;"><span>12346 x 5 = 61,730(Write this on the next line, shifting one place to the left)
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>Step 3: Add the partial products vertically.
</span></span><span style="display:flex;"><span>    111,114
</span></span><span style="display:flex;"><span>     86,422  
</span></span><span style="display:flex;"><span>      61,730
</span></span><span style="display:flex;"><span>      49,384
</span></span><span style="display:flex;"><span>-----------
</span></span><span style="display:flex;"><span>  1,218,650,664
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Therefore, the multiplication of 12346 and 98754 is 1,218,650,664.
</span></span></code></pre></div><p>As you can see, it returns back with some “near-correct” steps, but the final answer is wrong.</p>
<p>In this post, we will explore ways to address this kind of problem with generative AI models.</p>
<h2 id="solving-hallunication-introducing-tools">Solving Hallunication: Introducing Tools</h2>
<p>The fundamental issue underlying the hallucination problem is that the knowledge of the LLM-type AI models is fuzzy and probabilistic in nature, like us. Hence, it does not actually understand the logical deductions or steps involved with a multiplication problem, but rather it is simply trying to autocomplete the sentence in a reasonable way.</p>
<p>To solve this, let us take a step back and think about how we humans would solve the problem: since then we can aim to mimic the human behaviour in the AI. As a human, you might possibly use a calculator for this (unless, of course, you are a nerd 🤓 and you love doing multiplications ✖️). So, we will allow our AI system to use a calculator-like function for the same.</p>
<p>So now, the idea is as follows:</p>
<ul>
<li>
<p>Design a prompt for the LLM which describes the calculator function and its arguments.</p>
</li>
<li>
<p>When we ask the LLM the query about the multiplication result, it responds in a specific way by showing that it is using the calculator function with proper arguments.</p>
</li>
<li>
<p>Here, we stop the LLM and intervene. We parse the output of the LLM using Regular Expression (if you don’t know what it means, check out this <a href="https://regex101.com/">cool website</a>) and get the arguments.</p>
</li>
<li>
<p>Then we call a normal computer code to perform the calculation for us. This can be your typical software program written in programming languages (e.g. - Python 🐍, Golang 🏃, Java, etc.)</p>
</li>
</ul>
<p>Once the computer code runs, we pass the result in a nicely formatted string and ask the LLM to continue generation from there on.</p>
<div class="mermaid">
graph LR
    User --> LLM
    LLM --> FinalOutput[Final Output]
    LLM --> PartialOutput[Partial Output]
    PartialOutput -- Extraction --> Tool
    Tool --> SoftwareCode[Software Code]
    SoftwareCode -- Result --> LLM
</div>
<p>This entire system of using external software code along with the powerful natural language capabilities of LLM is called <strong>Function Calling</strong>.</p>
<h3 id="designing-prompt-for-function-calling">Designing Prompt for Function Calling</h3>
<p>The system prompts for a function calling generative AI system usually uses a specific template.</p>
<p>We start by saying:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-md" data-lang="md"><span style="display:flex;"><span>You are a helpful chatbot …. In this environment, you have access to the following tool. You may use it by responding in the format 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">function_call</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">function_name</span>&gt;Give the name of the function you want to use&lt;/<span style="color:#f92672">function_name</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">input</span>&gt;<span style="color:#960050;background-color:#1e0010">&lt;</span>{parameter name}&gt;value of the parameter&lt;/<span style="color:#f92672">parameter_name</span>&gt; &lt;/<span style="color:#f92672">input</span>&gt;
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">function_call</span>&gt;
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>This is the initial setup before we describe the function (or tool) to the generative AI. This prompt asks the LLM to respond in a specific format if it uses a function, this helps us in parsing the output of the LLM when we intervene.</p>
<p>Once we have this initial prompt added, then we add the description of the function, namely our calculator function.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-md" data-lang="md"><span style="display:flex;"><span>&lt;<span style="color:#f92672">function</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">name</span>&gt;run_calculator&lt;/<span style="color:#f92672">name</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">description</span>&gt;A function that performs basic mathematical calculation operation.&lt;/<span style="color:#f92672">description</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">arguments</span>&gt;
</span></span><span style="display:flex;"><span>   &lt;<span style="color:#f92672">operation</span>&gt;
</span></span><span style="display:flex;"><span>      &lt;<span style="color:#f92672">type</span>&gt;string&lt;/<span style="color:#f92672">type</span>&gt;
</span></span><span style="display:flex;"><span>      &lt;<span style="color:#f92672">description</span>&gt;The binary operation to perform between two numbers, must be one of +, -, *, / &lt;/<span style="color:#f92672">description</span>&gt;
</span></span><span style="display:flex;"><span>   &lt;/<span style="color:#f92672">operation</span>&gt;
</span></span><span style="display:flex;"><span>   &lt;<span style="color:#f92672">first_number</span>&gt;
</span></span><span style="display:flex;"><span>      &lt;<span style="color:#f92672">type</span>&gt;number&lt;/<span style="color:#f92672">type</span>&gt;&lt;<span style="color:#f92672">description</span>&gt;The first number&lt;/<span style="color:#f92672">description</span>&gt;
</span></span><span style="display:flex;"><span>   &lt;/<span style="color:#f92672">first_number</span>&gt;
</span></span><span style="display:flex;"><span>   &lt;<span style="color:#f92672">second_number</span>&gt;
</span></span><span style="display:flex;"><span>      &lt;<span style="color:#f92672">type</span>&gt;number&lt;/<span style="color:#f92672">type</span>&gt;&lt;<span style="color:#f92672">description</span>&gt;The second number&lt;/<span style="color:#f92672">description</span>&gt;
</span></span><span style="display:flex;"><span>   &lt;/<span style="color:#f92672">second_number</span>&gt;
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">arguments</span>&gt;
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">function</span>&gt;
</span></span></code></pre></div><p>Note that, this above prompt describes the calculator function in detail. It tells that the function name is <strong>“run_calculator”</strong>, provides a description of what it does, and then describes its three arguments. The first argument is a string denoting the binary operation (addition, subtraction, multiplication or division, what to perform), and the next two arguments are the two numbers before and after the binary operation symbol.</p>
<p>There are multiple ways to generate it. The following are two extreme ideas:</p>
<ul>
<li>
<p>We writing this prompt entirely manually.</p>
</li>
<li>
<p>We write the actual code for this <code>run_calculator</code> program (or function/tool). We then use the code comprehension ability of LLM to generate a docstring for the function (i.e., basically a human-readable documentation for the function and its arguments). Usually, these docstrings come in a predefined format (e.g. - JsDoc, NumpyDoc, Doxygen, these are some of the popular ones you may have heard about). These predefined formatted docstrings can then be easily parsed and converted into the format required by the above prompt. Here, the entire prompt generation process itself is automated.</p>
</li>
</ul>
<p>Usually, people tend to prefer a middle ground. They begin with an auto-generated tool description for the prompt and then tweak it to get the most performance out.</p>
<h3 id="parsing-function-calling-response">Parsing Function Calling Response</h3>
<p>Given the query “What is the result of 12346 times 98754?”, the LLM now responds according to the format given in the system prompt.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-md" data-lang="md"><span style="display:flex;"><span>Assistant: Sure, to multiply 12346 by 98754, we can use the provided tools as follows.
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">function_call</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">function_name</span>&gt;run_calculator&lt;/<span style="color:#f92672">function_name</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">input</span>&gt;
</span></span><span style="display:flex;"><span>    &lt;<span style="color:#f92672">operation</span>&gt;&#34;*&#34;&lt;/<span style="color:#f92672">operation</span>&gt;
</span></span><span style="display:flex;"><span>    &lt;<span style="color:#f92672">first_number</span>&gt;12346&lt;/<span style="color:#f92672">first_number</span>&gt;
</span></span><span style="display:flex;"><span>    &lt;<span style="color:#f92672">second_number</span>&gt;98754&lt;/<span style="color:#f92672">second_number</span>&gt;
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">input</span>&gt;
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">function_call</span>&gt;
</span></span></code></pre></div><p>Note that, the LLM might continue to say something more. Here, a useful setting <strong>“stop sequence”</strong> might be useful. See more in my previous post.</p>
<p>Usually, a single RegExp expression can give you all the required things, i.e. the name of the function and its parameter values. For example, in Python, you can use <code>&lt;(.*?)&gt;(.*)&lt;/\1&gt;</code> (To verify this, copy the response of the LLM into this <a href="https://regex101.com/">RegExp testing website</a>.) For the sake of completion, let me break it down. The first part, <code>&lt;(.*?)&gt;</code> matches any tag-like string, yielding the name of the parameter (i.e., operation or first_number or second_number). The middle part captures the actual value of the parameter. And, the final part ensures that the tag captured in the first part has a closing tag as well.</p>
<p>So, at the end of this step, your software application will contain three variables as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-js" data-lang="js"><span style="display:flex;"><span><span style="color:#a6e22e">string</span> <span style="color:#a6e22e">op</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;*&#34;</span>;
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">number</span> <span style="color:#a6e22e">firstNum</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">12346</span>;
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">number</span> <span style="color:#a6e22e">secondNum</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">98754</span>;
</span></span></code></pre></div><h3 id="building-the-calculator-function">Building the Calculator Function</h3>
<p>Once you have the values extracted from the LLM response, it is time to call your application code to actually execute this simple calculation. You can use any programming language for this, but here’s a sample code in Python. (Well, I usually pick Python for demonstration as it has a very clear syntax guided by the <a href="https://peps.python.org/pep-0020/">Zen of Python</a>)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_calculator</span>(op, first_num, second_num):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> op <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;+&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> first_num <span style="color:#f92672">+</span> second_num
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> op <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;-&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> first_num <span style="color:#f92672">-</span> second_num
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> op <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;*&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> first_num <span style="color:#f92672">*</span> second_num
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> op <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;/&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> first_num <span style="color:#f92672">/</span> second_num
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">Exception</span>(<span style="color:#e6db74">&#34;Not implemented&#34;</span>)
</span></span></code></pre></div><p>Now that we have our <code>run_calculator</code> function ready, we simply pass the extracted arguments to this function. It returns back the output number $1219216884$.</p>
<h3 id="final-step-passing-the-result-back-to-llm">Final Step: Passing the result back to LLM</h3>
<p>In the final step, we pass the resulting number of the calculation back to the LLM. At this point, the LLM is expected to take a hint of this result and then continue generating the final answer in a natural conversational tone.</p>
<p>To perform this, we repeat the prompt and the response from before, and append our magic from the Python function after that. For instance, we use:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-md" data-lang="md"><span style="display:flex;"><span>SYSTEM: You are a helpful chatbot …. In this environment, you have access to the following tool...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>User: Tell me what is 12356 times 98754?
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Assistant: Sure, to multiply 12346 by 98754, we can use ...
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">function_call</span>&gt;
</span></span><span style="display:flex;"><span>By calling this function, I get back the following output. 
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">function_result</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">result</span>&gt;1219216884&lt;/<span style="color:#f92672">result</span>&gt;
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">function_result</span>&gt;
</span></span><span style="display:flex;"><span>So, my final response would be:
</span></span></code></pre></div><p>Basically, we keep the system prompt as it is, we keep the user query as it is, but we partially complete the assistant response including the part where it performed the function calling, and then we add an additional part where we programmatically inject the result of the function in a nice formatted way (shown in bold in the above prompt).</p>
<p>This means that when the LLM is trying to complete this statement, it is highly likely that it will autocomplete with <strong>&ldquo;12346 times 98754 results is 1219216884”</strong>, or something like that, which is the correct answer.</p>
<h2 id="the-good">The Good</h2>
<p>Let’s now talk about what’s so good about this function-calling procedure.</p>
<ul>
<li>
<p>The fundamental idea of the function calling technique in LLM is to intervene in the output of the LLM to inject logical flow that can be programmed in a deterministic way. We already saw in the previous post that a RAG technique can be used similarly to inject information to the LLM through prompting. However, in some case, RAG is not possible. For instance, in the calculator example, the user can ask for any number multiplied with any number, and it is not possible to retrieve a relevant multiplication table without extracting the exact numbers of multiply.</p>
</li>
<li>
<p>Note that, so far we have only added a single function to the system prompt to the LLM. However, you can provide it with multiple function descriptions, and only some of them may be useful for the LLM to answer the query presented by the user. For example, you want to build an AI assistant for stock trading, and you can provide it with two functions:</p>
<ul>
<li>Given a stock symbol, fetches the current and historical price of the past few days.</li>
<li>Given a stock symbol, fetches the fundamental ratios of the company.</li>
</ul>
</li>
<li>
<p>The LLM can now have a technical research tool and a fundamental research tool available at its disposal, and by combining their information it can behave like a human-like stock trading strategy specialist.</p>
</li>
<li>
<p>Allowing usage of multiple functions increases the capability of LLM drastically. For example, you can now write functions that take a piece of code (say a git command) and executes it. If the code generated by an LLM contains an error, but we can feed the error back to the LLM in the result prompt, such as</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-md" data-lang="md"><span style="display:flex;"><span>Assistant: Sure, to multiply 12346 by 98754, we can use ...
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">function_call</span>&gt;
</span></span><span style="display:flex;"><span>By calling this function, I get back the following output.
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">function_result</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">error</span>&gt;SyntaxError at line 2; near &#39;this&#39;, expected &#39;.&#39; but got &#39;;&#39; &lt;/<span style="color:#f92672">error</span>&gt;
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">function_result</span>&gt; 
</span></span></code></pre></div><p>which tells the LLM what exact error it did. Now, it can recify its errors and retry the function calling again.</p>
<p>If you think about it a bit, this is exactly how we provide feedback to ChatGPT when it does not provide a response that we particularly like. But by using function calling, now no human intervention is needed, and completely autonomously the LLM is able to execute the user request, by using the tools at its disposal.</p>
<p>This brings a new kind of generative AI system, called <strong>AI Agents</strong>.</p>
<h3 id="and-the-bad">And the bad!</h3>
<p>However, as with everything, there is a price to pay for all these good stuffs.</p>
<p>First is the high cost associated with this technique. Note that, you will need to send two calls to the LLM here, one before the function call intervention, and another after the function call output to respond back the answer. If you see, both of these send the same system prompt, which also contains tokens describing the functions. This means, on average, you send 2x more tokens in this technique, but approximately 2x more cost.</p>
<p>The second thing is that the more tools (or functions) you want to use, the more tokens are present in the system prompt. It increases latency, and cost and reduces the accuracy of the model since now the model needs to remember a much longer context to make a behaviour. For example: Imagine you are preparing for an exam, it is much easier to remember a single chapter rather than the entire book.</p>
<p>Finally, most of the models have a predefined context window length, i.e., the maximum number of tokens you can pass to the LLM. Since in our daily lives, we may end up using hundreds of different tools for different purposes, it becomes extremely difficult to add more than 10 function descriptions in the prompt, beyond which most of the LLM tends to perform poorly.</p>
<p>One solution to this is to keep the function description in a vector database and pull them dynamically based on relevancy, much like an RAG model. In that way, your prompt never contains more than (say 4) function descriptions.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Throughout a few of my previous posts, we have explored ways about how to make large language models effective and efficient using prompt engineering techniques.</p>
<p>As we understand, the prompt may contain valuable insights for the LLM to act upon, which may also contain some sensitive data injected through modes of RAG or function calling systems. This means, an adversary can create carefully constructed prompts to break the system and inject malicious code into your application. In the next post, we discuss some of these security vulnerabilities and how we can protect against these threats.</p>
<p>Thank you very much for being a valued reader! 🙏🏽 Stay tuned for upcoming posts on the Generative AI series. Subscribe to my newsletter <a href="https://statwizard.substack.com/?showWelcome=true">here</a> to get notified when the next post is out. 📢</p>
<p>Until next time.</p>

                </div>
            </div>
        </div>
    </div>
    <div class="flex flex-col justify-center items-center my-4 p-4" style="background-color: #14101b;">
        <h2 class="text-lg text-white my-4">
            Thank you very much for being a valued reader! 🙏🏽 Subscribe below to get notified when new posts are out. 📢 Stay tuned until next time!
        </h2>
        <iframe src="https://statwizard.substack.com/embed" width="480" height="150" style="border:1px solid black; background:black;" frameborder="0" scrolling="no"></iframe>
    </div>
</div>



<div class="my-6 max-w-6xl md:mx-auto mx-4 text-center shadow-md rounded-md p-4">
    <p class="italic text-lg font-semibold my-4">Explore more posts like this</p>
    <div class="flex flex-row justify-center gap-4">
        
            <a href="//localhost:1313/posts/2024/making-most-from-prompts/" 
                class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                    transition-all ease-in-out rounded-md shadow-sm">
                Previous Post
            </a>
        
        
            <a href="//localhost:1313/posts/2024/hack-into-llm/" class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900
             focus:bg-neutral-900 transition-all ease-in-out rounded-md shadow-sm">
                Next Post
            </a>
        
    </div>
    <div class="flex flex-row justify-center gap-4 my-4">
        <a href="//localhost:1313/posts/" 
            class="px-6 py-2 bg-blue-800 text-white hover:bg-neutral-900 focus:bg-neutral-900 
                transition-all ease-in-out rounded-md shadow-sm">
            See all posts
        </a>
    </div>
</div>


<div class="my-6 max-w-6xl md:mx-auto mx-4">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "statwizard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>



      </div>
    </div>

    <script>
    $(document).ready(() => {

        
        $('#mobile-menu-button').click(() => {
            $('#mobile-menu').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('hidden');
            $('#mobile-menu-close-icon').toggleClass('block');
            $('#mobile-menu-open-icon').toggleClass('hidden');
            $('#mobile-menu-open-icon').toggleClass('block');
        });

    });
</script>


<script async src="https://kit.fontawesome.com/ca14d5004b.js" crossorigin="anonymous"></script>
 
    


    <script type="module" defer>
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, securityLevel: "loose" });
    </script>



    
    <script defer>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>



  </body>
</html>
